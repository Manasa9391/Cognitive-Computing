{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Problem1_(1).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Manasa9391/Cognitive-Computing/blob/master/Problem1_(1).ipynb)"
      ]
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "aJnMU23BsTpF"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "#from keras.optimizers import RMSpro\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1yy7RMnP3va9"
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Flatten,Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "GRlS4Xl9tFKg"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "dXSNq8Gv96Ty"
      },
      "cell_type": "code",
      "source": [
        "def save_history(history, result_file):\n",
        "    loss = history.history['loss']\n",
        "    acc = history.history['acc']\n",
        "    val_loss = history.history['val_loss']\n",
        "    val_acc = history.history['val_acc']\n",
        "    nb_epoch = len(acc)\n",
        "\n",
        "    with open(result_file, \"w\") as fp:\n",
        "        fp.write(\"epoch\\tloss\\tacc\\tval_loss\\tval_acc\\n\")\n",
        "        for i in range(nb_epoch):\n",
        "            fp.write(\"%d\\t%f\\t%f\\t%f\\t%f\\n\" %\n",
        "                     (i, loss[i], acc[i], val_loss[i], val_acc[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3893
        },
        "colab_type": "code",
        "id": "qD46C2OG51rL",
        "outputId": "42d2d601-4a7f-4bb3-f308-d7da7cf4c1dc"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    nb_epoch = 100\n",
        "    batch_size = 128\n",
        "    nb_classes = 10\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "    X_train = X_train.reshape(50000, 32 * 32 * 3)\n",
        "    X_test = X_test.reshape(10000, 32 * 32 * 3)\n",
        "\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "    X_train /= 255.0\n",
        "    X_test /= 255.0\n",
        "\n",
        "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "    Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "    # MLP\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1024, input_shape=(3072, )))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    # training\n",
        "    history = model.fit(X_train, Y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        nb_epoch=nb_epoch,\n",
        "                        verbose=1,\n",
        "                        validation_data=(X_test, Y_test))\n",
        "\n",
        "    save_history(history, 'history.txt')\n",
        "\n",
        "    loss, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
        "    print('Test loss:', loss)\n",
        "    print('Test acc:', acc)\n",
        "    \n",
        "    \n",
        "  \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 1024)              3146752   \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 3,676,682\n",
            "Trainable params: 3,676,682\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "50000/50000 [==============================] - 5s 103us/step - loss: 2.0060 - acc: 0.2716 - val_loss: 1.7846 - val_acc: 0.3552\n",
            "Epoch 2/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.8314 - acc: 0.3381 - val_loss: 1.7041 - val_acc: 0.3999\n",
            "Epoch 3/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.7759 - acc: 0.3576 - val_loss: 1.6938 - val_acc: 0.3971\n",
            "Epoch 4/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.7486 - acc: 0.3661 - val_loss: 1.6216 - val_acc: 0.4235\n",
            "Epoch 5/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.7263 - acc: 0.3770 - val_loss: 1.6280 - val_acc: 0.4250\n",
            "Epoch 6/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.6967 - acc: 0.3885 - val_loss: 1.6119 - val_acc: 0.4285\n",
            "Epoch 7/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.6841 - acc: 0.3923 - val_loss: 1.6183 - val_acc: 0.4188\n",
            "Epoch 8/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.6674 - acc: 0.3957 - val_loss: 1.5870 - val_acc: 0.4288\n",
            "Epoch 9/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.6596 - acc: 0.3998 - val_loss: 1.5634 - val_acc: 0.4433\n",
            "Epoch 10/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.6505 - acc: 0.4055 - val_loss: 1.5799 - val_acc: 0.4412\n",
            "Epoch 11/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.6334 - acc: 0.4113 - val_loss: 1.5596 - val_acc: 0.4470\n",
            "Epoch 12/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.6281 - acc: 0.4110 - val_loss: 1.5512 - val_acc: 0.4517\n",
            "Epoch 13/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.6191 - acc: 0.4170 - val_loss: 1.5531 - val_acc: 0.4477\n",
            "Epoch 14/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.6147 - acc: 0.4168 - val_loss: 1.5589 - val_acc: 0.4578\n",
            "Epoch 15/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.6058 - acc: 0.4197 - val_loss: 1.5705 - val_acc: 0.4380\n",
            "Epoch 16/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.5969 - acc: 0.4231 - val_loss: 1.5449 - val_acc: 0.4581\n",
            "Epoch 17/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.5976 - acc: 0.4232 - val_loss: 1.5389 - val_acc: 0.4556\n",
            "Epoch 18/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.5894 - acc: 0.4290 - val_loss: 1.5178 - val_acc: 0.4636\n",
            "Epoch 19/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.5866 - acc: 0.4289 - val_loss: 1.5382 - val_acc: 0.4503\n",
            "Epoch 20/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.5802 - acc: 0.4336 - val_loss: 1.5398 - val_acc: 0.4528\n",
            "Epoch 21/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.5673 - acc: 0.4346 - val_loss: 1.5010 - val_acc: 0.4662\n",
            "Epoch 22/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.5713 - acc: 0.4335 - val_loss: 1.4956 - val_acc: 0.4759\n",
            "Epoch 23/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.5648 - acc: 0.4344 - val_loss: 1.5314 - val_acc: 0.4562\n",
            "Epoch 24/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.5630 - acc: 0.4377 - val_loss: 1.5076 - val_acc: 0.4614\n",
            "Epoch 25/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.5513 - acc: 0.4392 - val_loss: 1.5041 - val_acc: 0.4661\n",
            "Epoch 26/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.5508 - acc: 0.4416 - val_loss: 1.5063 - val_acc: 0.4670\n",
            "Epoch 27/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.5503 - acc: 0.4438 - val_loss: 1.4918 - val_acc: 0.4686\n",
            "Epoch 28/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.5451 - acc: 0.4445 - val_loss: 1.5252 - val_acc: 0.4584\n",
            "Epoch 29/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.5410 - acc: 0.4465 - val_loss: 1.5022 - val_acc: 0.4679\n",
            "Epoch 30/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.5405 - acc: 0.4473 - val_loss: 1.5123 - val_acc: 0.4635\n",
            "Epoch 31/100\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.5359 - acc: 0.4457 - val_loss: 1.5106 - val_acc: 0.4607\n",
            "Epoch 32/100\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 1.5278 - acc: 0.4502 - val_loss: 1.5030 - val_acc: 0.4683\n",
            "Epoch 33/100\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 1.5273 - acc: 0.4484 - val_loss: 1.4989 - val_acc: 0.4710\n",
            "Epoch 34/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.5262 - acc: 0.4499 - val_loss: 1.5033 - val_acc: 0.4629\n",
            "Epoch 35/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.5251 - acc: 0.4511 - val_loss: 1.4934 - val_acc: 0.4779\n",
            "Epoch 36/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.5129 - acc: 0.4552 - val_loss: 1.4822 - val_acc: 0.4745\n",
            "Epoch 37/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.5170 - acc: 0.4536 - val_loss: 1.4894 - val_acc: 0.4669\n",
            "Epoch 38/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.5108 - acc: 0.4583 - val_loss: 1.4917 - val_acc: 0.4726\n",
            "Epoch 39/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.5022 - acc: 0.4607 - val_loss: 1.4733 - val_acc: 0.4768\n",
            "Epoch 40/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.5047 - acc: 0.4561 - val_loss: 1.4770 - val_acc: 0.4771\n",
            "Epoch 41/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.5032 - acc: 0.4613 - val_loss: 1.4841 - val_acc: 0.4729\n",
            "Epoch 42/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.4971 - acc: 0.4613 - val_loss: 1.4879 - val_acc: 0.4667\n",
            "Epoch 43/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.4919 - acc: 0.4628 - val_loss: 1.4867 - val_acc: 0.4749\n",
            "Epoch 44/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.4968 - acc: 0.4614 - val_loss: 1.4792 - val_acc: 0.4697\n",
            "Epoch 45/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.4957 - acc: 0.4641 - val_loss: 1.4632 - val_acc: 0.4836\n",
            "Epoch 46/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.4905 - acc: 0.4662 - val_loss: 1.5179 - val_acc: 0.4610\n",
            "Epoch 47/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.4903 - acc: 0.4649 - val_loss: 1.5163 - val_acc: 0.4635\n",
            "Epoch 48/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.4865 - acc: 0.4640 - val_loss: 1.4836 - val_acc: 0.4742\n",
            "Epoch 49/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.4901 - acc: 0.4660 - val_loss: 1.4901 - val_acc: 0.4798\n",
            "Epoch 50/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.4891 - acc: 0.4673 - val_loss: 1.4623 - val_acc: 0.4793\n",
            "Epoch 51/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.4723 - acc: 0.4717 - val_loss: 1.4656 - val_acc: 0.4886\n",
            "Epoch 52/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.4815 - acc: 0.4682 - val_loss: 1.4597 - val_acc: 0.4794\n",
            "Epoch 53/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.4748 - acc: 0.4695 - val_loss: 1.4620 - val_acc: 0.4839\n",
            "Epoch 54/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.4677 - acc: 0.4702 - val_loss: 1.4612 - val_acc: 0.4863\n",
            "Epoch 55/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.4761 - acc: 0.4679 - val_loss: 1.4671 - val_acc: 0.4814\n",
            "Epoch 56/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.4749 - acc: 0.4682 - val_loss: 1.5203 - val_acc: 0.4646\n",
            "Epoch 57/100\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.4724 - acc: 0.4713 - val_loss: 1.4617 - val_acc: 0.4846\n",
            "Epoch 58/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.4663 - acc: 0.4716 - val_loss: 1.4538 - val_acc: 0.4849\n",
            "Epoch 59/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.4653 - acc: 0.4741 - val_loss: 1.4791 - val_acc: 0.4784\n",
            "Epoch 60/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.4607 - acc: 0.4739 - val_loss: 1.4588 - val_acc: 0.4837\n",
            "Epoch 61/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.4638 - acc: 0.4749 - val_loss: 1.4895 - val_acc: 0.4624\n",
            "Epoch 62/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.4571 - acc: 0.4779 - val_loss: 1.4603 - val_acc: 0.4797\n",
            "Epoch 63/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.4513 - acc: 0.4773 - val_loss: 1.4611 - val_acc: 0.4776\n",
            "Epoch 64/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.4579 - acc: 0.4758 - val_loss: 1.4710 - val_acc: 0.4750\n",
            "Epoch 65/100\n",
            "50000/50000 [==============================] - 5s 98us/step - loss: 1.4512 - acc: 0.4765 - val_loss: 1.4543 - val_acc: 0.4886\n",
            "Epoch 66/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.4514 - acc: 0.4798 - val_loss: 1.4719 - val_acc: 0.4779\n",
            "Epoch 67/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.4506 - acc: 0.4789 - val_loss: 1.4886 - val_acc: 0.4705\n",
            "Epoch 68/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.4700 - acc: 0.4689 - val_loss: 1.4652 - val_acc: 0.4845\n",
            "Epoch 69/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.4614 - acc: 0.4748 - val_loss: 1.4787 - val_acc: 0.4754\n",
            "Epoch 70/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.4427 - acc: 0.4796 - val_loss: 1.4640 - val_acc: 0.4835\n",
            "Epoch 71/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.4493 - acc: 0.4785 - val_loss: 1.4625 - val_acc: 0.4845\n",
            "Epoch 72/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.4435 - acc: 0.4815 - val_loss: 1.4619 - val_acc: 0.4782\n",
            "Epoch 73/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.4455 - acc: 0.4809 - val_loss: 1.4754 - val_acc: 0.4815\n",
            "Epoch 74/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.4480 - acc: 0.4823 - val_loss: 1.4696 - val_acc: 0.4812\n",
            "Epoch 75/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.4384 - acc: 0.4828 - val_loss: 1.4654 - val_acc: 0.4843\n",
            "Epoch 76/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.4374 - acc: 0.4841 - val_loss: 1.4515 - val_acc: 0.4842\n",
            "Epoch 77/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.4457 - acc: 0.4833 - val_loss: 1.4727 - val_acc: 0.4764\n",
            "Epoch 78/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.4322 - acc: 0.4874 - val_loss: 1.4600 - val_acc: 0.4836\n",
            "Epoch 79/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.4321 - acc: 0.4856 - val_loss: 1.4371 - val_acc: 0.4952\n",
            "Epoch 80/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.4312 - acc: 0.4858 - val_loss: 1.4570 - val_acc: 0.4839\n",
            "Epoch 81/100\n",
            "50000/50000 [==============================] - 5s 93us/step - loss: 1.4282 - acc: 0.4859 - val_loss: 1.4579 - val_acc: 0.4794\n",
            "Epoch 82/100\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.4284 - acc: 0.4859 - val_loss: 1.4614 - val_acc: 0.4834\n",
            "Epoch 83/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.4276 - acc: 0.4873 - val_loss: 1.4641 - val_acc: 0.4896\n",
            "Epoch 84/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.4317 - acc: 0.4891 - val_loss: 1.4684 - val_acc: 0.4793\n",
            "Epoch 85/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.4219 - acc: 0.4866 - val_loss: 1.4650 - val_acc: 0.4863\n",
            "Epoch 86/100\n",
            "50000/50000 [==============================] - 5s 98us/step - loss: 1.4242 - acc: 0.4862 - val_loss: 1.4515 - val_acc: 0.4871\n",
            "Epoch 87/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.4216 - acc: 0.4921 - val_loss: 1.4838 - val_acc: 0.4719\n",
            "Epoch 88/100\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.4227 - acc: 0.4887 - val_loss: 1.4577 - val_acc: 0.4811\n",
            "Epoch 89/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.4251 - acc: 0.4865 - val_loss: 1.4527 - val_acc: 0.4884\n",
            "Epoch 90/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.4205 - acc: 0.4929 - val_loss: 1.4634 - val_acc: 0.4855\n",
            "Epoch 91/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.4217 - acc: 0.4928 - val_loss: 1.4738 - val_acc: 0.4669\n",
            "Epoch 92/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.4207 - acc: 0.4890 - val_loss: 1.4472 - val_acc: 0.4844\n",
            "Epoch 93/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.4128 - acc: 0.4921 - val_loss: 1.4459 - val_acc: 0.4934\n",
            "Epoch 94/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.4071 - acc: 0.4963 - val_loss: 1.4716 - val_acc: 0.4769\n",
            "Epoch 95/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.4156 - acc: 0.4911 - val_loss: 1.4443 - val_acc: 0.4925\n",
            "Epoch 96/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.4160 - acc: 0.4906 - val_loss: 1.4494 - val_acc: 0.4839\n",
            "Epoch 97/100\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 1.4104 - acc: 0.4941 - val_loss: 1.4659 - val_acc: 0.4823\n",
            "Epoch 98/100\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 1.4071 - acc: 0.4922 - val_loss: 1.4536 - val_acc: 0.4926\n",
            "Epoch 99/100\n",
            "50000/50000 [==============================] - 5s 98us/step - loss: 1.4107 - acc: 0.4950 - val_loss: 1.4870 - val_acc: 0.4810\n",
            "Epoch 100/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.4082 - acc: 0.4963 - val_loss: 1.4742 - val_acc: 0.4837\n",
            "Test loss: 1.4742318756103516\n",
            "Test acc: 0.4837\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "7VK6htE5Mc8g",
        "outputId": "d427f515-f15c-4c1f-d8c8-26becf485e0e"
      },
      "cell_type": "code",
      "source": [
        "def convertCIFER10Data(image):\n",
        "    img = image.astype('float32')\n",
        "    img /= 255\n",
        "    c = np.zeros(3072).reshape((1,3072))\n",
        "    c[0] = img\n",
        "    return c\n",
        "  \n",
        "right = 0\n",
        "mistake = 0\n",
        "import random\n",
        "for i in range(100):\n",
        "    index = random.randint(0, X_test.shape[0])\n",
        "    image = X_test[index]\n",
        "    #print(index)\n",
        "    data = convertCIFER10Data(image)\n",
        "\n",
        "  \n",
        "    ret = model.predict(data, batch_size=1) \n",
        "    #print(ret)\n",
        "\n",
        "    bestnum = 0.0\n",
        "    bestclass = 0\n",
        "    for n in [0,1,2,3,4,5,6,7,8,9]:\n",
        "        if bestnum < ret[0][n]:\n",
        "            bestnum = ret[0][n]\n",
        "            bestclass = n\n",
        "    i = np.array(list(Y_test[index]).index(1))\n",
        "    if i == bestclass:\n",
        "        #print(i)\n",
        "        #plt.title(cifar10_labels[bestclass])\n",
        "        right += 1\n",
        "    else:\n",
        "        #plt.title(cifar10_labels[bestclass] + \"!=\" + cifar10_labels[y_test[index][0]], color='#ff0000')\n",
        "        mistake += 1\n",
        "print(\"The number of correct answers:\", right)\n",
        "print(\"The number of mistake:\", mistake)\n",
        "print(\"A correct answer rate:\", right/(mistake + right)*100, '%')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of correct answers: 14\n",
            "The number of mistake: 86\n",
            "A correct answer rate: 14.000000000000002 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "aiyYQOO0z0AX",
        "outputId": "77a58972-8e4b-4146-a7de-1bace28cdb47"
      },
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate(X_train, Y_train, verbose=0)\n",
        "print('Train loss:', loss)\n",
        "print('Train acc:', acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train loss: 1.2827943950653076\n",
            "Train acc: 0.5569\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "colab_type": "code",
        "id": "_U1eErx6RMVf",
        "outputId": "77a91955-c78c-4efb-ef63-ceb59836f8c1"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.subplot(212)  \n",
        "plt.plot(history.history['loss'])  \n",
        "plt.plot(history.history['val_loss'])  \n",
        "plt.title('model loss')  \n",
        "plt.ylabel('loss')  \n",
        "plt.xlabel('epoch')  \n",
        "plt.legend(['train', 'test'], loc='upper left')  \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAADECAYAAAC2lamMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f1238562eb8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "igy2Bk_HyclF"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "colab_type": "code",
        "id": "ecXUpuxeQvgn",
        "outputId": "e98808b7-2e2c-4f81-c5d9-181c0e967ebe"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt \n",
        "plt.subplot(211)  \n",
        "plt.plot(history.history['acc'])  \n",
        "plt.plot(history.history['val_acc'])  \n",
        "plt.title('model accuracy')  \n",
        "plt.ylabel('accuracy')  \n",
        "plt.xlabel('epoch')  \n",
        "plt.legend(['train', 'test'], loc='upper left')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f12363cccc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAADECAYAAACGGbitAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noS4iIhIlFOoiIiJR4v8AdwMy2t7ZidoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f1238a0e550>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4g7_elUErfO3"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OIyeKJc0N6yZ"
      },
      "cell_type": "code",
      "source": [
        "# Modify the following parameters and discuss the effect of changing parameters on loss and\n",
        "#accuracy\n",
        "#1. No of epochs--60"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2567
        },
        "colab_type": "code",
        "id": "zBo-_DCDPRpF",
        "outputId": "b94f07d7-4a92-4ea7-aa79-281d7466058c"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    nb_epoch = 60\n",
        "    batch_size = 128\n",
        "    nb_classes = 10\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "    X_train = X_train.reshape(50000, 32 * 32 * 3)\n",
        "    X_test = X_test.reshape(10000, 32 * 32 * 3)\n",
        "\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "    X_train /= 255.0\n",
        "    X_test /= 255.0\n",
        "\n",
        "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "    Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "    # MLP2\n",
        "    model2 = Sequential()\n",
        "    model2.add(Dense(1024, input_shape=(3072, )))\n",
        "    model2.add(Activation('relu'))\n",
        "    model2.add(Dropout(0.2))\n",
        "    model2.add(Dense(512))\n",
        "    model2.add(Activation('relu'))\n",
        "    model2.add(Dropout(0.5))\n",
        "    model2.add(Dense(10))\n",
        "    model2.add(Activation('softmax'))\n",
        "\n",
        "    model2.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    model2.summary()\n",
        "\n",
        "    # training\n",
        "    history = model2.fit(X_train, Y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        nb_epoch=nb_epoch,\n",
        "                        verbose=1,\n",
        "                        validation_data=(X_test, Y_test))\n",
        "\n",
        "    save_history(history, 'history.txt')\n",
        "\n",
        "    loss2, acc2 = model2.evaluate(X_test, Y_test, verbose=0)\n",
        "    print('Test loss:', loss2)\n",
        "    print('Test acc:', acc2)\n",
        "    \n",
        "    \n",
        "    loss22, acc22 = model2.evaluate(X_test, Y_test, verbose=0)\n",
        "    print('Train loss:', loss22)\n",
        "    print('Train acc:', acc22)    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_7 (Dense)              (None, 1024)              3146752   \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 3,676,682\n",
            "Trainable params: 3,676,682\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/60\n",
            "50000/50000 [==============================] - 5s 108us/step - loss: 2.0049 - acc: 0.2699 - val_loss: 1.8118 - val_acc: 0.3484\n",
            "Epoch 2/60\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 1.8429 - acc: 0.3306 - val_loss: 1.7037 - val_acc: 0.3931\n",
            "Epoch 3/60\n",
            "50000/50000 [==============================] - 5s 98us/step - loss: 1.7829 - acc: 0.3566 - val_loss: 1.6707 - val_acc: 0.4038\n",
            "Epoch 4/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.7511 - acc: 0.3651 - val_loss: 1.6716 - val_acc: 0.4060\n",
            "Epoch 5/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.7320 - acc: 0.3740 - val_loss: 1.6468 - val_acc: 0.4150\n",
            "Epoch 6/60\n",
            "50000/50000 [==============================] - 5s 98us/step - loss: 1.6951 - acc: 0.3862 - val_loss: 1.6126 - val_acc: 0.4298\n",
            "Epoch 7/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.6808 - acc: 0.3920 - val_loss: 1.6213 - val_acc: 0.4240\n",
            "Epoch 8/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.6608 - acc: 0.4002 - val_loss: 1.5637 - val_acc: 0.4470\n",
            "Epoch 9/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.6557 - acc: 0.3990 - val_loss: 1.6026 - val_acc: 0.4273\n",
            "Epoch 10/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.6580 - acc: 0.3990 - val_loss: 1.5775 - val_acc: 0.4410\n",
            "Epoch 11/60\n",
            "50000/50000 [==============================] - 5s 98us/step - loss: 1.6382 - acc: 0.4102 - val_loss: 1.5634 - val_acc: 0.4443\n",
            "Epoch 12/60\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.6292 - acc: 0.4103 - val_loss: 1.5649 - val_acc: 0.4500\n",
            "Epoch 13/60\n",
            "50000/50000 [==============================] - 5s 98us/step - loss: 1.6192 - acc: 0.4148 - val_loss: 1.5631 - val_acc: 0.4403\n",
            "Epoch 14/60\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.6153 - acc: 0.4165 - val_loss: 1.5639 - val_acc: 0.4474\n",
            "Epoch 15/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.6018 - acc: 0.4211 - val_loss: 1.5294 - val_acc: 0.4606\n",
            "Epoch 16/60\n",
            "50000/50000 [==============================] - 5s 98us/step - loss: 1.5999 - acc: 0.4216 - val_loss: 1.5503 - val_acc: 0.4578\n",
            "Epoch 17/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.6020 - acc: 0.4231 - val_loss: 1.5137 - val_acc: 0.4636\n",
            "Epoch 18/60\n",
            "50000/50000 [==============================] - 5s 98us/step - loss: 1.5852 - acc: 0.4295 - val_loss: 1.5309 - val_acc: 0.4551\n",
            "Epoch 19/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.5809 - acc: 0.4307 - val_loss: 1.5063 - val_acc: 0.4652\n",
            "Epoch 20/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.5770 - acc: 0.4341 - val_loss: 1.5750 - val_acc: 0.4479\n",
            "Epoch 21/60\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.5682 - acc: 0.4374 - val_loss: 1.5180 - val_acc: 0.4587\n",
            "Epoch 22/60\n",
            "50000/50000 [==============================] - 5s 98us/step - loss: 1.5662 - acc: 0.4345 - val_loss: 1.5270 - val_acc: 0.4623\n",
            "Epoch 23/60\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.5591 - acc: 0.4371 - val_loss: 1.4967 - val_acc: 0.4675\n",
            "Epoch 24/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.5621 - acc: 0.4375 - val_loss: 1.5094 - val_acc: 0.4658\n",
            "Epoch 25/60\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.5570 - acc: 0.4388 - val_loss: 1.5134 - val_acc: 0.4680\n",
            "Epoch 26/60\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.5462 - acc: 0.4436 - val_loss: 1.5081 - val_acc: 0.4736\n",
            "Epoch 27/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.5374 - acc: 0.4464 - val_loss: 1.4886 - val_acc: 0.4848\n",
            "Epoch 28/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.5391 - acc: 0.4448 - val_loss: 1.4738 - val_acc: 0.4800\n",
            "Epoch 29/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.5293 - acc: 0.4468 - val_loss: 1.4884 - val_acc: 0.4774\n",
            "Epoch 30/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.5264 - acc: 0.4500 - val_loss: 1.5004 - val_acc: 0.4604\n",
            "Epoch 31/60\n",
            "50000/50000 [==============================] - 5s 98us/step - loss: 1.5239 - acc: 0.4508 - val_loss: 1.5047 - val_acc: 0.4638\n",
            "Epoch 32/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.5284 - acc: 0.4494 - val_loss: 1.5021 - val_acc: 0.4628\n",
            "Epoch 33/60\n",
            "50000/50000 [==============================] - 5s 98us/step - loss: 1.5233 - acc: 0.4499 - val_loss: 1.5054 - val_acc: 0.4676\n",
            "Epoch 34/60\n",
            "50000/50000 [==============================] - 5s 98us/step - loss: 1.5187 - acc: 0.4499 - val_loss: 1.4978 - val_acc: 0.4690\n",
            "Epoch 35/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.5104 - acc: 0.4559 - val_loss: 1.4832 - val_acc: 0.4770\n",
            "Epoch 36/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.5143 - acc: 0.4554 - val_loss: 1.4554 - val_acc: 0.4842\n",
            "Epoch 37/60\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.5148 - acc: 0.4546 - val_loss: 1.4771 - val_acc: 0.4755\n",
            "Epoch 38/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.5025 - acc: 0.4611 - val_loss: 1.4776 - val_acc: 0.4759\n",
            "Epoch 39/60\n",
            "50000/50000 [==============================] - 5s 98us/step - loss: 1.5033 - acc: 0.4582 - val_loss: 1.5014 - val_acc: 0.4628\n",
            "Epoch 40/60\n",
            "50000/50000 [==============================] - 5s 98us/step - loss: 1.4961 - acc: 0.4629 - val_loss: 1.4838 - val_acc: 0.4755\n",
            "Epoch 41/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.4953 - acc: 0.4593 - val_loss: 1.4718 - val_acc: 0.4746\n",
            "Epoch 42/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.4910 - acc: 0.4631 - val_loss: 1.4472 - val_acc: 0.4823\n",
            "Epoch 43/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.4934 - acc: 0.4611 - val_loss: 1.4701 - val_acc: 0.4797\n",
            "Epoch 44/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.4880 - acc: 0.4631 - val_loss: 1.4752 - val_acc: 0.4749\n",
            "Epoch 45/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.4902 - acc: 0.4593 - val_loss: 1.4612 - val_acc: 0.4836\n",
            "Epoch 46/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.4778 - acc: 0.4680 - val_loss: 1.4694 - val_acc: 0.4819\n",
            "Epoch 47/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.4890 - acc: 0.4621 - val_loss: 1.4769 - val_acc: 0.4737\n",
            "Epoch 48/60\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.4853 - acc: 0.4633 - val_loss: 1.5024 - val_acc: 0.4680\n",
            "Epoch 49/60\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.4750 - acc: 0.4678 - val_loss: 1.4549 - val_acc: 0.4777\n",
            "Epoch 50/60\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.4771 - acc: 0.4685 - val_loss: 1.4631 - val_acc: 0.4755\n",
            "Epoch 51/60\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.4807 - acc: 0.4665 - val_loss: 1.4870 - val_acc: 0.4703\n",
            "Epoch 52/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.4815 - acc: 0.4686 - val_loss: 1.4672 - val_acc: 0.4787\n",
            "Epoch 53/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.4750 - acc: 0.4699 - val_loss: 1.4642 - val_acc: 0.4826\n",
            "Epoch 54/60\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.4797 - acc: 0.4674 - val_loss: 1.4528 - val_acc: 0.4860\n",
            "Epoch 55/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.4575 - acc: 0.4747 - val_loss: 1.4682 - val_acc: 0.4703\n",
            "Epoch 56/60\n",
            "50000/50000 [==============================] - 5s 98us/step - loss: 1.4685 - acc: 0.4703 - val_loss: 1.4610 - val_acc: 0.4802\n",
            "Epoch 57/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.4678 - acc: 0.4701 - val_loss: 1.4506 - val_acc: 0.4826\n",
            "Epoch 58/60\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.4654 - acc: 0.4726 - val_loss: 1.4713 - val_acc: 0.4810\n",
            "Epoch 59/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.4602 - acc: 0.4731 - val_loss: 1.4600 - val_acc: 0.4820\n",
            "Epoch 60/60\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.4647 - acc: 0.4738 - val_loss: 1.4634 - val_acc: 0.4881\n",
            "Test loss: 1.4634426864624024\n",
            "Test acc: 0.4881\n",
            "Train loss: 1.4634426864624024\n",
            "Train acc: 0.4881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "z_Hvb0QorjnN",
        "outputId": "8a96ac92-6d39-42c8-dde5-cafdad66a791"
      },
      "cell_type": "code",
      "source": [
        "def convertCIFER10Data(image):\n",
        "    img = image.astype('float32')\n",
        "    img /= 255\n",
        "    c = np.zeros(3072).reshape((1,3072))\n",
        "    c[0] = img\n",
        "    return c\n",
        "  \n",
        "right = 0\n",
        "mistake = 0\n",
        "import random\n",
        "for i in range(100):\n",
        "    index = random.randint(0, X_test.shape[0])\n",
        "    image = X_test[index]\n",
        "    #print(index)\n",
        "    data = convertCIFER10Data(image)\n",
        "\n",
        "  \n",
        "    ret = model.predict(data, batch_size=1) \n",
        "    #print(ret)\n",
        "\n",
        "    bestnum = 0.0\n",
        "    bestclass = 0\n",
        "    for n in [0,1,2,3,4,5,6,7,8,9]:\n",
        "        if bestnum < ret[0][n]:\n",
        "            bestnum = ret[0][n]\n",
        "            bestclass = n\n",
        "    i = np.array(list(Y_test[index]).index(1))\n",
        "    if i == bestclass:\n",
        "        #print(i)\n",
        "        #plt.title(cifar10_labels[bestclass])\n",
        "        right += 1\n",
        "    else:\n",
        "        #plt.title(cifar10_labels[bestclass] + \"!=\" + cifar10_labels[y_test[index][0]], color='#ff0000')\n",
        "        mistake += 1\n",
        "print(\"The number of correct answers:\", right)\n",
        "print(\"The number of mistake:\", mistake)\n",
        "print(\"A correct answer rate:\", right/(mistake + right)*100, '%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of correct answers: 12\n",
            "The number of mistake: 88\n",
            "A correct answer rate: 12.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Ro8eSw8Pn1kb"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "REvWVPqGvBEG"
      },
      "cell_type": "code",
      "source": [
        "#2. Batch size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3927
        },
        "colab_type": "code",
        "id": "WVSSsFe9u-cM",
        "outputId": "b177663e-f115-455c-cebf-16c66b4d10dc"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    nb_epoch = 100\n",
        "    batch_size = 256\n",
        "    nb_classes = 10\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "    X_train = X_train.reshape(50000, 32 * 32 * 3)\n",
        "    X_test = X_test.reshape(10000, 32 * 32 * 3)\n",
        "\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "    X_train /= 255.0\n",
        "    X_test /= 255.0\n",
        "\n",
        "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "    Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "    # MLP\n",
        "    model3 = Sequential()\n",
        "    model3.add(Dense(1024, input_shape=(3072, )))\n",
        "    model3.add(Activation('relu'))\n",
        "    model3.add(Dropout(0.2))\n",
        "    model3.add(Dense(512))\n",
        "    model3.add(Activation('relu'))\n",
        "    model3.add(Dropout(0.5))\n",
        "    model3.add(Dense(10))\n",
        "    model3.add(Activation('softmax'))\n",
        "\n",
        "    model3.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    model3.summary()\n",
        "\n",
        "    # training\n",
        "    history = model3.fit(X_train, Y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        nb_epoch=nb_epoch,\n",
        "                        verbose=1,\n",
        "                        validation_data=(X_test, Y_test))\n",
        "\n",
        "    save_history(history, 'history.txt')\n",
        "\n",
        "    loss, acc = model3.evaluate(X_test, Y_test, verbose=0)\n",
        "    print('Test loss:', loss)\n",
        "    print('Test acc:', acc)\n",
        "    \n",
        "train_loss,train_acc=model3.evaluate(X_train,Y_train,verbose=0)\n",
        "print('Train loss:', train_loss)\n",
        "print('Train acc:', train_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 1024)              3146752   \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 3,676,682\n",
            "Trainable params: 3,676,682\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "50000/50000 [==============================] - 3s 69us/step - loss: 2.0409 - acc: 0.2613 - val_loss: 1.8270 - val_acc: 0.3578\n",
            "Epoch 2/100\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.8232 - acc: 0.3409 - val_loss: 1.7084 - val_acc: 0.3947\n",
            "Epoch 3/100\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.7626 - acc: 0.3660 - val_loss: 1.6890 - val_acc: 0.3996\n",
            "Epoch 4/100\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.7190 - acc: 0.3864 - val_loss: 1.5958 - val_acc: 0.4418\n",
            "Epoch 5/100\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.6899 - acc: 0.3926 - val_loss: 1.6329 - val_acc: 0.4191\n",
            "Epoch 6/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.6691 - acc: 0.3999 - val_loss: 1.5925 - val_acc: 0.4390\n",
            "Epoch 7/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.6530 - acc: 0.4069 - val_loss: 1.5523 - val_acc: 0.4522\n",
            "Epoch 8/100\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.6178 - acc: 0.4177 - val_loss: 1.5496 - val_acc: 0.4594\n",
            "Epoch 9/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.6138 - acc: 0.4179 - val_loss: 1.5293 - val_acc: 0.4638\n",
            "Epoch 10/100\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.6016 - acc: 0.4248 - val_loss: 1.5164 - val_acc: 0.4584\n",
            "Epoch 11/100\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.5781 - acc: 0.4327 - val_loss: 1.5129 - val_acc: 0.4636\n",
            "Epoch 12/100\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.5712 - acc: 0.4344 - val_loss: 1.4942 - val_acc: 0.4771\n",
            "Epoch 13/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.5601 - acc: 0.4409 - val_loss: 1.5046 - val_acc: 0.4763\n",
            "Epoch 14/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.5557 - acc: 0.4414 - val_loss: 1.4848 - val_acc: 0.4709\n",
            "Epoch 15/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.5454 - acc: 0.4444 - val_loss: 1.4812 - val_acc: 0.4760\n",
            "Epoch 16/100\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.5381 - acc: 0.4470 - val_loss: 1.4744 - val_acc: 0.4778\n",
            "Epoch 17/100\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.5390 - acc: 0.4462 - val_loss: 1.4787 - val_acc: 0.4691\n",
            "Epoch 18/100\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.5177 - acc: 0.4535 - val_loss: 1.4429 - val_acc: 0.4877\n",
            "Epoch 19/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.5151 - acc: 0.4580 - val_loss: 1.4586 - val_acc: 0.4768\n",
            "Epoch 20/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.5139 - acc: 0.4567 - val_loss: 1.4743 - val_acc: 0.4831\n",
            "Epoch 21/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.5023 - acc: 0.4628 - val_loss: 1.4525 - val_acc: 0.4914\n",
            "Epoch 22/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.4995 - acc: 0.4612 - val_loss: 1.4336 - val_acc: 0.4928\n",
            "Epoch 23/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.4868 - acc: 0.4660 - val_loss: 1.4460 - val_acc: 0.4851\n",
            "Epoch 24/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.4792 - acc: 0.4695 - val_loss: 1.4645 - val_acc: 0.4792\n",
            "Epoch 25/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.4749 - acc: 0.4667 - val_loss: 1.4284 - val_acc: 0.4900\n",
            "Epoch 26/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.4802 - acc: 0.4690 - val_loss: 1.4363 - val_acc: 0.4873\n",
            "Epoch 27/100\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.4774 - acc: 0.4682 - val_loss: 1.4412 - val_acc: 0.4852\n",
            "Epoch 28/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.4625 - acc: 0.4738 - val_loss: 1.4260 - val_acc: 0.4918\n",
            "Epoch 29/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.4634 - acc: 0.4700 - val_loss: 1.4455 - val_acc: 0.4912\n",
            "Epoch 30/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.4546 - acc: 0.4774 - val_loss: 1.4223 - val_acc: 0.4992\n",
            "Epoch 31/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.4588 - acc: 0.4762 - val_loss: 1.4257 - val_acc: 0.4997\n",
            "Epoch 32/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.4460 - acc: 0.4802 - val_loss: 1.4195 - val_acc: 0.4993\n",
            "Epoch 33/100\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.4419 - acc: 0.4819 - val_loss: 1.4272 - val_acc: 0.4964\n",
            "Epoch 34/100\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.4493 - acc: 0.4808 - val_loss: 1.4124 - val_acc: 0.4989\n",
            "Epoch 35/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.4340 - acc: 0.4838 - val_loss: 1.4317 - val_acc: 0.4925\n",
            "Epoch 36/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.4302 - acc: 0.4853 - val_loss: 1.4149 - val_acc: 0.4982\n",
            "Epoch 37/100\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.4251 - acc: 0.4910 - val_loss: 1.4184 - val_acc: 0.4991\n",
            "Epoch 38/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.4206 - acc: 0.4893 - val_loss: 1.4106 - val_acc: 0.4989\n",
            "Epoch 39/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.4257 - acc: 0.4902 - val_loss: 1.4079 - val_acc: 0.5087\n",
            "Epoch 40/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.4244 - acc: 0.4893 - val_loss: 1.4056 - val_acc: 0.5013\n",
            "Epoch 41/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.4058 - acc: 0.4934 - val_loss: 1.4062 - val_acc: 0.4999\n",
            "Epoch 42/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.4163 - acc: 0.4912 - val_loss: 1.4014 - val_acc: 0.5017\n",
            "Epoch 43/100\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.4065 - acc: 0.4954 - val_loss: 1.4245 - val_acc: 0.5004\n",
            "Epoch 44/100\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.4083 - acc: 0.4920 - val_loss: 1.4195 - val_acc: 0.5017\n",
            "Epoch 45/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3986 - acc: 0.4980 - val_loss: 1.3893 - val_acc: 0.5068\n",
            "Epoch 46/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.4012 - acc: 0.4967 - val_loss: 1.4311 - val_acc: 0.4958\n",
            "Epoch 47/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.4068 - acc: 0.4953 - val_loss: 1.4125 - val_acc: 0.5049\n",
            "Epoch 48/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3962 - acc: 0.4987 - val_loss: 1.3842 - val_acc: 0.5101\n",
            "Epoch 49/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3887 - acc: 0.5013 - val_loss: 1.4119 - val_acc: 0.5071\n",
            "Epoch 50/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3910 - acc: 0.5001 - val_loss: 1.4031 - val_acc: 0.5042\n",
            "Epoch 51/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3915 - acc: 0.5005 - val_loss: 1.4058 - val_acc: 0.5036\n",
            "Epoch 52/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3833 - acc: 0.5055 - val_loss: 1.3990 - val_acc: 0.5059\n",
            "Epoch 53/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3791 - acc: 0.5051 - val_loss: 1.4018 - val_acc: 0.5051\n",
            "Epoch 54/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3760 - acc: 0.5061 - val_loss: 1.3914 - val_acc: 0.5053\n",
            "Epoch 55/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3764 - acc: 0.5080 - val_loss: 1.3828 - val_acc: 0.5141\n",
            "Epoch 56/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3665 - acc: 0.5089 - val_loss: 1.4006 - val_acc: 0.5079\n",
            "Epoch 57/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3723 - acc: 0.5077 - val_loss: 1.3810 - val_acc: 0.5124\n",
            "Epoch 58/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3697 - acc: 0.5079 - val_loss: 1.4127 - val_acc: 0.5014\n",
            "Epoch 59/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3778 - acc: 0.5062 - val_loss: 1.4053 - val_acc: 0.5078\n",
            "Epoch 60/100\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.3656 - acc: 0.5083 - val_loss: 1.4191 - val_acc: 0.4912\n",
            "Epoch 61/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3662 - acc: 0.5098 - val_loss: 1.3846 - val_acc: 0.5103\n",
            "Epoch 62/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3641 - acc: 0.5125 - val_loss: 1.3840 - val_acc: 0.5039\n",
            "Epoch 63/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3583 - acc: 0.5144 - val_loss: 1.3872 - val_acc: 0.5128\n",
            "Epoch 64/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3606 - acc: 0.5118 - val_loss: 1.3844 - val_acc: 0.5109\n",
            "Epoch 65/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3471 - acc: 0.5153 - val_loss: 1.3926 - val_acc: 0.5061\n",
            "Epoch 66/100\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.3444 - acc: 0.5185 - val_loss: 1.4065 - val_acc: 0.5059\n",
            "Epoch 67/100\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.3510 - acc: 0.5146 - val_loss: 1.4224 - val_acc: 0.4998\n",
            "Epoch 68/100\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.3484 - acc: 0.5178 - val_loss: 1.4017 - val_acc: 0.5035\n",
            "Epoch 69/100\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.3478 - acc: 0.5175 - val_loss: 1.4038 - val_acc: 0.5060\n",
            "Epoch 70/100\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.3475 - acc: 0.5188 - val_loss: 1.3851 - val_acc: 0.5082\n",
            "Epoch 71/100\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.3435 - acc: 0.5187 - val_loss: 1.4011 - val_acc: 0.5062\n",
            "Epoch 72/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3442 - acc: 0.5166 - val_loss: 1.3975 - val_acc: 0.5067\n",
            "Epoch 73/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3379 - acc: 0.5181 - val_loss: 1.3931 - val_acc: 0.5076\n",
            "Epoch 74/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3410 - acc: 0.5176 - val_loss: 1.3912 - val_acc: 0.5081\n",
            "Epoch 75/100\n",
            "50000/50000 [==============================] - 3s 52us/step - loss: 1.3253 - acc: 0.5244 - val_loss: 1.4072 - val_acc: 0.5029\n",
            "Epoch 76/100\n",
            "50000/50000 [==============================] - 3s 52us/step - loss: 1.3262 - acc: 0.5223 - val_loss: 1.3949 - val_acc: 0.5086\n",
            "Epoch 77/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3285 - acc: 0.5214 - val_loss: 1.3787 - val_acc: 0.5193\n",
            "Epoch 78/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3331 - acc: 0.5194 - val_loss: 1.3849 - val_acc: 0.5120\n",
            "Epoch 79/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3243 - acc: 0.5231 - val_loss: 1.4012 - val_acc: 0.5017\n",
            "Epoch 80/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3246 - acc: 0.5232 - val_loss: 1.3711 - val_acc: 0.5211\n",
            "Epoch 81/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3273 - acc: 0.5222 - val_loss: 1.3787 - val_acc: 0.5149\n",
            "Epoch 82/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3219 - acc: 0.5255 - val_loss: 1.4100 - val_acc: 0.5048\n",
            "Epoch 83/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3260 - acc: 0.5251 - val_loss: 1.3738 - val_acc: 0.5166\n",
            "Epoch 84/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3168 - acc: 0.5271 - val_loss: 1.3898 - val_acc: 0.5093\n",
            "Epoch 85/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3152 - acc: 0.5266 - val_loss: 1.3813 - val_acc: 0.5128\n",
            "Epoch 86/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3080 - acc: 0.5293 - val_loss: 1.3930 - val_acc: 0.5111\n",
            "Epoch 87/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3119 - acc: 0.5288 - val_loss: 1.3923 - val_acc: 0.5111\n",
            "Epoch 88/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3142 - acc: 0.5284 - val_loss: 1.3689 - val_acc: 0.5154\n",
            "Epoch 89/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3133 - acc: 0.5275 - val_loss: 1.3863 - val_acc: 0.5166\n",
            "Epoch 90/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3055 - acc: 0.5298 - val_loss: 1.3735 - val_acc: 0.5115\n",
            "Epoch 91/100\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.3107 - acc: 0.5272 - val_loss: 1.3748 - val_acc: 0.5114\n",
            "Epoch 92/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3101 - acc: 0.5281 - val_loss: 1.3735 - val_acc: 0.5200\n",
            "Epoch 93/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3038 - acc: 0.5303 - val_loss: 1.3760 - val_acc: 0.5189\n",
            "Epoch 94/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3007 - acc: 0.5287 - val_loss: 1.3899 - val_acc: 0.5088\n",
            "Epoch 95/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.2965 - acc: 0.5317 - val_loss: 1.3920 - val_acc: 0.5070\n",
            "Epoch 96/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.3000 - acc: 0.5347 - val_loss: 1.3811 - val_acc: 0.5133\n",
            "Epoch 97/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.2909 - acc: 0.5380 - val_loss: 1.3842 - val_acc: 0.5129\n",
            "Epoch 98/100\n",
            "50000/50000 [==============================] - 3s 52us/step - loss: 1.3033 - acc: 0.5327 - val_loss: 1.3951 - val_acc: 0.5027\n",
            "Epoch 99/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.2946 - acc: 0.5345 - val_loss: 1.3819 - val_acc: 0.5137\n",
            "Epoch 100/100\n",
            "50000/50000 [==============================] - 3s 53us/step - loss: 1.2896 - acc: 0.5374 - val_loss: 1.4196 - val_acc: 0.5000\n",
            "Test loss: 1.4196208274841309\n",
            "Test acc: 0.5\n",
            "Train loss: 1.1424696315765381\n",
            "Train acc: 0.60242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "oA_qSY7VscZ1",
        "outputId": "3e47029d-581b-4281-a49f-47f7b8788647"
      },
      "cell_type": "code",
      "source": [
        "def convertCIFER10Data(image):\n",
        "    img = image.astype('float32')\n",
        "    img /= 255\n",
        "    c = np.zeros(3072).reshape((1,3072))\n",
        "    c[0] = img\n",
        "    return c\n",
        "  \n",
        "right = 0\n",
        "mistake = 0\n",
        "import random\n",
        "for i in range(100):\n",
        "    index = random.randint(0, X_test.shape[0])\n",
        "    image = X_test[index]\n",
        "    #print(index)\n",
        "    data = convertCIFER10Data(image)\n",
        "\n",
        "  \n",
        "    ret = model.predict(data, batch_size=1) \n",
        "    #print(ret)\n",
        "\n",
        "    bestnum = 0.0\n",
        "    bestclass = 0\n",
        "    for n in [0,1,2,3,4,5,6,7,8,9]:\n",
        "        if bestnum < ret[0][n]:\n",
        "            bestnum = ret[0][n]\n",
        "            bestclass = n\n",
        "    i = np.array(list(Y_test[index]).index(1))\n",
        "    if i == bestclass:\n",
        "        #print(i)\n",
        "        #plt.title(cifar10_labels[bestclass])\n",
        "        right += 1\n",
        "    else:\n",
        "        #plt.title(cifar10_labels[bestclass] + \"!=\" + cifar10_labels[y_test[index][0]], color='#ff0000')\n",
        "        mistake += 1\n",
        "print(\"The number of correct answers:\", right)\n",
        "print(\"The number of mistake:\", mistake)\n",
        "print(\"A correct answer rate:\", right/(mistake + right)*100, '%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of correct answers: 3\n",
            "The number of mistake: 97\n",
            "A correct answer rate: 3.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2584
        },
        "colab_type": "code",
        "id": "3dE_ij-quuku",
        "outputId": "1fc99912-a856-490d-cdb3-0a8f70ec34b4"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    nb_epoch = 60\n",
        "    batch_size = 128\n",
        "    nb_classes = 10\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "    X_train = X_train.reshape(50000, 32 * 32 * 3)\n",
        "    X_test = X_test.reshape(10000, 32 * 32 * 3)\n",
        "\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "    X_train /= 255.0\n",
        "    X_test /= 255.0\n",
        "\n",
        "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "    Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "    # MLP\n",
        "    model4 = Sequential()\n",
        "    model4.add(Dense(1024, input_shape=(3072, )))\n",
        "    model4.add(Activation('relu'))\n",
        "    model4.add(Dropout(0.2))\n",
        "    model4.add(Dense(512))\n",
        "    model4.add(Activation('relu'))\n",
        "    model4.add(Dropout(0.5))\n",
        "    model4.add(Dense(10))\n",
        "    model4.add(Activation('softmax'))\n",
        "    opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
        "    model4.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=opt,\n",
        "                  metrics=['accuracy'])\n",
        "    model4.summary()\n",
        "\n",
        "    # training\n",
        "    history = model4.fit(X_train, Y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        nb_epoch=nb_epoch,\n",
        "                        verbose=1,\n",
        "                        validation_data=(X_test, Y_test))\n",
        "\n",
        "    save_history(history, 'history.txt')\n",
        "\n",
        "    loss, acc = model4.evaluate(X_test, Y_test, verbose=0)\n",
        "    print('Test loss:', loss)\n",
        "    print('Test acc:', acc)\n",
        "    \n",
        "    loss_t,acc_t =model4.evaluate(X_train, Y_train, verbose=0)\n",
        "    print('Train loss:', loss_t)\n",
        "    print('Train acc:', acc_t)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_13 (Dense)             (None, 1024)              3146752   \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 3,676,682\n",
            "Trainable params: 3,676,682\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/60\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 1.9921 - acc: 0.2746 - val_loss: 1.8318 - val_acc: 0.3528\n",
            "Epoch 2/60\n",
            "50000/50000 [==============================] - 4s 89us/step - loss: 1.8128 - acc: 0.3525 - val_loss: 1.7289 - val_acc: 0.3915\n",
            "Epoch 3/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.7371 - acc: 0.3829 - val_loss: 1.6393 - val_acc: 0.4240\n",
            "Epoch 4/60\n",
            "50000/50000 [==============================] - 4s 89us/step - loss: 1.6869 - acc: 0.3992 - val_loss: 1.5948 - val_acc: 0.4354\n",
            "Epoch 5/60\n",
            "50000/50000 [==============================] - 4s 90us/step - loss: 1.6448 - acc: 0.4169 - val_loss: 1.6075 - val_acc: 0.4262\n",
            "Epoch 6/60\n",
            "50000/50000 [==============================] - 4s 89us/step - loss: 1.6078 - acc: 0.4323 - val_loss: 1.5746 - val_acc: 0.4448\n",
            "Epoch 7/60\n",
            "50000/50000 [==============================] - 4s 89us/step - loss: 1.5757 - acc: 0.4407 - val_loss: 1.5200 - val_acc: 0.4555\n",
            "Epoch 8/60\n",
            "50000/50000 [==============================] - 4s 89us/step - loss: 1.5534 - acc: 0.4494 - val_loss: 1.5021 - val_acc: 0.4662\n",
            "Epoch 9/60\n",
            "50000/50000 [==============================] - 4s 89us/step - loss: 1.5268 - acc: 0.4613 - val_loss: 1.5323 - val_acc: 0.4686\n",
            "Epoch 10/60\n",
            "50000/50000 [==============================] - 4s 89us/step - loss: 1.5056 - acc: 0.4696 - val_loss: 1.4795 - val_acc: 0.4697\n",
            "Epoch 11/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.4837 - acc: 0.4760 - val_loss: 1.4299 - val_acc: 0.4895\n",
            "Epoch 12/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.4591 - acc: 0.4848 - val_loss: 1.4457 - val_acc: 0.4845\n",
            "Epoch 13/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.4482 - acc: 0.4876 - val_loss: 1.4266 - val_acc: 0.4959\n",
            "Epoch 14/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.4302 - acc: 0.4963 - val_loss: 1.4143 - val_acc: 0.4934\n",
            "Epoch 15/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.4113 - acc: 0.5016 - val_loss: 1.4462 - val_acc: 0.4930\n",
            "Epoch 16/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.3992 - acc: 0.5052 - val_loss: 1.3835 - val_acc: 0.5158\n",
            "Epoch 17/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.3790 - acc: 0.5131 - val_loss: 1.4157 - val_acc: 0.4959\n",
            "Epoch 18/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.3706 - acc: 0.5158 - val_loss: 1.3839 - val_acc: 0.5124\n",
            "Epoch 19/60\n",
            "50000/50000 [==============================] - 4s 87us/step - loss: 1.3514 - acc: 0.5227 - val_loss: 1.3875 - val_acc: 0.5013\n",
            "Epoch 20/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.3411 - acc: 0.5251 - val_loss: 1.3698 - val_acc: 0.5090\n",
            "Epoch 21/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.3285 - acc: 0.5315 - val_loss: 1.3404 - val_acc: 0.5181\n",
            "Epoch 22/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.3098 - acc: 0.5385 - val_loss: 1.3865 - val_acc: 0.5209\n",
            "Epoch 23/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.3000 - acc: 0.5426 - val_loss: 1.4490 - val_acc: 0.4869\n",
            "Epoch 24/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.2913 - acc: 0.5447 - val_loss: 1.3973 - val_acc: 0.5091\n",
            "Epoch 25/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.2740 - acc: 0.5490 - val_loss: 1.3604 - val_acc: 0.5107\n",
            "Epoch 26/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.2665 - acc: 0.5512 - val_loss: 1.3391 - val_acc: 0.5251\n",
            "Epoch 27/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.2534 - acc: 0.5577 - val_loss: 1.4322 - val_acc: 0.5042\n",
            "Epoch 28/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.2426 - acc: 0.5626 - val_loss: 1.3986 - val_acc: 0.5022\n",
            "Epoch 29/60\n",
            "50000/50000 [==============================] - 4s 89us/step - loss: 1.2342 - acc: 0.5650 - val_loss: 1.3760 - val_acc: 0.5151\n",
            "Epoch 30/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.2230 - acc: 0.5670 - val_loss: 1.3455 - val_acc: 0.5219\n",
            "Epoch 31/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.2131 - acc: 0.5730 - val_loss: 1.3034 - val_acc: 0.5445\n",
            "Epoch 32/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.2058 - acc: 0.5750 - val_loss: 1.3317 - val_acc: 0.5353\n",
            "Epoch 33/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.1946 - acc: 0.5789 - val_loss: 1.2827 - val_acc: 0.5471\n",
            "Epoch 34/60\n",
            "50000/50000 [==============================] - 4s 89us/step - loss: 1.1835 - acc: 0.5827 - val_loss: 1.3605 - val_acc: 0.5176\n",
            "Epoch 35/60\n",
            "50000/50000 [==============================] - 4s 89us/step - loss: 1.1793 - acc: 0.5819 - val_loss: 1.3319 - val_acc: 0.5300\n",
            "Epoch 36/60\n",
            "50000/50000 [==============================] - 4s 89us/step - loss: 1.1592 - acc: 0.5905 - val_loss: 1.4594 - val_acc: 0.5050\n",
            "Epoch 37/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.1598 - acc: 0.5913 - val_loss: 1.3000 - val_acc: 0.5401\n",
            "Epoch 38/60\n",
            "50000/50000 [==============================] - 4s 89us/step - loss: 1.1444 - acc: 0.5936 - val_loss: 1.2906 - val_acc: 0.5484\n",
            "Epoch 39/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.1357 - acc: 0.5996 - val_loss: 1.2937 - val_acc: 0.5390\n",
            "Epoch 40/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.1294 - acc: 0.6003 - val_loss: 1.3052 - val_acc: 0.5478\n",
            "Epoch 41/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.1123 - acc: 0.6074 - val_loss: 1.3682 - val_acc: 0.5253\n",
            "Epoch 42/60\n",
            "50000/50000 [==============================] - 5s 90us/step - loss: 1.1058 - acc: 0.6083 - val_loss: 1.3160 - val_acc: 0.5403\n",
            "Epoch 43/60\n",
            "50000/50000 [==============================] - 5s 92us/step - loss: 1.1022 - acc: 0.6079 - val_loss: 1.3756 - val_acc: 0.5189\n",
            "Epoch 44/60\n",
            "50000/50000 [==============================] - 5s 92us/step - loss: 1.0954 - acc: 0.6130 - val_loss: 1.2821 - val_acc: 0.5534\n",
            "Epoch 45/60\n",
            "50000/50000 [==============================] - 5s 92us/step - loss: 1.0822 - acc: 0.6153 - val_loss: 1.3174 - val_acc: 0.5449\n",
            "Epoch 46/60\n",
            "50000/50000 [==============================] - 4s 87us/step - loss: 1.0764 - acc: 0.6189 - val_loss: 1.2955 - val_acc: 0.5480\n",
            "Epoch 47/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.0675 - acc: 0.6226 - val_loss: 1.3095 - val_acc: 0.5466\n",
            "Epoch 48/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.0603 - acc: 0.6252 - val_loss: 1.3132 - val_acc: 0.5487\n",
            "Epoch 49/60\n",
            "50000/50000 [==============================] - 4s 89us/step - loss: 1.0512 - acc: 0.6266 - val_loss: 1.2868 - val_acc: 0.5451\n",
            "Epoch 50/60\n",
            "50000/50000 [==============================] - 4s 87us/step - loss: 1.0422 - acc: 0.6326 - val_loss: 1.3175 - val_acc: 0.5493\n",
            "Epoch 51/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.0364 - acc: 0.6340 - val_loss: 1.2922 - val_acc: 0.5548\n",
            "Epoch 52/60\n",
            "50000/50000 [==============================] - 4s 89us/step - loss: 1.0268 - acc: 0.6354 - val_loss: 1.3325 - val_acc: 0.5366\n",
            "Epoch 53/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.0181 - acc: 0.6429 - val_loss: 1.2812 - val_acc: 0.5548\n",
            "Epoch 54/60\n",
            "50000/50000 [==============================] - 4s 87us/step - loss: 1.0123 - acc: 0.6395 - val_loss: 1.2899 - val_acc: 0.5581\n",
            "Epoch 55/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 1.0046 - acc: 0.6448 - val_loss: 1.2994 - val_acc: 0.5400\n",
            "Epoch 56/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 0.9964 - acc: 0.6462 - val_loss: 1.3035 - val_acc: 0.5504\n",
            "Epoch 57/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 0.9896 - acc: 0.6517 - val_loss: 1.3311 - val_acc: 0.5485\n",
            "Epoch 58/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 0.9819 - acc: 0.6520 - val_loss: 1.3015 - val_acc: 0.5543\n",
            "Epoch 59/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 0.9743 - acc: 0.6524 - val_loss: 1.4358 - val_acc: 0.5288\n",
            "Epoch 60/60\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 0.9737 - acc: 0.6551 - val_loss: 1.3094 - val_acc: 0.5529\n",
            "Test loss: 1.309408580970764\n",
            "Test acc: 0.5529\n",
            "Train loss: 0.8316176794624328\n",
            "Train acc: 0.71364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "QkVK0bpptFgE",
        "outputId": "90117ce9-f1d0-4f86-c7de-ca1af87427d3"
      },
      "cell_type": "code",
      "source": [
        "def convertCIFER10Data(image):\n",
        "    img = image.astype('float32')\n",
        "    img /= 255\n",
        "    c = np.zeros(3072).reshape((1,3072))\n",
        "    c[0] = img\n",
        "    return c\n",
        "  \n",
        "right = 0\n",
        "mistake = 0\n",
        "import random\n",
        "for i in range(100):\n",
        "    index = random.randint(0, X_test.shape[0])\n",
        "    image = X_test[index]\n",
        "    #print(index)\n",
        "    data = convertCIFER10Data(image)\n",
        "\n",
        "  \n",
        "    ret = model.predict(data, batch_size=1) \n",
        "    #print(ret)\n",
        "\n",
        "    bestnum = 0.0\n",
        "    bestclass = 0\n",
        "    for n in [0,1,2,3,4,5,6,7,8,9]:\n",
        "        if bestnum < ret[0][n]:\n",
        "            bestnum = ret[0][n]\n",
        "            bestclass = n\n",
        "    i = np.array(list(Y_test[index]).index(1))\n",
        "    if i == bestclass:\n",
        "        #print(i)\n",
        "        #plt.title(cifar10_labels[bestclass])\n",
        "        right += 1\n",
        "    else:\n",
        "        #plt.title(cifar10_labels[bestclass] + \"!=\" + cifar10_labels[y_test[index][0]], color='#ff0000')\n",
        "        mistake += 1\n",
        "print(\"The number of correct answers:\", right)\n",
        "print(\"The number of mistake:\", mistake)\n",
        "print(\"A correct answer rate:\", right/(mistake + right)*100, '%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of correct answers: 12\n",
            "The number of mistake: 88\n",
            "A correct answer rate: 12.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7uWiicnyKBlF"
      },
      "cell_type": "code",
      "source": [
        "#3. Network configuration\n",
        "#a. Number of neurons in a layer\n",
        "#b. Number of layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7361
        },
        "colab_type": "code",
        "id": "XIIiDNa8D5Sd",
        "outputId": "9417bf56-6fdb-44f6-f7bf-e07c2904e86b"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    nb_epoch = 200\n",
        "    batch_size = 128\n",
        "    nb_classes = 10\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "    X_train = X_train.reshape(50000, 32 * 32 * 3)\n",
        "    X_test = X_test.reshape(10000, 32 * 32 * 3)\n",
        "\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "    X_train /= 255.0\n",
        "    X_test /= 255.0\n",
        "\n",
        "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "    Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "    # MLP\n",
        "    model5 = Sequential()\n",
        "    model5.add(Dense(1024, input_shape=(3072, )))\n",
        "    model5.add(Activation('relu'))\n",
        "    model5.add(Dropout(0.2))\n",
        "    \n",
        "              \n",
        "    model5.add(Dense(512))\n",
        "    model5.add(Activation('relu'))\n",
        "    model5.add(Dropout(0.5))\n",
        "    model5.add(Dense(512))\n",
        "    model5.add(Activation('relu'))\n",
        "    model5.add(Dropout(0.2))\n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    # training\n",
        "    history = model.fit(X_train, Y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        nb_epoch=nb_epoch,\n",
        "                        verbose=1,\n",
        "                        validation_data=(X_test, Y_test))\n",
        "\n",
        "    save_history(history, 'history.txt')\n",
        "\n",
        "    loss, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
        "    print('Test loss:', loss)\n",
        "    print('Test acc:', acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 1024)              3146752   \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 3,676,792\n",
            "Trainable params: 3,676,792\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/200\n",
            "50000/50000 [==============================] - 6s 121us/step - loss: 2.1261 - acc: 0.2624 - val_loss: 2.0244 - val_acc: 0.3285\n",
            "Epoch 2/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.9587 - acc: 0.3380 - val_loss: 1.8953 - val_acc: 0.3591\n",
            "Epoch 3/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.8534 - acc: 0.3639 - val_loss: 1.8089 - val_acc: 0.3735\n",
            "Epoch 4/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.7782 - acc: 0.3782 - val_loss: 1.7400 - val_acc: 0.4006\n",
            "Epoch 5/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.7268 - acc: 0.3938 - val_loss: 1.7124 - val_acc: 0.4028\n",
            "Epoch 6/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.6921 - acc: 0.4037 - val_loss: 1.6815 - val_acc: 0.4148\n",
            "Epoch 7/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.6658 - acc: 0.4114 - val_loss: 1.6457 - val_acc: 0.4471\n",
            "Epoch 8/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.6413 - acc: 0.4259 - val_loss: 1.6272 - val_acc: 0.4477\n",
            "Epoch 9/200\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.6187 - acc: 0.4345 - val_loss: 1.6139 - val_acc: 0.4510\n",
            "Epoch 10/200\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.6061 - acc: 0.4365 - val_loss: 1.5972 - val_acc: 0.4492\n",
            "Epoch 11/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.5851 - acc: 0.4471 - val_loss: 1.5970 - val_acc: 0.4488\n",
            "Epoch 12/200\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.5749 - acc: 0.4512 - val_loss: 1.5743 - val_acc: 0.4649\n",
            "Epoch 13/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.5633 - acc: 0.4567 - val_loss: 1.5718 - val_acc: 0.4633\n",
            "Epoch 14/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.5504 - acc: 0.4609 - val_loss: 1.5493 - val_acc: 0.4717\n",
            "Epoch 15/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.5467 - acc: 0.4600 - val_loss: 1.5697 - val_acc: 0.4580\n",
            "Epoch 16/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.5407 - acc: 0.4609 - val_loss: 1.5373 - val_acc: 0.4674\n",
            "Epoch 17/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.5300 - acc: 0.4647 - val_loss: 1.5470 - val_acc: 0.4584\n",
            "Epoch 18/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.5320 - acc: 0.4671 - val_loss: 1.5402 - val_acc: 0.4666\n",
            "Epoch 19/200\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.5189 - acc: 0.4702 - val_loss: 1.5209 - val_acc: 0.4702\n",
            "Epoch 20/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.5143 - acc: 0.4676 - val_loss: 1.5632 - val_acc: 0.4487\n",
            "Epoch 21/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.5103 - acc: 0.4738 - val_loss: 1.5385 - val_acc: 0.4659\n",
            "Epoch 22/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.5118 - acc: 0.4707 - val_loss: 1.5488 - val_acc: 0.4547\n",
            "Epoch 23/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4938 - acc: 0.4765 - val_loss: 1.5236 - val_acc: 0.4696\n",
            "Epoch 24/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.5017 - acc: 0.4732 - val_loss: 1.5519 - val_acc: 0.4574\n",
            "Epoch 25/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4964 - acc: 0.4757 - val_loss: 1.5293 - val_acc: 0.4716\n",
            "Epoch 26/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4889 - acc: 0.4787 - val_loss: 1.5282 - val_acc: 0.4667\n",
            "Epoch 27/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4840 - acc: 0.4784 - val_loss: 1.5194 - val_acc: 0.4672\n",
            "Epoch 28/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4855 - acc: 0.4790 - val_loss: 1.5224 - val_acc: 0.4646\n",
            "Epoch 29/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4773 - acc: 0.4804 - val_loss: 1.5094 - val_acc: 0.4692\n",
            "Epoch 30/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4757 - acc: 0.4834 - val_loss: 1.4966 - val_acc: 0.4768\n",
            "Epoch 31/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4679 - acc: 0.4833 - val_loss: 1.5068 - val_acc: 0.4753\n",
            "Epoch 32/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4649 - acc: 0.4835 - val_loss: 1.5192 - val_acc: 0.4700\n",
            "Epoch 33/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4754 - acc: 0.4789 - val_loss: 1.5098 - val_acc: 0.4666\n",
            "Epoch 34/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4665 - acc: 0.4864 - val_loss: 1.4954 - val_acc: 0.4775\n",
            "Epoch 35/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4656 - acc: 0.4844 - val_loss: 1.5018 - val_acc: 0.4754\n",
            "Epoch 36/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4636 - acc: 0.4862 - val_loss: 1.5183 - val_acc: 0.4642\n",
            "Epoch 37/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4608 - acc: 0.4839 - val_loss: 1.4849 - val_acc: 0.4773\n",
            "Epoch 38/200\n",
            "50000/50000 [==============================] - 5s 107us/step - loss: 1.4606 - acc: 0.4869 - val_loss: 1.4999 - val_acc: 0.4757\n",
            "Epoch 39/200\n",
            "50000/50000 [==============================] - 5s 108us/step - loss: 1.4549 - acc: 0.4875 - val_loss: 1.5083 - val_acc: 0.4651\n",
            "Epoch 40/200\n",
            "50000/50000 [==============================] - 5s 109us/step - loss: 1.4632 - acc: 0.4869 - val_loss: 1.4792 - val_acc: 0.4834\n",
            "Epoch 41/200\n",
            "50000/50000 [==============================] - 5s 110us/step - loss: 1.4495 - acc: 0.4886 - val_loss: 1.4852 - val_acc: 0.4777\n",
            "Epoch 42/200\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.4571 - acc: 0.4852 - val_loss: 1.5214 - val_acc: 0.4657\n",
            "Epoch 43/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4518 - acc: 0.4861 - val_loss: 1.4946 - val_acc: 0.4769\n",
            "Epoch 44/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4476 - acc: 0.4916 - val_loss: 1.4912 - val_acc: 0.4748\n",
            "Epoch 45/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4430 - acc: 0.4920 - val_loss: 1.4894 - val_acc: 0.4729\n",
            "Epoch 46/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4450 - acc: 0.4909 - val_loss: 1.4923 - val_acc: 0.4765\n",
            "Epoch 47/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4461 - acc: 0.4891 - val_loss: 1.5062 - val_acc: 0.4708\n",
            "Epoch 48/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4411 - acc: 0.4927 - val_loss: 1.4848 - val_acc: 0.4804\n",
            "Epoch 49/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4440 - acc: 0.4896 - val_loss: 1.4880 - val_acc: 0.4747\n",
            "Epoch 50/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4373 - acc: 0.4936 - val_loss: 1.4887 - val_acc: 0.4795\n",
            "Epoch 51/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4383 - acc: 0.4928 - val_loss: 1.4769 - val_acc: 0.4815\n",
            "Epoch 52/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4315 - acc: 0.4947 - val_loss: 1.4859 - val_acc: 0.4740\n",
            "Epoch 53/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4445 - acc: 0.4893 - val_loss: 1.4912 - val_acc: 0.4786\n",
            "Epoch 54/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4297 - acc: 0.4955 - val_loss: 1.4886 - val_acc: 0.4807\n",
            "Epoch 55/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4313 - acc: 0.4961 - val_loss: 1.4804 - val_acc: 0.4726\n",
            "Epoch 56/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4306 - acc: 0.4969 - val_loss: 1.4826 - val_acc: 0.4747\n",
            "Epoch 57/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4283 - acc: 0.4974 - val_loss: 1.4942 - val_acc: 0.4767\n",
            "Epoch 58/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4249 - acc: 0.4961 - val_loss: 1.5021 - val_acc: 0.4685\n",
            "Epoch 59/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4246 - acc: 0.4950 - val_loss: 1.4796 - val_acc: 0.4776\n",
            "Epoch 60/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4223 - acc: 0.4983 - val_loss: 1.5133 - val_acc: 0.4645\n",
            "Epoch 61/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4280 - acc: 0.4954 - val_loss: 1.4805 - val_acc: 0.4782\n",
            "Epoch 62/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4324 - acc: 0.4974 - val_loss: 1.4808 - val_acc: 0.4727\n",
            "Epoch 63/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4260 - acc: 0.4988 - val_loss: 1.4676 - val_acc: 0.4767\n",
            "Epoch 64/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4237 - acc: 0.4981 - val_loss: 1.4768 - val_acc: 0.4809\n",
            "Epoch 65/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4317 - acc: 0.4953 - val_loss: 1.5130 - val_acc: 0.4560\n",
            "Epoch 66/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4228 - acc: 0.4973 - val_loss: 1.4760 - val_acc: 0.4725\n",
            "Epoch 67/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4174 - acc: 0.5000 - val_loss: 1.4801 - val_acc: 0.4799\n",
            "Epoch 68/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4136 - acc: 0.5010 - val_loss: 1.4698 - val_acc: 0.4782\n",
            "Epoch 69/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4187 - acc: 0.5001 - val_loss: 1.4969 - val_acc: 0.4746\n",
            "Epoch 70/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4196 - acc: 0.5001 - val_loss: 1.4930 - val_acc: 0.4680\n",
            "Epoch 71/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4099 - acc: 0.5006 - val_loss: 1.4826 - val_acc: 0.4742\n",
            "Epoch 72/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4076 - acc: 0.5028 - val_loss: 1.4820 - val_acc: 0.4776\n",
            "Epoch 73/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3996 - acc: 0.5047 - val_loss: 1.4733 - val_acc: 0.4860\n",
            "Epoch 74/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4106 - acc: 0.5032 - val_loss: 1.4841 - val_acc: 0.4817\n",
            "Epoch 75/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4072 - acc: 0.5037 - val_loss: 1.4911 - val_acc: 0.4756\n",
            "Epoch 76/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4091 - acc: 0.4997 - val_loss: 1.4672 - val_acc: 0.4815\n",
            "Epoch 77/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4087 - acc: 0.5041 - val_loss: 1.4856 - val_acc: 0.4743\n",
            "Epoch 78/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4140 - acc: 0.4982 - val_loss: 1.4863 - val_acc: 0.4743\n",
            "Epoch 79/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4021 - acc: 0.5043 - val_loss: 1.5080 - val_acc: 0.4700\n",
            "Epoch 80/200\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.4134 - acc: 0.4978 - val_loss: 1.4886 - val_acc: 0.4747\n",
            "Epoch 81/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4017 - acc: 0.5034 - val_loss: 1.4794 - val_acc: 0.4808\n",
            "Epoch 82/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4007 - acc: 0.5045 - val_loss: 1.5139 - val_acc: 0.4633\n",
            "Epoch 83/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4046 - acc: 0.5042 - val_loss: 1.4856 - val_acc: 0.4773\n",
            "Epoch 84/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4011 - acc: 0.5048 - val_loss: 1.4693 - val_acc: 0.4758\n",
            "Epoch 85/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4102 - acc: 0.5020 - val_loss: 1.4766 - val_acc: 0.4774\n",
            "Epoch 86/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3965 - acc: 0.5093 - val_loss: 1.4820 - val_acc: 0.4770\n",
            "Epoch 87/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4016 - acc: 0.5047 - val_loss: 1.4715 - val_acc: 0.4789\n",
            "Epoch 88/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4075 - acc: 0.5016 - val_loss: 1.4857 - val_acc: 0.4761\n",
            "Epoch 89/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3973 - acc: 0.5043 - val_loss: 1.4908 - val_acc: 0.4773\n",
            "Epoch 90/200\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.3994 - acc: 0.5057 - val_loss: 1.4740 - val_acc: 0.4769\n",
            "Epoch 91/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3968 - acc: 0.5034 - val_loss: 1.4780 - val_acc: 0.4798\n",
            "Epoch 92/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3928 - acc: 0.5085 - val_loss: 1.4674 - val_acc: 0.4845\n",
            "Epoch 93/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3946 - acc: 0.5080 - val_loss: 1.5005 - val_acc: 0.4673\n",
            "Epoch 94/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3978 - acc: 0.5049 - val_loss: 1.4640 - val_acc: 0.4841\n",
            "Epoch 95/200\n",
            "50000/50000 [==============================] - 5s 103us/step - loss: 1.3972 - acc: 0.5075 - val_loss: 1.4759 - val_acc: 0.4776\n",
            "Epoch 96/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3952 - acc: 0.5068 - val_loss: 1.4804 - val_acc: 0.4812\n",
            "Epoch 97/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3944 - acc: 0.5090 - val_loss: 1.4856 - val_acc: 0.4744\n",
            "Epoch 98/200\n",
            "50000/50000 [==============================] - 5s 107us/step - loss: 1.3943 - acc: 0.5081 - val_loss: 1.4880 - val_acc: 0.4732\n",
            "Epoch 99/200\n",
            "50000/50000 [==============================] - 5s 109us/step - loss: 1.3950 - acc: 0.5084 - val_loss: 1.4878 - val_acc: 0.4730\n",
            "Epoch 100/200\n",
            "50000/50000 [==============================] - 5s 109us/step - loss: 1.3867 - acc: 0.5093 - val_loss: 1.4750 - val_acc: 0.4759\n",
            "Epoch 101/200\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.3900 - acc: 0.5067 - val_loss: 1.4689 - val_acc: 0.4794\n",
            "Epoch 102/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3925 - acc: 0.5086 - val_loss: 1.4639 - val_acc: 0.4835\n",
            "Epoch 103/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3882 - acc: 0.5110 - val_loss: 1.4817 - val_acc: 0.4726\n",
            "Epoch 104/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3905 - acc: 0.5065 - val_loss: 1.4642 - val_acc: 0.4826\n",
            "Epoch 105/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3900 - acc: 0.5072 - val_loss: 1.4792 - val_acc: 0.4806\n",
            "Epoch 106/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3917 - acc: 0.5079 - val_loss: 1.4740 - val_acc: 0.4784\n",
            "Epoch 107/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3910 - acc: 0.5086 - val_loss: 1.4917 - val_acc: 0.4716\n",
            "Epoch 108/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3857 - acc: 0.5082 - val_loss: 1.4644 - val_acc: 0.4874\n",
            "Epoch 109/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3847 - acc: 0.5116 - val_loss: 1.4796 - val_acc: 0.4779\n",
            "Epoch 110/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3832 - acc: 0.5110 - val_loss: 1.4828 - val_acc: 0.4749\n",
            "Epoch 111/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3795 - acc: 0.5139 - val_loss: 1.4789 - val_acc: 0.4754\n",
            "Epoch 112/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3759 - acc: 0.5178 - val_loss: 1.4691 - val_acc: 0.4826\n",
            "Epoch 113/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3767 - acc: 0.5132 - val_loss: 1.4663 - val_acc: 0.4818\n",
            "Epoch 114/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3827 - acc: 0.5123 - val_loss: 1.4797 - val_acc: 0.4812\n",
            "Epoch 115/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3773 - acc: 0.5120 - val_loss: 1.4895 - val_acc: 0.4726\n",
            "Epoch 116/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3808 - acc: 0.5111 - val_loss: 1.4970 - val_acc: 0.4709\n",
            "Epoch 117/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3834 - acc: 0.5105 - val_loss: 1.4850 - val_acc: 0.4736\n",
            "Epoch 118/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3849 - acc: 0.5106 - val_loss: 1.4946 - val_acc: 0.4656\n",
            "Epoch 119/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3840 - acc: 0.5120 - val_loss: 1.5047 - val_acc: 0.4721\n",
            "Epoch 120/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3795 - acc: 0.5132 - val_loss: 1.4942 - val_acc: 0.4765\n",
            "Epoch 121/200\n",
            "50000/50000 [==============================] - 5s 107us/step - loss: 1.3796 - acc: 0.5122 - val_loss: 1.4726 - val_acc: 0.4767\n",
            "Epoch 122/200\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.3745 - acc: 0.5149 - val_loss: 1.4977 - val_acc: 0.4688\n",
            "Epoch 123/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3774 - acc: 0.5133 - val_loss: 1.4636 - val_acc: 0.4811\n",
            "Epoch 124/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3727 - acc: 0.5146 - val_loss: 1.4604 - val_acc: 0.4819\n",
            "Epoch 125/200\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.3788 - acc: 0.5120 - val_loss: 1.4893 - val_acc: 0.4744\n",
            "Epoch 126/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3729 - acc: 0.5152 - val_loss: 1.4676 - val_acc: 0.4820\n",
            "Epoch 127/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3709 - acc: 0.5161 - val_loss: 1.4791 - val_acc: 0.4752\n",
            "Epoch 128/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3759 - acc: 0.5127 - val_loss: 1.4679 - val_acc: 0.4828\n",
            "Epoch 129/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3886 - acc: 0.5083 - val_loss: 1.4842 - val_acc: 0.4693\n",
            "Epoch 130/200\n",
            "50000/50000 [==============================] - 5s 107us/step - loss: 1.3733 - acc: 0.5148 - val_loss: 1.4802 - val_acc: 0.4802\n",
            "Epoch 131/200\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.3774 - acc: 0.5112 - val_loss: 1.4907 - val_acc: 0.4724\n",
            "Epoch 132/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3634 - acc: 0.5182 - val_loss: 1.4766 - val_acc: 0.4828\n",
            "Epoch 133/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3689 - acc: 0.5186 - val_loss: 1.4847 - val_acc: 0.4805\n",
            "Epoch 134/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3616 - acc: 0.5186 - val_loss: 1.4827 - val_acc: 0.4682\n",
            "Epoch 135/200\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.3750 - acc: 0.5163 - val_loss: 1.4875 - val_acc: 0.4781\n",
            "Epoch 136/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3710 - acc: 0.5137 - val_loss: 1.4812 - val_acc: 0.4717\n",
            "Epoch 137/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3714 - acc: 0.5154 - val_loss: 1.4614 - val_acc: 0.4890\n",
            "Epoch 138/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3698 - acc: 0.5141 - val_loss: 1.4575 - val_acc: 0.4916\n",
            "Epoch 139/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3714 - acc: 0.5160 - val_loss: 1.4718 - val_acc: 0.4803\n",
            "Epoch 140/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3672 - acc: 0.5154 - val_loss: 1.4646 - val_acc: 0.4803\n",
            "Epoch 141/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3737 - acc: 0.5154 - val_loss: 1.4766 - val_acc: 0.4761\n",
            "Epoch 142/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3589 - acc: 0.5203 - val_loss: 1.4777 - val_acc: 0.4733\n",
            "Epoch 143/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3701 - acc: 0.5142 - val_loss: 1.4718 - val_acc: 0.4782\n",
            "Epoch 144/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3595 - acc: 0.5182 - val_loss: 1.4644 - val_acc: 0.4856\n",
            "Epoch 145/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3654 - acc: 0.5182 - val_loss: 1.4659 - val_acc: 0.4829\n",
            "Epoch 146/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3660 - acc: 0.5125 - val_loss: 1.4749 - val_acc: 0.4822\n",
            "Epoch 147/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3716 - acc: 0.5154 - val_loss: 1.4717 - val_acc: 0.4792\n",
            "Epoch 148/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3635 - acc: 0.5193 - val_loss: 1.4903 - val_acc: 0.4758\n",
            "Epoch 149/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3615 - acc: 0.5195 - val_loss: 1.4762 - val_acc: 0.4739\n",
            "Epoch 150/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3644 - acc: 0.5197 - val_loss: 1.4742 - val_acc: 0.4765\n",
            "Epoch 151/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3616 - acc: 0.5212 - val_loss: 1.4690 - val_acc: 0.4800\n",
            "Epoch 152/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3561 - acc: 0.5217 - val_loss: 1.4866 - val_acc: 0.4696\n",
            "Epoch 153/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3632 - acc: 0.5179 - val_loss: 1.4742 - val_acc: 0.4761\n",
            "Epoch 154/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3516 - acc: 0.5212 - val_loss: 1.4903 - val_acc: 0.4724\n",
            "Epoch 155/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3573 - acc: 0.5234 - val_loss: 1.4600 - val_acc: 0.4860\n",
            "Epoch 156/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3599 - acc: 0.5190 - val_loss: 1.4882 - val_acc: 0.4774\n",
            "Epoch 157/200\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.3557 - acc: 0.5214 - val_loss: 1.4564 - val_acc: 0.4840\n",
            "Epoch 158/200\n",
            "50000/50000 [==============================] - 5s 108us/step - loss: 1.3573 - acc: 0.5222 - val_loss: 1.4884 - val_acc: 0.4714\n",
            "Epoch 159/200\n",
            "50000/50000 [==============================] - 5s 108us/step - loss: 1.3668 - acc: 0.5158 - val_loss: 1.4723 - val_acc: 0.4789\n",
            "Epoch 160/200\n",
            "50000/50000 [==============================] - 5s 107us/step - loss: 1.3614 - acc: 0.5200 - val_loss: 1.4765 - val_acc: 0.4804\n",
            "Epoch 161/200\n",
            "50000/50000 [==============================] - 5s 103us/step - loss: 1.3542 - acc: 0.5234 - val_loss: 1.4741 - val_acc: 0.4738\n",
            "Epoch 162/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3636 - acc: 0.5167 - val_loss: 1.4632 - val_acc: 0.4867\n",
            "Epoch 163/200\n",
            "50000/50000 [==============================] - 5s 103us/step - loss: 1.3595 - acc: 0.5194 - val_loss: 1.4695 - val_acc: 0.4844\n",
            "Epoch 164/200\n",
            "50000/50000 [==============================] - 5s 103us/step - loss: 1.3586 - acc: 0.5211 - val_loss: 1.4789 - val_acc: 0.4780\n",
            "Epoch 165/200\n",
            "50000/50000 [==============================] - 5s 103us/step - loss: 1.3531 - acc: 0.5222 - val_loss: 1.4672 - val_acc: 0.4819\n",
            "Epoch 166/200\n",
            "50000/50000 [==============================] - 5s 102us/step - loss: 1.3640 - acc: 0.5211 - val_loss: 1.4661 - val_acc: 0.4843\n",
            "Epoch 167/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3457 - acc: 0.5270 - val_loss: 1.4697 - val_acc: 0.4830\n",
            "Epoch 168/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3569 - acc: 0.5214 - val_loss: 1.5026 - val_acc: 0.4735\n",
            "Epoch 169/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3532 - acc: 0.5207 - val_loss: 1.4690 - val_acc: 0.4830\n",
            "Epoch 170/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3501 - acc: 0.5232 - val_loss: 1.5244 - val_acc: 0.4603\n",
            "Epoch 171/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3566 - acc: 0.5212 - val_loss: 1.4932 - val_acc: 0.4731\n",
            "Epoch 172/200\n",
            "50000/50000 [==============================] - 5s 103us/step - loss: 1.3520 - acc: 0.5224 - val_loss: 1.4830 - val_acc: 0.4755\n",
            "Epoch 173/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3553 - acc: 0.5210 - val_loss: 1.4757 - val_acc: 0.4804\n",
            "Epoch 174/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3533 - acc: 0.5232 - val_loss: 1.4701 - val_acc: 0.4818\n",
            "Epoch 175/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3466 - acc: 0.5276 - val_loss: 1.4883 - val_acc: 0.4797\n",
            "Epoch 176/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3532 - acc: 0.5225 - val_loss: 1.4633 - val_acc: 0.4828\n",
            "Epoch 177/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3554 - acc: 0.5235 - val_loss: 1.4695 - val_acc: 0.4790\n",
            "Epoch 178/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3424 - acc: 0.5255 - val_loss: 1.4938 - val_acc: 0.4728\n",
            "Epoch 179/200\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.3505 - acc: 0.5239 - val_loss: 1.4703 - val_acc: 0.4812\n",
            "Epoch 180/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3508 - acc: 0.5241 - val_loss: 1.4767 - val_acc: 0.4804\n",
            "Epoch 181/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3442 - acc: 0.5244 - val_loss: 1.4799 - val_acc: 0.4710\n",
            "Epoch 182/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3482 - acc: 0.5235 - val_loss: 1.4741 - val_acc: 0.4812\n",
            "Epoch 183/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3494 - acc: 0.5251 - val_loss: 1.4754 - val_acc: 0.4823\n",
            "Epoch 184/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3523 - acc: 0.5225 - val_loss: 1.4757 - val_acc: 0.4744\n",
            "Epoch 185/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3462 - acc: 0.5283 - val_loss: 1.4930 - val_acc: 0.4727\n",
            "Epoch 186/200\n",
            "50000/50000 [==============================] - 5s 103us/step - loss: 1.3500 - acc: 0.5255 - val_loss: 1.4786 - val_acc: 0.4738\n",
            "Epoch 187/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3501 - acc: 0.5232 - val_loss: 1.5064 - val_acc: 0.4718\n",
            "Epoch 188/200\n",
            "50000/50000 [==============================] - 5s 103us/step - loss: 1.3468 - acc: 0.5239 - val_loss: 1.4825 - val_acc: 0.4787\n",
            "Epoch 189/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3420 - acc: 0.5256 - val_loss: 1.4817 - val_acc: 0.4755\n",
            "Epoch 190/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3556 - acc: 0.5203 - val_loss: 1.4693 - val_acc: 0.4831\n",
            "Epoch 191/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3379 - acc: 0.5270 - val_loss: 1.4746 - val_acc: 0.4834\n",
            "Epoch 192/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3415 - acc: 0.5256 - val_loss: 1.4734 - val_acc: 0.4841\n",
            "Epoch 193/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3474 - acc: 0.5257 - val_loss: 1.4636 - val_acc: 0.4860\n",
            "Epoch 194/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3468 - acc: 0.5273 - val_loss: 1.4908 - val_acc: 0.4675\n",
            "Epoch 195/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3505 - acc: 0.5220 - val_loss: 1.4755 - val_acc: 0.4768\n",
            "Epoch 196/200\n",
            "50000/50000 [==============================] - 5s 103us/step - loss: 1.3382 - acc: 0.5289 - val_loss: 1.5021 - val_acc: 0.4686\n",
            "Epoch 197/200\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3476 - acc: 0.5275 - val_loss: 1.4852 - val_acc: 0.4812\n",
            "Epoch 198/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3504 - acc: 0.5220 - val_loss: 1.4620 - val_acc: 0.4838\n",
            "Epoch 199/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3332 - acc: 0.5279 - val_loss: 1.4934 - val_acc: 0.4722\n",
            "Epoch 200/200\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3426 - acc: 0.5275 - val_loss: 1.4802 - val_acc: 0.4730\n",
            "Test loss: 1.4802386316299438\n",
            "Test acc: 0.473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "w4i767pitoQh",
        "outputId": "5edd89e8-9f1d-46ca-8408-6c9a1ad472e9"
      },
      "cell_type": "code",
      "source": [
        "def convertCIFER10Data(image):\n",
        "    img = image.astype('float32')\n",
        "    img /= 255\n",
        "    c = np.zeros(3072).reshape((1,3072))\n",
        "    c[0] = img\n",
        "    return c\n",
        "  \n",
        "right = 0\n",
        "mistake = 0\n",
        "import random\n",
        "for i in range(100):\n",
        "    index = random.randint(0, X_test.shape[0])\n",
        "    image = X_test[index]\n",
        "    #print(index)\n",
        "    data = convertCIFER10Data(image)\n",
        "\n",
        "  \n",
        "    ret = model.predict(data, batch_size=1) \n",
        "    #print(ret)\n",
        "\n",
        "    bestnum = 0.0\n",
        "    bestclass = 0\n",
        "    for n in [0,1,2,3,4,5,6,7,8,9]:\n",
        "        if bestnum < ret[0][n]:\n",
        "            bestnum = ret[0][n]\n",
        "            bestclass = n\n",
        "    i = np.array(list(Y_test[index]).index(1))\n",
        "    if i == bestclass:\n",
        "        #print(i)\n",
        "        #plt.title(cifar10_labels[bestclass])\n",
        "        right += 1\n",
        "    else:\n",
        "        #plt.title(cifar10_labels[bestclass] + \"!=\" + cifar10_labels[y_test[index][0]], color='#ff0000')\n",
        "        mistake += 1\n",
        "print(\"The number of correct answers:\", right)\n",
        "print(\"The number of mistake:\", mistake)\n",
        "print(\"A correct answer rate:\", right/(mistake + right)*100, '%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of correct answers: 14\n",
            "The number of mistake: 86\n",
            "A correct answer rate: 14.000000000000002 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5QajF6notiFL"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3995
        },
        "colab_type": "code",
        "id": "k_z8dia5tlnc",
        "outputId": "ea387196-7d5f-48d6-87c3-88c1294b1052"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    nb_epoch = 100\n",
        "    batch_size = 128\n",
        "    nb_classes = 10\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "    X_train = X_train.reshape(50000, 32 * 32 * 3)\n",
        "    X_test = X_test.reshape(10000, 32 * 32 * 3)\n",
        "\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "    X_train /= 255.0\n",
        "    X_test /= 255.0\n",
        "\n",
        "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "    Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "    # MLP\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1024, input_shape=(3072, )))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    # training\n",
        "    history = model.fit(X_train, Y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        nb_epoch=nb_epoch,\n",
        "                        verbose=1,\n",
        "                        validation_data=(X_test, Y_test))\n",
        "\n",
        "    save_history(history, 'history.txt')\n",
        "\n",
        "    loss, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
        "    print('Test loss:', loss)\n",
        "    print('Test acc:', acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_24 (Dense)             (None, 1024)              3146752   \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 3,939,338\n",
            "Trainable params: 3,939,338\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "50000/50000 [==============================] - 6s 125us/step - loss: 1.9650 - acc: 0.2805 - val_loss: 1.7642 - val_acc: 0.3661\n",
            "Epoch 2/100\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.8160 - acc: 0.3422 - val_loss: 1.6891 - val_acc: 0.4004\n",
            "Epoch 3/100\n",
            "50000/50000 [==============================] - 5s 107us/step - loss: 1.7624 - acc: 0.3629 - val_loss: 1.6816 - val_acc: 0.3994\n",
            "Epoch 4/100\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.7193 - acc: 0.3770 - val_loss: 1.6300 - val_acc: 0.4182\n",
            "Epoch 5/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.6844 - acc: 0.3906 - val_loss: 1.6077 - val_acc: 0.4303\n",
            "Epoch 6/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.6487 - acc: 0.4039 - val_loss: 1.5671 - val_acc: 0.4440\n",
            "Epoch 7/100\n",
            "50000/50000 [==============================] - 5s 110us/step - loss: 1.6385 - acc: 0.4076 - val_loss: 1.6060 - val_acc: 0.4289\n",
            "Epoch 8/100\n",
            "50000/50000 [==============================] - 5s 109us/step - loss: 1.6210 - acc: 0.4126 - val_loss: 1.5395 - val_acc: 0.4508\n",
            "Epoch 9/100\n",
            "50000/50000 [==============================] - 6s 111us/step - loss: 1.5991 - acc: 0.4210 - val_loss: 1.5231 - val_acc: 0.4601\n",
            "Epoch 10/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.5801 - acc: 0.4322 - val_loss: 1.5346 - val_acc: 0.4598\n",
            "Epoch 11/100\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.5799 - acc: 0.4293 - val_loss: 1.5147 - val_acc: 0.4633\n",
            "Epoch 12/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.5673 - acc: 0.4341 - val_loss: 1.5288 - val_acc: 0.4550\n",
            "Epoch 13/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.5576 - acc: 0.4394 - val_loss: 1.5332 - val_acc: 0.4526\n",
            "Epoch 14/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.5482 - acc: 0.4436 - val_loss: 1.4953 - val_acc: 0.4664\n",
            "Epoch 15/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.5330 - acc: 0.4467 - val_loss: 1.4786 - val_acc: 0.4721\n",
            "Epoch 16/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.5237 - acc: 0.4531 - val_loss: 1.4730 - val_acc: 0.4793\n",
            "Epoch 17/100\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.5172 - acc: 0.4523 - val_loss: 1.4901 - val_acc: 0.4672\n",
            "Epoch 18/100\n",
            "50000/50000 [==============================] - 5s 103us/step - loss: 1.5124 - acc: 0.4539 - val_loss: 1.4780 - val_acc: 0.4735\n",
            "Epoch 19/100\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4993 - acc: 0.4592 - val_loss: 1.4610 - val_acc: 0.4824\n",
            "Epoch 20/100\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4874 - acc: 0.4632 - val_loss: 1.4668 - val_acc: 0.4740\n",
            "Epoch 21/100\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4813 - acc: 0.4655 - val_loss: 1.4656 - val_acc: 0.4811\n",
            "Epoch 22/100\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4857 - acc: 0.4624 - val_loss: 1.4863 - val_acc: 0.4749\n",
            "Epoch 23/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4838 - acc: 0.4657 - val_loss: 1.4535 - val_acc: 0.4874\n",
            "Epoch 24/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4692 - acc: 0.4720 - val_loss: 1.4990 - val_acc: 0.4650\n",
            "Epoch 25/100\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4631 - acc: 0.4730 - val_loss: 1.4440 - val_acc: 0.4904\n",
            "Epoch 26/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4542 - acc: 0.4761 - val_loss: 1.4314 - val_acc: 0.4924\n",
            "Epoch 27/100\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.4536 - acc: 0.4750 - val_loss: 1.4543 - val_acc: 0.4826\n",
            "Epoch 28/100\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.4471 - acc: 0.4804 - val_loss: 1.4286 - val_acc: 0.4911\n",
            "Epoch 29/100\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.4362 - acc: 0.4849 - val_loss: 1.4287 - val_acc: 0.4933\n",
            "Epoch 30/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4392 - acc: 0.4811 - val_loss: 1.4441 - val_acc: 0.4764\n",
            "Epoch 31/100\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.4322 - acc: 0.4840 - val_loss: 1.4255 - val_acc: 0.4886\n",
            "Epoch 32/100\n",
            "50000/50000 [==============================] - 5s 107us/step - loss: 1.4243 - acc: 0.4861 - val_loss: 1.4599 - val_acc: 0.4854\n",
            "Epoch 33/100\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.4284 - acc: 0.4820 - val_loss: 1.4409 - val_acc: 0.4917\n",
            "Epoch 34/100\n",
            "50000/50000 [==============================] - 5s 109us/step - loss: 1.4223 - acc: 0.4838 - val_loss: 1.4249 - val_acc: 0.4935\n",
            "Epoch 35/100\n",
            "50000/50000 [==============================] - 5s 108us/step - loss: 1.4192 - acc: 0.4879 - val_loss: 1.4420 - val_acc: 0.4920\n",
            "Epoch 36/100\n",
            "50000/50000 [==============================] - 5s 107us/step - loss: 1.4201 - acc: 0.4885 - val_loss: 1.4201 - val_acc: 0.5010\n",
            "Epoch 37/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4068 - acc: 0.4910 - val_loss: 1.4158 - val_acc: 0.4995\n",
            "Epoch 38/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.4049 - acc: 0.4934 - val_loss: 1.4449 - val_acc: 0.4861\n",
            "Epoch 39/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3974 - acc: 0.4941 - val_loss: 1.3848 - val_acc: 0.5051\n",
            "Epoch 40/100\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.4022 - acc: 0.4953 - val_loss: 1.4188 - val_acc: 0.4945\n",
            "Epoch 41/100\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3972 - acc: 0.4964 - val_loss: 1.4193 - val_acc: 0.4973\n",
            "Epoch 42/100\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3886 - acc: 0.4980 - val_loss: 1.4243 - val_acc: 0.4966\n",
            "Epoch 43/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3845 - acc: 0.5006 - val_loss: 1.3970 - val_acc: 0.5039\n",
            "Epoch 44/100\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3850 - acc: 0.5003 - val_loss: 1.4134 - val_acc: 0.4967\n",
            "Epoch 45/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3855 - acc: 0.4973 - val_loss: 1.3943 - val_acc: 0.5065\n",
            "Epoch 46/100\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.3799 - acc: 0.5023 - val_loss: 1.4262 - val_acc: 0.4939\n",
            "Epoch 47/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3767 - acc: 0.5020 - val_loss: 1.3995 - val_acc: 0.5034\n",
            "Epoch 48/100\n",
            "50000/50000 [==============================] - 5s 107us/step - loss: 1.3736 - acc: 0.5053 - val_loss: 1.4036 - val_acc: 0.5026\n",
            "Epoch 49/100\n",
            "50000/50000 [==============================] - 5s 107us/step - loss: 1.3680 - acc: 0.5056 - val_loss: 1.3989 - val_acc: 0.5075\n",
            "Epoch 50/100\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.3639 - acc: 0.5076 - val_loss: 1.4227 - val_acc: 0.4958\n",
            "Epoch 51/100\n",
            "50000/50000 [==============================] - 5s 107us/step - loss: 1.3642 - acc: 0.5090 - val_loss: 1.3974 - val_acc: 0.5009\n",
            "Epoch 52/100\n",
            "50000/50000 [==============================] - 5s 107us/step - loss: 1.3658 - acc: 0.5062 - val_loss: 1.4140 - val_acc: 0.5013\n",
            "Epoch 53/100\n",
            "50000/50000 [==============================] - 5s 107us/step - loss: 1.3578 - acc: 0.5102 - val_loss: 1.4049 - val_acc: 0.5095\n",
            "Epoch 54/100\n",
            "50000/50000 [==============================] - 5s 107us/step - loss: 1.3513 - acc: 0.5115 - val_loss: 1.3944 - val_acc: 0.5108\n",
            "Epoch 55/100\n",
            "50000/50000 [==============================] - 5s 107us/step - loss: 1.3536 - acc: 0.5124 - val_loss: 1.4284 - val_acc: 0.5011\n",
            "Epoch 56/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3471 - acc: 0.5111 - val_loss: 1.3901 - val_acc: 0.5112\n",
            "Epoch 57/100\n",
            "50000/50000 [==============================] - 5s 108us/step - loss: 1.3511 - acc: 0.5128 - val_loss: 1.4000 - val_acc: 0.5057\n",
            "Epoch 58/100\n",
            "50000/50000 [==============================] - 5s 107us/step - loss: 1.3495 - acc: 0.5103 - val_loss: 1.4083 - val_acc: 0.5038\n",
            "Epoch 59/100\n",
            "50000/50000 [==============================] - 5s 107us/step - loss: 1.3405 - acc: 0.5192 - val_loss: 1.3882 - val_acc: 0.5103\n",
            "Epoch 60/100\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.3446 - acc: 0.5155 - val_loss: 1.4255 - val_acc: 0.4984\n",
            "Epoch 61/100\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.3376 - acc: 0.5179 - val_loss: 1.4003 - val_acc: 0.5035\n",
            "Epoch 62/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3389 - acc: 0.5189 - val_loss: 1.4059 - val_acc: 0.5003\n",
            "Epoch 63/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3302 - acc: 0.5218 - val_loss: 1.4037 - val_acc: 0.5069\n",
            "Epoch 64/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3243 - acc: 0.5212 - val_loss: 1.3919 - val_acc: 0.5050\n",
            "Epoch 65/100\n",
            "50000/50000 [==============================] - 5s 107us/step - loss: 1.3277 - acc: 0.5193 - val_loss: 1.3944 - val_acc: 0.5069\n",
            "Epoch 66/100\n",
            "50000/50000 [==============================] - 5s 110us/step - loss: 1.3264 - acc: 0.5207 - val_loss: 1.3974 - val_acc: 0.5040\n",
            "Epoch 67/100\n",
            "50000/50000 [==============================] - 5s 110us/step - loss: 1.3212 - acc: 0.5216 - val_loss: 1.3887 - val_acc: 0.5105\n",
            "Epoch 68/100\n",
            "50000/50000 [==============================] - 5s 108us/step - loss: 1.3211 - acc: 0.5209 - val_loss: 1.3999 - val_acc: 0.5013\n",
            "Epoch 69/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3132 - acc: 0.5254 - val_loss: 1.4060 - val_acc: 0.5123\n",
            "Epoch 70/100\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.3157 - acc: 0.5264 - val_loss: 1.4052 - val_acc: 0.5016\n",
            "Epoch 71/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3154 - acc: 0.5248 - val_loss: 1.3984 - val_acc: 0.5066\n",
            "Epoch 72/100\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.3155 - acc: 0.5226 - val_loss: 1.4010 - val_acc: 0.4990\n",
            "Epoch 73/100\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.3231 - acc: 0.5214 - val_loss: 1.3750 - val_acc: 0.5130\n",
            "Epoch 74/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3097 - acc: 0.5277 - val_loss: 1.3882 - val_acc: 0.5111\n",
            "Epoch 75/100\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.3114 - acc: 0.5267 - val_loss: 1.4038 - val_acc: 0.5061\n",
            "Epoch 76/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.2991 - acc: 0.5309 - val_loss: 1.3801 - val_acc: 0.5130\n",
            "Epoch 77/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.2998 - acc: 0.5329 - val_loss: 1.4028 - val_acc: 0.5051\n",
            "Epoch 78/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.2991 - acc: 0.5327 - val_loss: 1.4022 - val_acc: 0.5040\n",
            "Epoch 79/100\n",
            "50000/50000 [==============================] - 5s 107us/step - loss: 1.2940 - acc: 0.5345 - val_loss: 1.3842 - val_acc: 0.5087\n",
            "Epoch 80/100\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.2996 - acc: 0.5308 - val_loss: 1.3886 - val_acc: 0.5005\n",
            "Epoch 81/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.3074 - acc: 0.5293 - val_loss: 1.3992 - val_acc: 0.5095\n",
            "Epoch 82/100\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.2972 - acc: 0.5319 - val_loss: 1.3766 - val_acc: 0.5123\n",
            "Epoch 83/100\n",
            "50000/50000 [==============================] - 5s 107us/step - loss: 1.3013 - acc: 0.5315 - val_loss: 1.3930 - val_acc: 0.5018\n",
            "Epoch 84/100\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.2933 - acc: 0.5368 - val_loss: 1.3835 - val_acc: 0.5144\n",
            "Epoch 85/100\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.2890 - acc: 0.5361 - val_loss: 1.4038 - val_acc: 0.5076\n",
            "Epoch 86/100\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.2853 - acc: 0.5372 - val_loss: 1.3873 - val_acc: 0.5098\n",
            "Epoch 87/100\n",
            "50000/50000 [==============================] - 5s 104us/step - loss: 1.2823 - acc: 0.5360 - val_loss: 1.3977 - val_acc: 0.5045\n",
            "Epoch 88/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.2821 - acc: 0.5407 - val_loss: 1.4045 - val_acc: 0.5069\n",
            "Epoch 89/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.2723 - acc: 0.5403 - val_loss: 1.3765 - val_acc: 0.5180\n",
            "Epoch 90/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.2797 - acc: 0.5365 - val_loss: 1.4069 - val_acc: 0.4992\n",
            "Epoch 91/100\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.2807 - acc: 0.5410 - val_loss: 1.3884 - val_acc: 0.5089\n",
            "Epoch 92/100\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.2763 - acc: 0.5413 - val_loss: 1.3911 - val_acc: 0.5135\n",
            "Epoch 93/100\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.2692 - acc: 0.5431 - val_loss: 1.4019 - val_acc: 0.5112\n",
            "Epoch 94/100\n",
            "50000/50000 [==============================] - 5s 107us/step - loss: 1.2657 - acc: 0.5425 - val_loss: 1.3934 - val_acc: 0.5094\n",
            "Epoch 95/100\n",
            "50000/50000 [==============================] - 5s 107us/step - loss: 1.2781 - acc: 0.5392 - val_loss: 1.4044 - val_acc: 0.5004\n",
            "Epoch 96/100\n",
            "50000/50000 [==============================] - 5s 107us/step - loss: 1.2666 - acc: 0.5414 - val_loss: 1.3922 - val_acc: 0.5105\n",
            "Epoch 97/100\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.2762 - acc: 0.5372 - val_loss: 1.3840 - val_acc: 0.5082\n",
            "Epoch 98/100\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.2741 - acc: 0.5404 - val_loss: 1.3851 - val_acc: 0.5160\n",
            "Epoch 99/100\n",
            "50000/50000 [==============================] - 5s 105us/step - loss: 1.2675 - acc: 0.5424 - val_loss: 1.3981 - val_acc: 0.5135\n",
            "Epoch 100/100\n",
            "50000/50000 [==============================] - 5s 106us/step - loss: 1.2708 - acc: 0.5426 - val_loss: 1.4010 - val_acc: 0.5063\n",
            "Test loss: 1.4010094118118286\n",
            "Test acc: 0.5063\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "UY0pTL_ftqWS",
        "outputId": "b5db9269-f645-4428-fc5b-71eef5e10536"
      },
      "cell_type": "code",
      "source": [
        "def convertCIFER10Data(image):\n",
        "    img = image.astype('float32')\n",
        "    img /= 255\n",
        "    c = np.zeros(3072).reshape((1,3072))\n",
        "    c[0] = img\n",
        "    return c\n",
        "  \n",
        "right = 0\n",
        "mistake = 0\n",
        "import random\n",
        "for i in range(100):\n",
        "    index = random.randint(0, X_test.shape[0])\n",
        "    image = X_test[index]\n",
        "    #print(index)\n",
        "    data = convertCIFER10Data(image)\n",
        "\n",
        "  \n",
        "    ret = model.predict(data, batch_size=1) \n",
        "    #print(ret)\n",
        "\n",
        "    bestnum = 0.0\n",
        "    bestclass = 0\n",
        "    for n in [0,1,2,3,4,5,6,7,8,9]:\n",
        "        if bestnum < ret[0][n]:\n",
        "            bestnum = ret[0][n]\n",
        "            bestclass = n\n",
        "    i = np.array(list(Y_test[index]).index(1))\n",
        "    if i == bestclass:\n",
        "        #print(i)\n",
        "        #plt.title(cifar10_labels[bestclass])\n",
        "        right += 1\n",
        "    else:\n",
        "        #plt.title(cifar10_labels[bestclass] + \"!=\" + cifar10_labels[y_test[index][0]], color='#ff0000')\n",
        "        mistake += 1\n",
        "print(\"The number of correct answers:\", right)\n",
        "print(\"The number of mistake:\", mistake)\n",
        "print(\"A correct answer rate:\", right/(mistake + right)*100, '%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of correct answers: 10\n",
            "The number of mistake: 90\n",
            "A correct answer rate: 10.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "zDEbiCzQjBZh"
      },
      "cell_type": "code",
      "source": [
        "#Learning rate and Optimizers\n",
        "from keras.optimizers import SGD,RMSprop,Adagrad,Adam,Adadelta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3927
        },
        "colab_type": "code",
        "id": "jTe8-24U7Ulu",
        "outputId": "469fc606-c3e7-430b-fd3c-31d11bab5abb"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    nb_epoch = 100\n",
        "    batch_size = 128\n",
        "    nb_classes = 10\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "    X_train = X_train.reshape(50000, 32 * 32 * 3)\n",
        "    X_test = X_test.reshape(10000, 32 * 32 * 3)\n",
        "\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "    X_train /= 255.0\n",
        "    X_test /= 255.0\n",
        "\n",
        "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "    Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "    # MLP\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1024, input_shape=(3072, )))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    \n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation('softmax'))\n",
        "    sgd = SGD(lr = 0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=sgd,\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    # training\n",
        "    history = model.fit(X_train, Y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        nb_epoch=nb_epoch,\n",
        "                        verbose=1,\n",
        "                        validation_data=(X_test, Y_test))\n",
        "\n",
        "    save_history(history, 'history.txt')\n",
        "\n",
        "    loss, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
        "    print('Test loss:', loss)\n",
        "    print('Test acc:', acc)\n",
        "    \n",
        "loss_t, acc_t = model.evaluate(X_train, Y_train, verbose=0)\n",
        "print('Train loss:', loss_t)\n",
        "print('Train acc:', acc_t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_31 (Dense)             (None, 1024)              3146752   \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_33 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 3,676,682\n",
            "Trainable params: 3,676,682\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 14.3987 - acc: 0.1002 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 2/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 3/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 4/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 5/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 6/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 7/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 8/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 9/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 10/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 11/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 12/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 13/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 14/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 15/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 16/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 17/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 18/100\n",
            "50000/50000 [==============================] - 4s 85us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 19/100\n",
            "50000/50000 [==============================] - 4s 87us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 20/100\n",
            "50000/50000 [==============================] - 4s 86us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 21/100\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 22/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 23/100\n",
            "50000/50000 [==============================] - 4s 82us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 24/100\n",
            "50000/50000 [==============================] - 4s 82us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 25/100\n",
            "50000/50000 [==============================] - 4s 82us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 26/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 27/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 28/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 29/100\n",
            "50000/50000 [==============================] - 4s 82us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 30/100\n",
            "50000/50000 [==============================] - 4s 82us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 31/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 32/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 33/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 34/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 35/100\n",
            "50000/50000 [==============================] - 4s 82us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 36/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 37/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 38/100\n",
            "50000/50000 [==============================] - 4s 82us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 39/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 40/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 41/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 42/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 43/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 44/100\n",
            "50000/50000 [==============================] - 4s 85us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 45/100\n",
            "50000/50000 [==============================] - 4s 85us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 46/100\n",
            "50000/50000 [==============================] - 4s 85us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 47/100\n",
            "50000/50000 [==============================] - 4s 85us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 48/100\n",
            "50000/50000 [==============================] - 4s 85us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 49/100\n",
            "50000/50000 [==============================] - 4s 86us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 50/100\n",
            "50000/50000 [==============================] - 4s 86us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 51/100\n",
            "50000/50000 [==============================] - 4s 85us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 52/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 53/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 54/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 55/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 56/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 57/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 58/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 59/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 60/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 61/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 62/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 63/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 64/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 65/100\n",
            "50000/50000 [==============================] - 4s 85us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 66/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 67/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 68/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 69/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 70/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 71/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 72/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 73/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 74/100\n",
            "50000/50000 [==============================] - 4s 85us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 75/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 76/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 77/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 78/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 79/100\n",
            "50000/50000 [==============================] - 4s 85us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 80/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 81/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 82/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 83/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 84/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 85/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 86/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 87/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 88/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 89/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 90/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 91/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 92/100\n",
            "50000/50000 [==============================] - 4s 85us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 93/100\n",
            "50000/50000 [==============================] - 4s 87us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 94/100\n",
            "50000/50000 [==============================] - 4s 87us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 95/100\n",
            "50000/50000 [==============================] - 4s 88us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 96/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 97/100\n",
            "50000/50000 [==============================] - 4s 82us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 98/100\n",
            "50000/50000 [==============================] - 4s 83us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 99/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 100/100\n",
            "50000/50000 [==============================] - 4s 84us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Test loss: 14.506285690307617\n",
            "Test acc: 0.1\n",
            "Train loss: 14.506285676879882\n",
            "Train acc: 0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "E7VTQUJntsGG",
        "outputId": "1afb3add-9088-4b28-aed5-a4ee6b417235"
      },
      "cell_type": "code",
      "source": [
        "def convertCIFER10Data(image):\n",
        "    img = image.astype('float32')\n",
        "    img /= 255\n",
        "    c = np.zeros(3072).reshape((1,3072))\n",
        "    c[0] = img\n",
        "    return c\n",
        "  \n",
        "right = 0\n",
        "mistake = 0\n",
        "import random\n",
        "for i in range(100):\n",
        "    index = random.randint(0, X_test.shape[0])\n",
        "    image = X_test[index]\n",
        "    #print(index)\n",
        "    data = convertCIFER10Data(image)\n",
        "\n",
        "  \n",
        "    ret = model.predict(data, batch_size=1) \n",
        "    #print(ret)\n",
        "\n",
        "    bestnum = 0.0\n",
        "    bestclass = 0\n",
        "    for n in [0,1,2,3,4,5,6,7,8,9]:\n",
        "        if bestnum < ret[0][n]:\n",
        "            bestnum = ret[0][n]\n",
        "            bestclass = n\n",
        "    i = np.array(list(Y_test[index]).index(1))\n",
        "    if i == bestclass:\n",
        "        #print(i)\n",
        "        #plt.title(cifar10_labels[bestclass])\n",
        "        right += 1\n",
        "    else:\n",
        "        #plt.title(cifar10_labels[bestclass] + \"!=\" + cifar10_labels[y_test[index][0]], color='#ff0000')\n",
        "        mistake += 1\n",
        "print(\"The number of correct answers:\", right)\n",
        "print(\"The number of mistake:\", mistake)\n",
        "print(\"A correct answer rate:\", right/(mistake + right)*100, '%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of correct answers: 9\n",
            "The number of mistake: 91\n",
            "A correct answer rate: 9.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2567
        },
        "colab_type": "code",
        "id": "pAtIwb9TEAAR",
        "outputId": "3aaac7de-fe93-447d-e9f0-f8643451f07e"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    nb_epoch = 60\n",
        "    batch_size = 128\n",
        "    nb_classes = 10\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "    X_train = X_train.reshape(50000, 32 * 32 * 3)\n",
        "    X_test = X_test.reshape(10000, 32 * 32 * 3)\n",
        "\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "    X_train /= 255.0\n",
        "    X_test /= 255.0\n",
        "\n",
        "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "    Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "    # MLP\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1024, input_shape=(3072, )))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    \n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation('softmax'))\n",
        "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0,\n",
        "                amsgrad=False)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    # training\n",
        "    history = model.fit(X_train, Y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        nb_epoch=nb_epoch,\n",
        "                        verbose=1,\n",
        "                        validation_data=(X_test, Y_test))\n",
        "\n",
        "    save_history(history, 'history.txt')\n",
        "\n",
        "    loss, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
        "    print('Test loss:', loss)\n",
        "    print('Test acc:', acc)\n",
        "    \n",
        "loss_t, acc_t = model.evaluate(X_train, Y_train, verbose=0)\n",
        "print('Train loss:', loss_t)\n",
        "print('Train acc:', acc_t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_34 (Dense)             (None, 1024)              3146752   \n",
            "_________________________________________________________________\n",
            "activation_34 (Activation)   (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "activation_35 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_36 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 3,676,682\n",
            "Trainable params: 3,676,682\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/60\n",
            "50000/50000 [==============================] - 6s 118us/step - loss: 2.1117 - acc: 0.2155 - val_loss: 1.9261 - val_acc: 0.3117\n",
            "Epoch 2/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 2.0097 - acc: 0.2441 - val_loss: 1.9613 - val_acc: 0.2910\n",
            "Epoch 3/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.9819 - acc: 0.2591 - val_loss: 1.9063 - val_acc: 0.3350\n",
            "Epoch 4/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.9440 - acc: 0.2746 - val_loss: 1.9095 - val_acc: 0.3331\n",
            "Epoch 5/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.9277 - acc: 0.2814 - val_loss: 1.8909 - val_acc: 0.3442\n",
            "Epoch 6/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.9119 - acc: 0.2865 - val_loss: 1.9079 - val_acc: 0.3386\n",
            "Epoch 7/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.8954 - acc: 0.2983 - val_loss: 1.8784 - val_acc: 0.3538\n",
            "Epoch 8/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.8985 - acc: 0.2987 - val_loss: 1.8859 - val_acc: 0.3544\n",
            "Epoch 9/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.8924 - acc: 0.2978 - val_loss: 1.8966 - val_acc: 0.3427\n",
            "Epoch 10/60\n",
            "50000/50000 [==============================] - 5s 101us/step - loss: 1.8856 - acc: 0.3081 - val_loss: 1.8723 - val_acc: 0.3672\n",
            "Epoch 11/60\n",
            "50000/50000 [==============================] - 5s 102us/step - loss: 1.8729 - acc: 0.3093 - val_loss: 1.8589 - val_acc: 0.3501\n",
            "Epoch 12/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.8564 - acc: 0.3177 - val_loss: 1.8486 - val_acc: 0.3787\n",
            "Epoch 13/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.8536 - acc: 0.3202 - val_loss: 1.8601 - val_acc: 0.3467\n",
            "Epoch 14/60\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 1.8464 - acc: 0.3240 - val_loss: 1.8450 - val_acc: 0.3674\n",
            "Epoch 15/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.8416 - acc: 0.3264 - val_loss: 1.8516 - val_acc: 0.3762\n",
            "Epoch 16/60\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 1.8407 - acc: 0.3240 - val_loss: 1.8431 - val_acc: 0.3793\n",
            "Epoch 17/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.8301 - acc: 0.3300 - val_loss: 1.8680 - val_acc: 0.3636\n",
            "Epoch 18/60\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 1.8268 - acc: 0.3297 - val_loss: 1.8486 - val_acc: 0.3726\n",
            "Epoch 19/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.8216 - acc: 0.3352 - val_loss: 1.8247 - val_acc: 0.3640\n",
            "Epoch 20/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.8288 - acc: 0.3338 - val_loss: 1.8311 - val_acc: 0.3789\n",
            "Epoch 21/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.8154 - acc: 0.3356 - val_loss: 1.8503 - val_acc: 0.3673\n",
            "Epoch 22/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.8186 - acc: 0.3348 - val_loss: 1.8532 - val_acc: 0.3624\n",
            "Epoch 23/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.8120 - acc: 0.3383 - val_loss: 1.8645 - val_acc: 0.3483\n",
            "Epoch 24/60\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 1.8190 - acc: 0.3377 - val_loss: 1.8344 - val_acc: 0.3737\n",
            "Epoch 25/60\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 1.8144 - acc: 0.3374 - val_loss: 1.8164 - val_acc: 0.3754\n",
            "Epoch 26/60\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 1.8072 - acc: 0.3421 - val_loss: 1.8050 - val_acc: 0.3840\n",
            "Epoch 27/60\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 1.7998 - acc: 0.3426 - val_loss: 1.8060 - val_acc: 0.3743\n",
            "Epoch 28/60\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 1.8069 - acc: 0.3420 - val_loss: 1.8225 - val_acc: 0.3922\n",
            "Epoch 29/60\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 1.7990 - acc: 0.3427 - val_loss: 1.8114 - val_acc: 0.3944\n",
            "Epoch 30/60\n",
            "50000/50000 [==============================] - 5s 101us/step - loss: 1.7902 - acc: 0.3478 - val_loss: 1.8118 - val_acc: 0.3850\n",
            "Epoch 31/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.7970 - acc: 0.3453 - val_loss: 1.8185 - val_acc: 0.3659\n",
            "Epoch 32/60\n",
            "50000/50000 [==============================] - 5s 101us/step - loss: 1.7907 - acc: 0.3474 - val_loss: 1.8267 - val_acc: 0.3829\n",
            "Epoch 33/60\n",
            "50000/50000 [==============================] - 5s 101us/step - loss: 1.7865 - acc: 0.3487 - val_loss: 1.7859 - val_acc: 0.3863\n",
            "Epoch 34/60\n",
            "50000/50000 [==============================] - 5s 102us/step - loss: 1.7886 - acc: 0.3465 - val_loss: 1.7928 - val_acc: 0.3964\n",
            "Epoch 35/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.7859 - acc: 0.3468 - val_loss: 1.8119 - val_acc: 0.3849\n",
            "Epoch 36/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.7835 - acc: 0.3490 - val_loss: 1.7880 - val_acc: 0.3984\n",
            "Epoch 37/60\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 1.7785 - acc: 0.3526 - val_loss: 1.8230 - val_acc: 0.3909\n",
            "Epoch 38/60\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 1.7766 - acc: 0.3511 - val_loss: 1.7811 - val_acc: 0.3979\n",
            "Epoch 39/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.7752 - acc: 0.3535 - val_loss: 1.7803 - val_acc: 0.4024\n",
            "Epoch 40/60\n",
            "50000/50000 [==============================] - 5s 98us/step - loss: 1.7779 - acc: 0.3509 - val_loss: 1.8005 - val_acc: 0.3875\n",
            "Epoch 41/60\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 1.7694 - acc: 0.3527 - val_loss: 1.8005 - val_acc: 0.3932\n",
            "Epoch 42/60\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 1.7644 - acc: 0.3588 - val_loss: 1.7895 - val_acc: 0.3966\n",
            "Epoch 43/60\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 1.7726 - acc: 0.3557 - val_loss: 1.8083 - val_acc: 0.3896\n",
            "Epoch 44/60\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 1.7636 - acc: 0.3579 - val_loss: 1.7913 - val_acc: 0.3882\n",
            "Epoch 45/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.7675 - acc: 0.3563 - val_loss: 1.8181 - val_acc: 0.3889\n",
            "Epoch 46/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.7671 - acc: 0.3566 - val_loss: 1.7848 - val_acc: 0.3987\n",
            "Epoch 47/60\n",
            "50000/50000 [==============================] - 5s 101us/step - loss: 1.7618 - acc: 0.3577 - val_loss: 1.7705 - val_acc: 0.4034\n",
            "Epoch 48/60\n",
            "50000/50000 [==============================] - 5s 103us/step - loss: 1.7578 - acc: 0.3591 - val_loss: 1.7847 - val_acc: 0.3900\n",
            "Epoch 49/60\n",
            "50000/50000 [==============================] - 5s 103us/step - loss: 1.7619 - acc: 0.3574 - val_loss: 1.7774 - val_acc: 0.4059\n",
            "Epoch 50/60\n",
            "50000/50000 [==============================] - 5s 102us/step - loss: 1.7523 - acc: 0.3638 - val_loss: 1.8040 - val_acc: 0.3845\n",
            "Epoch 51/60\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 1.7552 - acc: 0.3613 - val_loss: 1.7666 - val_acc: 0.3938\n",
            "Epoch 52/60\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 1.7541 - acc: 0.3633 - val_loss: 1.7866 - val_acc: 0.3894\n",
            "Epoch 53/60\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 1.7520 - acc: 0.3633 - val_loss: 1.7722 - val_acc: 0.4055\n",
            "Epoch 54/60\n",
            "50000/50000 [==============================] - 5s 102us/step - loss: 1.7488 - acc: 0.3626 - val_loss: 1.7829 - val_acc: 0.3982\n",
            "Epoch 55/60\n",
            "50000/50000 [==============================] - 5s 101us/step - loss: 1.7525 - acc: 0.3659 - val_loss: 1.7845 - val_acc: 0.4058\n",
            "Epoch 56/60\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 1.7495 - acc: 0.3663 - val_loss: 1.7975 - val_acc: 0.3943\n",
            "Epoch 57/60\n",
            "50000/50000 [==============================] - 5s 101us/step - loss: 1.7454 - acc: 0.3634 - val_loss: 1.7725 - val_acc: 0.4042\n",
            "Epoch 58/60\n",
            "50000/50000 [==============================] - 5s 101us/step - loss: 1.7503 - acc: 0.3629 - val_loss: 1.7745 - val_acc: 0.4035\n",
            "Epoch 59/60\n",
            "50000/50000 [==============================] - 5s 101us/step - loss: 1.7432 - acc: 0.3665 - val_loss: 1.7815 - val_acc: 0.3971\n",
            "Epoch 60/60\n",
            "50000/50000 [==============================] - 5s 101us/step - loss: 1.7554 - acc: 0.3619 - val_loss: 1.7801 - val_acc: 0.4084\n",
            "Test loss: 1.7801144855499267\n",
            "Test acc: 0.4084\n",
            "Train loss: 1.7518848648834229\n",
            "Train acc: 0.42442\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "3HDdS9OMtunW",
        "outputId": "b10cdfea-289d-4c89-fab4-53a109b41708"
      },
      "cell_type": "code",
      "source": [
        "def convertCIFER10Data(image):\n",
        "    img = image.astype('float32')\n",
        "    img /= 255\n",
        "    c = np.zeros(3072).reshape((1,3072))\n",
        "    c[0] = img\n",
        "    return c\n",
        "  \n",
        "right = 0\n",
        "mistake = 0\n",
        "import random\n",
        "for i in range(100):\n",
        "    index = random.randint(0, X_test.shape[0])\n",
        "    image = X_test[index]\n",
        "    #print(index)\n",
        "    data = convertCIFER10Data(image)\n",
        "\n",
        "  \n",
        "    ret = model.predict(data, batch_size=1) \n",
        "    #print(ret)\n",
        "\n",
        "    bestnum = 0.0\n",
        "    bestclass = 0\n",
        "    for n in [0,1,2,3,4,5,6,7,8,9]:\n",
        "        if bestnum < ret[0][n]:\n",
        "            bestnum = ret[0][n]\n",
        "            bestclass = n\n",
        "    i = np.array(list(Y_test[index]).index(1))\n",
        "    if i == bestclass:\n",
        "        #print(i)\n",
        "        #plt.title(cifar10_labels[bestclass])\n",
        "        right += 1\n",
        "    else:\n",
        "        #plt.title(cifar10_labels[bestclass] + \"!=\" + cifar10_labels[y_test[index][0]], color='#ff0000')\n",
        "        mistake += 1\n",
        "print(\"The number of correct answers:\", right)\n",
        "print(\"The number of mistake:\", mistake)\n",
        "print(\"A correct answer rate:\", right/(mistake + right)*100, '%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of correct answers: 8\n",
            "The number of mistake: 92\n",
            "A correct answer rate: 8.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5729
        },
        "colab_type": "code",
        "id": "7f1k-WU1_q3n",
        "outputId": "da2dad15-91de-45ae-e358-6f6501323971"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    nb_epoch = 150\n",
        "    batch_size = 256\n",
        "    nb_classes = 10\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "    X_train = X_train.reshape(50000, 32 * 32 * 3)\n",
        "    X_test = X_test.reshape(10000, 32 * 32 * 3)\n",
        "\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "    X_train /= 255.0\n",
        "    X_test /= 255.0\n",
        "\n",
        "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "    Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "    # MLP\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1024, input_shape=(3072, )))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    \n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation('softmax'))\n",
        "    rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='rmsprop',\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    # training\n",
        "    history = model.fit(X_train, Y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        nb_epoch=nb_epoch,\n",
        "                        verbose=1,\n",
        "                        validation_data=(X_test, Y_test))\n",
        "\n",
        "    save_history(history, 'history.txt')\n",
        "\n",
        "    loss, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
        "    print('Test loss:', loss)\n",
        "    print('Test acc:', acc)\n",
        "    \n",
        "loss_t, acc_t = model.evaluate(X_train, Y_train, verbose=0)\n",
        "print('Train loss:', loss_t)\n",
        "print('Train acc:', acc_t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_37 (Dense)             (None, 1024)              3146752   \n",
            "_________________________________________________________________\n",
            "activation_37 (Activation)   (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "activation_38 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "activation_39 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_40 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 3,939,338\n",
            "Trainable params: 3,939,338\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/150\n",
            "50000/50000 [==============================] - 4s 75us/step - loss: 2.3254 - acc: 0.1705 - val_loss: 2.0356 - val_acc: 0.2473\n",
            "Epoch 2/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 2.0365 - acc: 0.2462 - val_loss: 1.9294 - val_acc: 0.3187\n",
            "Epoch 3/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.9774 - acc: 0.2745 - val_loss: 1.9493 - val_acc: 0.3348\n",
            "Epoch 4/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.9277 - acc: 0.2963 - val_loss: 1.8892 - val_acc: 0.3306\n",
            "Epoch 5/150\n",
            "50000/50000 [==============================] - 3s 58us/step - loss: 1.9028 - acc: 0.3092 - val_loss: 1.9207 - val_acc: 0.3424\n",
            "Epoch 6/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.8814 - acc: 0.3161 - val_loss: 1.9005 - val_acc: 0.3497\n",
            "Epoch 7/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.8712 - acc: 0.3202 - val_loss: 1.9167 - val_acc: 0.3632\n",
            "Epoch 8/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.8557 - acc: 0.3262 - val_loss: 1.9300 - val_acc: 0.3514\n",
            "Epoch 9/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.8448 - acc: 0.3288 - val_loss: 1.9025 - val_acc: 0.3536\n",
            "Epoch 10/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.8344 - acc: 0.3358 - val_loss: 1.8526 - val_acc: 0.3765\n",
            "Epoch 11/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.8237 - acc: 0.3402 - val_loss: 1.8814 - val_acc: 0.3537\n",
            "Epoch 12/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.8175 - acc: 0.3421 - val_loss: 1.8838 - val_acc: 0.3751\n",
            "Epoch 13/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.8127 - acc: 0.3436 - val_loss: 1.9046 - val_acc: 0.3722\n",
            "Epoch 14/150\n",
            "50000/50000 [==============================] - 3s 58us/step - loss: 1.8059 - acc: 0.3463 - val_loss: 1.8917 - val_acc: 0.3802\n",
            "Epoch 15/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.8044 - acc: 0.3521 - val_loss: 1.8244 - val_acc: 0.3896\n",
            "Epoch 16/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7982 - acc: 0.3499 - val_loss: 1.8796 - val_acc: 0.3657\n",
            "Epoch 17/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7908 - acc: 0.3560 - val_loss: 1.8370 - val_acc: 0.3850\n",
            "Epoch 18/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7860 - acc: 0.3570 - val_loss: 1.8630 - val_acc: 0.3655\n",
            "Epoch 19/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7797 - acc: 0.3580 - val_loss: 1.8632 - val_acc: 0.3679\n",
            "Epoch 20/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.7743 - acc: 0.3632 - val_loss: 1.8530 - val_acc: 0.3915\n",
            "Epoch 21/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.7749 - acc: 0.3624 - val_loss: 1.8053 - val_acc: 0.3839\n",
            "Epoch 22/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7675 - acc: 0.3622 - val_loss: 1.8381 - val_acc: 0.3817\n",
            "Epoch 23/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7615 - acc: 0.3659 - val_loss: 1.8041 - val_acc: 0.4181\n",
            "Epoch 24/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7625 - acc: 0.3696 - val_loss: 1.8133 - val_acc: 0.3954\n",
            "Epoch 25/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7574 - acc: 0.3665 - val_loss: 1.8019 - val_acc: 0.3818\n",
            "Epoch 26/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7593 - acc: 0.3670 - val_loss: 1.8108 - val_acc: 0.4029\n",
            "Epoch 27/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7566 - acc: 0.3674 - val_loss: 1.8279 - val_acc: 0.3707\n",
            "Epoch 28/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7546 - acc: 0.3680 - val_loss: 1.8218 - val_acc: 0.3659\n",
            "Epoch 29/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7508 - acc: 0.3706 - val_loss: 1.8496 - val_acc: 0.3540\n",
            "Epoch 30/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7430 - acc: 0.3727 - val_loss: 1.7906 - val_acc: 0.4014\n",
            "Epoch 31/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7417 - acc: 0.3728 - val_loss: 1.7439 - val_acc: 0.4016\n",
            "Epoch 32/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7360 - acc: 0.3740 - val_loss: 1.7832 - val_acc: 0.4122\n",
            "Epoch 33/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7368 - acc: 0.3763 - val_loss: 1.8031 - val_acc: 0.3751\n",
            "Epoch 34/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7348 - acc: 0.3772 - val_loss: 1.7970 - val_acc: 0.3936\n",
            "Epoch 35/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7386 - acc: 0.3745 - val_loss: 1.8382 - val_acc: 0.3858\n",
            "Epoch 36/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7356 - acc: 0.3765 - val_loss: 1.7825 - val_acc: 0.4169\n",
            "Epoch 37/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7349 - acc: 0.3806 - val_loss: 1.7707 - val_acc: 0.4136\n",
            "Epoch 38/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7348 - acc: 0.3799 - val_loss: 1.7616 - val_acc: 0.4252\n",
            "Epoch 39/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7260 - acc: 0.3821 - val_loss: 1.7606 - val_acc: 0.3992\n",
            "Epoch 40/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7265 - acc: 0.3819 - val_loss: 1.7429 - val_acc: 0.4021\n",
            "Epoch 41/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.7246 - acc: 0.3786 - val_loss: 1.7790 - val_acc: 0.3807\n",
            "Epoch 42/150\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 1.7325 - acc: 0.3795 - val_loss: 1.8414 - val_acc: 0.3544\n",
            "Epoch 43/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7251 - acc: 0.3812 - val_loss: 1.7577 - val_acc: 0.4156\n",
            "Epoch 44/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7246 - acc: 0.3800 - val_loss: 1.7467 - val_acc: 0.4242\n",
            "Epoch 45/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7163 - acc: 0.3857 - val_loss: 1.7989 - val_acc: 0.3999\n",
            "Epoch 46/150\n",
            "50000/50000 [==============================] - 3s 58us/step - loss: 1.7197 - acc: 0.3854 - val_loss: 1.7295 - val_acc: 0.4386\n",
            "Epoch 47/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7212 - acc: 0.3833 - val_loss: 1.7340 - val_acc: 0.4100\n",
            "Epoch 48/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7189 - acc: 0.3838 - val_loss: 1.7165 - val_acc: 0.4232\n",
            "Epoch 49/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7233 - acc: 0.3835 - val_loss: 1.7235 - val_acc: 0.4273\n",
            "Epoch 50/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.7215 - acc: 0.3838 - val_loss: 1.7507 - val_acc: 0.4018\n",
            "Epoch 51/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7151 - acc: 0.3868 - val_loss: 1.7736 - val_acc: 0.3888\n",
            "Epoch 52/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.7127 - acc: 0.3863 - val_loss: 1.7441 - val_acc: 0.3994\n",
            "Epoch 53/150\n",
            "50000/50000 [==============================] - 3s 58us/step - loss: 1.7140 - acc: 0.3860 - val_loss: 1.7170 - val_acc: 0.4265\n",
            "Epoch 54/150\n",
            "50000/50000 [==============================] - 3s 58us/step - loss: 1.7135 - acc: 0.3859 - val_loss: 1.7404 - val_acc: 0.4095\n",
            "Epoch 55/150\n",
            "50000/50000 [==============================] - 3s 58us/step - loss: 1.7175 - acc: 0.3875 - val_loss: 1.7282 - val_acc: 0.4163\n",
            "Epoch 56/150\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 1.7165 - acc: 0.3874 - val_loss: 1.7077 - val_acc: 0.4293\n",
            "Epoch 57/150\n",
            "50000/50000 [==============================] - 3s 58us/step - loss: 1.7138 - acc: 0.3861 - val_loss: 1.7332 - val_acc: 0.4178\n",
            "Epoch 58/150\n",
            "50000/50000 [==============================] - 3s 58us/step - loss: 1.7067 - acc: 0.3897 - val_loss: 1.7356 - val_acc: 0.4017\n",
            "Epoch 59/150\n",
            "50000/50000 [==============================] - 3s 58us/step - loss: 1.7151 - acc: 0.3851 - val_loss: 1.7799 - val_acc: 0.4002\n",
            "Epoch 60/150\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 1.7093 - acc: 0.3879 - val_loss: 1.7366 - val_acc: 0.4149\n",
            "Epoch 61/150\n",
            "50000/50000 [==============================] - 3s 58us/step - loss: 1.7118 - acc: 0.3861 - val_loss: 1.7463 - val_acc: 0.4249\n",
            "Epoch 62/150\n",
            "50000/50000 [==============================] - 3s 58us/step - loss: 1.7103 - acc: 0.3882 - val_loss: 1.7307 - val_acc: 0.4050\n",
            "Epoch 63/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.7073 - acc: 0.3898 - val_loss: 1.7599 - val_acc: 0.4018\n",
            "Epoch 64/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7077 - acc: 0.3909 - val_loss: 1.7463 - val_acc: 0.3918\n",
            "Epoch 65/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7061 - acc: 0.3914 - val_loss: 1.7198 - val_acc: 0.4074\n",
            "Epoch 66/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7096 - acc: 0.3871 - val_loss: 1.6968 - val_acc: 0.4351\n",
            "Epoch 67/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7123 - acc: 0.3866 - val_loss: 1.7468 - val_acc: 0.3891\n",
            "Epoch 68/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.7099 - acc: 0.3878 - val_loss: 1.6794 - val_acc: 0.4313\n",
            "Epoch 69/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7058 - acc: 0.3917 - val_loss: 1.7251 - val_acc: 0.4252\n",
            "Epoch 70/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7094 - acc: 0.3889 - val_loss: 1.7440 - val_acc: 0.4062\n",
            "Epoch 71/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7012 - acc: 0.3894 - val_loss: 1.7152 - val_acc: 0.4136\n",
            "Epoch 72/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7077 - acc: 0.3918 - val_loss: 1.7092 - val_acc: 0.4190\n",
            "Epoch 73/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7057 - acc: 0.3865 - val_loss: 1.7316 - val_acc: 0.4272\n",
            "Epoch 74/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.7089 - acc: 0.3898 - val_loss: 1.7403 - val_acc: 0.4155\n",
            "Epoch 75/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7097 - acc: 0.3900 - val_loss: 1.7161 - val_acc: 0.4376\n",
            "Epoch 76/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.7069 - acc: 0.3889 - val_loss: 1.7507 - val_acc: 0.3928\n",
            "Epoch 77/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.7092 - acc: 0.3906 - val_loss: 1.7699 - val_acc: 0.3561\n",
            "Epoch 78/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.7049 - acc: 0.3922 - val_loss: 1.7234 - val_acc: 0.4069\n",
            "Epoch 79/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.7039 - acc: 0.3922 - val_loss: 1.7057 - val_acc: 0.4254\n",
            "Epoch 80/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.6999 - acc: 0.3919 - val_loss: 1.7506 - val_acc: 0.3852\n",
            "Epoch 81/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7013 - acc: 0.3907 - val_loss: 1.7114 - val_acc: 0.3930\n",
            "Epoch 82/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7095 - acc: 0.3929 - val_loss: 1.6864 - val_acc: 0.4215\n",
            "Epoch 83/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7059 - acc: 0.3944 - val_loss: 1.7267 - val_acc: 0.3946\n",
            "Epoch 84/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7122 - acc: 0.3896 - val_loss: 1.7303 - val_acc: 0.3994\n",
            "Epoch 85/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.7112 - acc: 0.3898 - val_loss: 1.7411 - val_acc: 0.3932\n",
            "Epoch 86/150\n",
            "50000/50000 [==============================] - 3s 58us/step - loss: 1.7058 - acc: 0.3889 - val_loss: 1.6988 - val_acc: 0.4002\n",
            "Epoch 87/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.7116 - acc: 0.3878 - val_loss: 1.7300 - val_acc: 0.3968\n",
            "Epoch 88/150\n",
            "50000/50000 [==============================] - 3s 58us/step - loss: 1.7080 - acc: 0.3921 - val_loss: 1.7023 - val_acc: 0.4210\n",
            "Epoch 89/150\n",
            "50000/50000 [==============================] - 3s 58us/step - loss: 1.7040 - acc: 0.3902 - val_loss: 1.6967 - val_acc: 0.4171\n",
            "Epoch 90/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7063 - acc: 0.3908 - val_loss: 1.7064 - val_acc: 0.4311\n",
            "Epoch 91/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7094 - acc: 0.3910 - val_loss: 1.7127 - val_acc: 0.4012\n",
            "Epoch 92/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7108 - acc: 0.3917 - val_loss: 1.7460 - val_acc: 0.4130\n",
            "Epoch 93/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7116 - acc: 0.3858 - val_loss: 1.6825 - val_acc: 0.4082\n",
            "Epoch 94/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7090 - acc: 0.3915 - val_loss: 1.7154 - val_acc: 0.3950\n",
            "Epoch 95/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7156 - acc: 0.3900 - val_loss: 1.6709 - val_acc: 0.4453\n",
            "Epoch 96/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7137 - acc: 0.3900 - val_loss: 1.7234 - val_acc: 0.4062\n",
            "Epoch 97/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7123 - acc: 0.3917 - val_loss: 1.6918 - val_acc: 0.4276\n",
            "Epoch 98/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7127 - acc: 0.3914 - val_loss: 1.7078 - val_acc: 0.4004\n",
            "Epoch 99/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7116 - acc: 0.3900 - val_loss: 1.7260 - val_acc: 0.3909\n",
            "Epoch 100/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7141 - acc: 0.3906 - val_loss: 1.6829 - val_acc: 0.4156\n",
            "Epoch 101/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7169 - acc: 0.3900 - val_loss: 1.6647 - val_acc: 0.4385\n",
            "Epoch 102/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7147 - acc: 0.3877 - val_loss: 1.6891 - val_acc: 0.4167\n",
            "Epoch 103/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7183 - acc: 0.3883 - val_loss: 1.7082 - val_acc: 0.3969\n",
            "Epoch 104/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7090 - acc: 0.3900 - val_loss: 1.6738 - val_acc: 0.4164\n",
            "Epoch 105/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7118 - acc: 0.3890 - val_loss: 1.6414 - val_acc: 0.4340\n",
            "Epoch 106/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7090 - acc: 0.3940 - val_loss: 1.7169 - val_acc: 0.3939\n",
            "Epoch 107/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7132 - acc: 0.3905 - val_loss: 1.6989 - val_acc: 0.4114\n",
            "Epoch 108/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7187 - acc: 0.3892 - val_loss: 1.8100 - val_acc: 0.3519\n",
            "Epoch 109/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7196 - acc: 0.3893 - val_loss: 1.7221 - val_acc: 0.3845\n",
            "Epoch 110/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7179 - acc: 0.3908 - val_loss: 1.7033 - val_acc: 0.4050\n",
            "Epoch 111/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7198 - acc: 0.3880 - val_loss: 1.7387 - val_acc: 0.3573\n",
            "Epoch 112/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7226 - acc: 0.3875 - val_loss: 1.7196 - val_acc: 0.4012\n",
            "Epoch 113/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7155 - acc: 0.3874 - val_loss: 1.7049 - val_acc: 0.3954\n",
            "Epoch 114/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7192 - acc: 0.3905 - val_loss: 1.6844 - val_acc: 0.4151\n",
            "Epoch 115/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7142 - acc: 0.3923 - val_loss: 1.6940 - val_acc: 0.4065\n",
            "Epoch 116/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7155 - acc: 0.3848 - val_loss: 1.7253 - val_acc: 0.3784\n",
            "Epoch 117/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7231 - acc: 0.3868 - val_loss: 1.7165 - val_acc: 0.3784\n",
            "Epoch 118/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7163 - acc: 0.3895 - val_loss: 1.6811 - val_acc: 0.4102\n",
            "Epoch 119/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7136 - acc: 0.3917 - val_loss: 1.6779 - val_acc: 0.4167\n",
            "Epoch 120/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7145 - acc: 0.3893 - val_loss: 1.7008 - val_acc: 0.4193\n",
            "Epoch 121/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7153 - acc: 0.3890 - val_loss: 1.7616 - val_acc: 0.3580\n",
            "Epoch 122/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.7244 - acc: 0.3878 - val_loss: 1.6916 - val_acc: 0.4225\n",
            "Epoch 123/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7183 - acc: 0.3894 - val_loss: 1.7122 - val_acc: 0.4232\n",
            "Epoch 124/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7273 - acc: 0.3860 - val_loss: 1.6802 - val_acc: 0.3962\n",
            "Epoch 125/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7204 - acc: 0.3893 - val_loss: 1.6974 - val_acc: 0.4001\n",
            "Epoch 126/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7235 - acc: 0.3849 - val_loss: 1.6950 - val_acc: 0.4136\n",
            "Epoch 127/150\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.7177 - acc: 0.3882 - val_loss: 1.7644 - val_acc: 0.3474\n",
            "Epoch 128/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7217 - acc: 0.3906 - val_loss: 1.6786 - val_acc: 0.3961\n",
            "Epoch 129/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7226 - acc: 0.3869 - val_loss: 1.6923 - val_acc: 0.3952\n",
            "Epoch 130/150\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.7237 - acc: 0.3892 - val_loss: 1.6693 - val_acc: 0.4136\n",
            "Epoch 131/150\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.7178 - acc: 0.3894 - val_loss: 1.7399 - val_acc: 0.3876\n",
            "Epoch 132/150\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.7191 - acc: 0.3922 - val_loss: 1.8656 - val_acc: 0.3376\n",
            "Epoch 133/150\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.7354 - acc: 0.3815 - val_loss: 1.7000 - val_acc: 0.3917\n",
            "Epoch 134/150\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.7310 - acc: 0.3845 - val_loss: 1.6748 - val_acc: 0.4018\n",
            "Epoch 135/150\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.7244 - acc: 0.3887 - val_loss: 1.7027 - val_acc: 0.3944\n",
            "Epoch 136/150\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.7312 - acc: 0.3872 - val_loss: 1.7012 - val_acc: 0.4105\n",
            "Epoch 137/150\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.7253 - acc: 0.3838 - val_loss: 1.7000 - val_acc: 0.3901\n",
            "Epoch 138/150\n",
            "50000/50000 [==============================] - 3s 54us/step - loss: 1.7345 - acc: 0.3888 - val_loss: 1.9476 - val_acc: 0.3581\n",
            "Epoch 139/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7364 - acc: 0.3857 - val_loss: 1.6806 - val_acc: 0.3998\n",
            "Epoch 140/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7260 - acc: 0.3894 - val_loss: 1.6880 - val_acc: 0.3845\n",
            "Epoch 141/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7253 - acc: 0.3850 - val_loss: 1.7139 - val_acc: 0.4061\n",
            "Epoch 142/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7301 - acc: 0.3858 - val_loss: 1.6640 - val_acc: 0.4278\n",
            "Epoch 143/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7290 - acc: 0.3867 - val_loss: 1.7094 - val_acc: 0.3908\n",
            "Epoch 144/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7267 - acc: 0.3863 - val_loss: 1.7182 - val_acc: 0.3781\n",
            "Epoch 145/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7272 - acc: 0.3828 - val_loss: 1.6789 - val_acc: 0.4027\n",
            "Epoch 146/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7418 - acc: 0.3807 - val_loss: 1.7023 - val_acc: 0.4042\n",
            "Epoch 147/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7292 - acc: 0.3852 - val_loss: 1.6815 - val_acc: 0.4210\n",
            "Epoch 148/150\n",
            "50000/50000 [==============================] - 3s 55us/step - loss: 1.7298 - acc: 0.3845 - val_loss: 1.6664 - val_acc: 0.4355\n",
            "Epoch 149/150\n",
            "50000/50000 [==============================] - 3s 57us/step - loss: 1.7350 - acc: 0.3864 - val_loss: 1.7077 - val_acc: 0.3931\n",
            "Epoch 150/150\n",
            "50000/50000 [==============================] - 3s 56us/step - loss: 1.7355 - acc: 0.3843 - val_loss: 1.7134 - val_acc: 0.3899\n",
            "Test loss: 1.713398427581787\n",
            "Test acc: 0.3899\n",
            "Train loss: 1.6646417357254029\n",
            "Train acc: 0.41342\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "QuPZxLIEtv9C",
        "outputId": "5d34cb0f-0bd9-4471-930e-c9d310098c04"
      },
      "cell_type": "code",
      "source": [
        "def convertCIFER10Data(image):\n",
        "    img = image.astype('float32')\n",
        "    img /= 255\n",
        "    c = np.zeros(3072).reshape((1,3072))\n",
        "    c[0] = img\n",
        "    return c\n",
        "  \n",
        "right = 0\n",
        "mistake = 0\n",
        "import random\n",
        "for i in range(100):\n",
        "    index = random.randint(0, X_test.shape[0])\n",
        "    image = X_test[index]\n",
        "    #print(index)\n",
        "    data = convertCIFER10Data(image)\n",
        "\n",
        "  \n",
        "    ret = model.predict(data, batch_size=1) \n",
        "    #print(ret)\n",
        "\n",
        "    bestnum = 0.0\n",
        "    bestclass = 0\n",
        "    for n in [0,1,2,3,4,5,6,7,8,9]:\n",
        "        if bestnum < ret[0][n]:\n",
        "            bestnum = ret[0][n]\n",
        "            bestclass = n\n",
        "    i = np.array(list(Y_test[index]).index(1))\n",
        "    if i == bestclass:\n",
        "        #print(i)\n",
        "        #plt.title(cifar10_labels[bestclass])\n",
        "        right += 1\n",
        "    else:\n",
        "        #plt.title(cifar10_labels[bestclass] + \"!=\" + cifar10_labels[y_test[index][0]], color='#ff0000')\n",
        "        mistake += 1\n",
        "print(\"The number of correct answers:\", right)\n",
        "print(\"The number of mistake:\", mistake)\n",
        "print(\"A correct answer rate:\", right/(mistake + right)*100, '%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of correct answers: 14\n",
            "The number of mistake: 86\n",
            "A correct answer rate: 14.000000000000002 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SXSFkBUkZ6OU"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "colab_type": "code",
        "id": "S9XEErWaR4SB",
        "outputId": "e84d2199-6ae0-47d9-c5d1-888d15c071c5"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.subplot(212)  \n",
        "plt.plot(history.history['loss'])  \n",
        "plt.plot(history.history['val_loss'])  \n",
        "plt.title('model loss')  \n",
        "plt.ylabel('loss')  \n",
        "plt.xlabel('epoch')  \n",
        "plt.legend(['train', 'test'], loc='upper left')  \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAADECAYAAAC2lamMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzs3Xd429W9+PG39rAlW7blve04cZzh\n7IRMAlkQNiWUWSikv0IvBdretve2vXBLKdCWlnI7GAVa9ghQIMEhiyTEZDlxhpN47ynbkizZ1tbv\nD9lKTGJjJ3GC7fN6Hp4Q6TvOsRV9vmd9jsTv9/sRBEEQBGHEkF7sAgiCIAiCMDQieAuCIAjCCCOC\ntyAIgiCMMCJ4C4IgCMIII4K3IAiCIIwwIngLgiAIwggjgrcgCPz3f/83zz777IDHvP/++3znO98Z\n9OuCIAwfEbwFQRAEYYQRwVsQRpi6ujoWLFjACy+8wIoVK1ixYgWFhYWsXbuWhQsX8vOf/zx47Kef\nfsrq1atZuXIld9xxBzU1NQCYzWbuvvtuli5dytq1a7HZbMFzysrKuO2221ixYgVXXXUVR44cGXTZ\nLBYLP/zhD1mxYgVXXHEFzz//fPC9P/7xj8Hy3nHHHTQ3Nw/4uiAI/ZNf7AIIgjB0ZrMZo9HIxo0b\neeCBB3jooYdYt24dEomERYsW8f3vfx+5XM4vf/lL1q1bR0pKCi+99BK/+tWveOWVV3jhhRcwGAy8\n9NJL1NXVcfXVVzNu3Dh8Ph/3338/99xzD9/61rcoKCjgvvvuY9u2bYMq19NPP01YWBgbN27EYrFw\n3XXXMX36dMLCwsjLy+OTTz5BoVDw6quv8uWXX5KTk3PG16+99tph/gkKwsgmWt6CMAJ5PB5WrlwJ\nQFZWFpMnTyYiIgKDwYDRaKSlpYVdu3YxZ84cUlJSAPjWt77Fnj178Hg87N+/n1WrVgGQmJjI7Nmz\nAaioqKCtrY0bb7wRgBkzZhAREcHBgwcHVa7t27dzyy23ABAeHs6yZcvYtWsXer2e9vZ2Pv74Y6xW\nK7fffjvXXnttv68LgjAwEbwFYQSSyWSo1WoApFIpWq22z3terxez2Yxerw++rtPp8Pv9mM1mrFYr\nOp0u+F7vcR0dHTgcDlatWsXKlStZuXIlbW1tWCyWQZWrvb29zz31ej1tbW3ExMTw7LPPkpeXx5Il\nS1i7di2NjY39vi4IwsBE8BaEUSoyMrJP0LVarUilUgwGA3q9vs84d3t7OwDR0dGEhISQl5cX/O+L\nL75g2bJlg7pnVFRUn3taLBaioqIAmDt3Ls8//zy7du0iLi6O3//+9wO+LghC/0TwFoRRav78+ezf\nv5/a2loA3nrrLebPn49cLic3N5fNmzcDUFNTQ0FBAQAJCQnExsaSl5cHBIL6ww8/TFdX16DuuWTJ\nEt5+++3guZs2bWLJkiV88cUXPProo/h8PrRaLRMmTEAikfT7uiAIAxMT1gRhlIqNjeWxxx7jvvvu\nw+12k5iYyK9//WsAvve97/HQQw+xdOlSMjIyWL58OQASiYSnn36aRx55hD/96U9IpVLuuuuuPt3y\nA3nwwQd55JFHWLlyJVKplLVr1zJlyhScTifr169nxYoVKJVKIiIiePzxx4mOjj7j64IgDEwi9vMW\nBEEQhJFFdJsLgiAIwggjgrcgCIIgjDAieAuCIAjCCCOCtyAIgiCMMCJ4C4IgCMIIM2KWiplMtq8/\naAgMBi1m8+DWro40o7Vuo7VeMHrrNlrrBaO3bqO1XjAy62Y06s74+phtecvlsotdhGEzWus2WusF\no7duo7VeMHrrNlrrBaOrbmM2eAuCIAjCSCWCtyAIgiCMMCJ4C4IgCMIIMyaDd4u5i//9x25aLd0X\nuyiCIAiCMGRjMniX1VvZd6yZY9Xmi10UQRAEQRiyMRm81crACjmHy3uRSyIIgiAIQzdGg3dguYDD\n6bnIJREEQRCEoRujwfv8tbw//3zLoI575pk/0NBQf873EwRBEIQxGbw1qkDLu9t1bi3vxsYGNm/e\nOKhjf/jDHxEfn3BO9xMEQRAEGEHpUc+n89XyfvrpJzl+vIiFC2exfPkqGhsb+NOf/spvf/u/mEwt\ndHd3c/fda5k/fyE/+MFaHn74P9m2bQudnXZqaqqpr6/jgQd+xLx5889HtQRBEIQxYtQE73e2lrHv\nRMugjvXjB+BAiYmf/DW/3+NmTYjmpqWZ/b7/7W/fzvvvv0NaWgY1NVX89a8vYja3M3v2XFatWk19\nfR2//OXPmD9/YZ/zWlqa+f3v/8zu3fn8+9/rRPAWBEEQhmTUBO+hkCABwO/3n7drZmfnAKDT6Tl+\nvIiPPnofiURKR4f1tGOnTMkFIDo6Grvdft7KIAiCIIwNoyZ437Q0c8BW8lfd/8ftGMM1PHLX7PNy\nf4VCAcCmTXl0dHTwl7+8SEdHB/fcc/tpx8pkJ5Pjn88HCEEQBGFsGJMT1gA0Kvk5j3lLpVK83r7X\nsFgsxMXFI5VK2b59K263+5zuIQiCIAhfNYaDt+Kc13mnpKRRXHyCzs6TXd9LliwlP38nP/zh99Fo\nNERHR/Pyyy+ca3EFQRAEIUjiHyH9tiaT7bxe7/HXD1DT2MHff7zkvF73m8Bo1J33n9c3wWitF4ze\nuo3WesHordtorReMzLoZjbozvj5mW95alRyXx4fX57vYRREEQRCEIRmzwVujEvnNBUEQhJFp7AZv\ndU/wdorgLQiCIIwsYzZ4a4Mtb7E5iSAIgjCyjNng3dtt3i26zQVBEIQRZuwGb7VoeQuCIAgj05gN\n3lpVICPauY55D3ZL0F6FhQcwm9vP6Z6CIAjC2Daswfupp55izZo13HDDDXz22Wd93tu9ezc33XQT\nN998Mz//+c/xXeAlWye7zc++5T2ULUF7rV//kQjegiAIwjkZttzmu3fvprS0lLfffhuz2cx1113H\n8uXLg+//6le/4l//+hexsbE88MAD7Ny5k8WLFw9XcU5zstv87FvevVuCvvTS81RUlGGz2fB6vTz4\n4E/IzBzHa6+9wvbt25BKpcyfv5Ds7Ins3Pk5lZUVPPbYU8TGxp6v6giCIAhjyLAF71mzZjFlyhQA\n9Ho93d3deL3e4KYc77//PqGhoQBERERgNpvP6X7vl33CwZYjgz7e6/Wjmuogr2MXO/LP/GOYFj2Z\n6zNX93uN3i1BpVIpc+ZcwlVXXUtlZQXPPPN7/vSnv/LWW6/x4Yd5yGQyPvxwHbNmzSUzM4uHH/5P\nEbgFQRCEszZswVsmk6HVagF47733WLRoUZ/dtHoDd0tLC7t27eKHP/zhcBXljCSBXUGDe3ufiyNH\nDmOxmNm4cQMATqcDgCVLLuPBB+9j2bKVLF++8pzvIwiCIAhwAXKbb968meeee46XXnoJna5vjta2\ntjbuvfdeHn74YRYsWDDgdTweL3K5bMBjhqK6qYMf/G4bqy5J5b4bpp7VNfbs2cPrr7+O2+1m7dq1\nTJs27bRjysvL+fTTT9myZQvvvvsud911F7/85S/Jyso61yoIgiAIY9Sw7ue9c+dO/v73v/Piiy+e\nFrjtdjv33nsvDz744NcGbgCzueu8lq13trnZ2n3Wieo7Ohx0dTnIzs7ho482kJiYSWVlBXv25LN6\n9bW8++6b3HXXvaxZcyf5+Xuorm7C4/HR2tqBwTB8yfFHYvL9wRit9YLRW7fRWi8YvXUbrfWCkVm3\n/jYmGbbgbbPZeOqpp3jllVcIDw8/7f0nnniCO++8k0WLFg1XEQZ0PtKj9m4JGhcXT3NzE/fddw8+\nn48HH/wxoaGhWCxm7r33DjQaLZMmTUGvDyM3dzq/+MVP+e1v/0B6esb5qo4gCIIwhgxb8N6wYQNm\ns5kHH3ww+NqcOXMYP348CxYs4MMPP6S6upr33nsPgNWrV7NmzZrhKs5pNMpAF/y5JGkxGAy8//76\nft9/6KH/PO21u+9ey913rz3rewqCIAjCsAXvNWvWDBiMjx49Oly3HhSZTIpSIRXpUQVBEIQRZ8xm\nWANQK+ViS1BBEARhxBnTwVujlOFwitzmgiAIwsgypoO3aHkLgiAII9EYD94ynG4vPt+wLnUXBEEQ\nhPNqTAfv3s1JROtbEARBGEnGdPBWn4flYoIgCIJwoYngjWh5C4IgCCPL2A7e52FPb0EQBEG40MZ2\n8BYtb0EQBGEEGuPB+9zzmwuCIAjChTamg/f5yG8uCIIgCBfamA7earFUTBAEQRiBxnbwFi1vQRAE\nYQQa08Fb0zPm3S3GvAVBEIQRZEwHb9HyFgRBEEaisR28VYHgLVregiAIwkgytoN371Ix0fIWBEEQ\nRpAxHrxFkhZBEARh5BnTwVsuk6KQS0XLWxAEQRhRxnTwhkDrW7S8BUEQhJFEBG8RvAVBEIQRZswH\nb41STrdTdJsLgiAII8eYD95qpQyny4vf77/YRREEQRCEQRmTwdvldbOrZh9unwe1So4fcLpF17kg\nCIIwMozJ4F3UdoJnvnyJA82H0IcoATBZHBe5VIIgCIIwOGMyeIcqQgBo6mohKzEcgBM15otZJEEQ\nBEEYtCEHb5fLRWNj43CU5YIxaiMBMHW3MSGlJ3hXi+AtCIIgjAzywRz03HPPodVqufHGG7nhhhsI\nCQlh/vz5PPjggwOe99RTT1FQUIDH4+F73/sey5cvD76Xn5/P008/jUwmY9GiRdx///3nVpMhCFPq\nUcmUmLpaiQrTEBWmprjGgs/nRyqVXLByCIIgCMLZGFTLe9u2bdx2223k5eVx6aWX8u6773LgwIEB\nz9m9ezelpaW8/fbbvPjiizz++ON93n/sscd49tlnefPNN9m1axdlZWVnX4shkkgkxIYaMXW34vf7\nyU4x0OX0UNNiu2BlEARBEISzNajgLZfLkUgk7Nixg8svvxwAn8834DmzZs3imWeeAUCv19Pd3Y3X\nG5jRXVtbS1hYGHFxcUilUhYvXsyXX355LvUYshidEafXRYfLzoQUAwAnqi0XtAyCIAiCcDYGFbx1\nOh1r166lvLycadOmsW3bNiSSgbuXZTIZWq0WgPfee49FixYhkwU2AjGZTERERASPjYiIwGQynW0d\nzkpsaHSgLN2tTEjuCd5i0pogCIIwAgxqzPsPf/gD+fn5TJ8+HQCVSsWTTz45qBts3ryZ9957j5de\neunsSwkYDFrkctk5XeNUcR1GAJzyTrLSokgwhlBaZ8EQEYJcNvIn4RuNuotdhGExWusFo7duo7Ve\nMHrrNlrrBaOnboMK3u3t7RgMBiIiInjnnXcoLCzku9/97teet3PnTv7+97/z4osvotOd/IFFR0fT\n2toa/HtzczPR0dEDXsts7hpMUQctVhe4X3lzHTmhNsYlhvP5wXr2H2kgIyHsvN7rQjMadZhMo2/8\nfrTWC0Zv3UZrvWD01m201gtGZt36e9gYVBPz5z//OQqFgmPHjvHuu++yYsUKHnvssQHPsdlsPPXU\nUzz33HOEh4f3eS8xMRG73U5dXR0ej4dt27Yxf/78QVbl/IgNDbS8Td2Bh4gJyWK9tyAIgjAyDKrl\nLZFImDJlCs888wy33norixcv5uWXXx7wnA0bNmA2m/ssJ5szZw7jx49n2bJlPPLII/zoRz8C4Ior\nriAtLe0cqjF0Bk0YCqkCU3cbQHDc+1iVmSvnpV7QsgiCIAjCUAwqeHd1dXH48GE2btzIa6+9hsvl\noqOjY8Bz1qxZw5o1a/p9f9asWbz99ttDK+15JJVIMWoiMXUFlovpQ5QkR4dSUmuh2+lBoxrUj0YQ\nBEEQLrhBdZvffffd/PKXv2TNmjVERETw7LPPsnr16uEu27AzaqNweJ3Y3Z0ATM2MwuvzU1TZfpFL\nJgiCIAj9G1Tz8oorruCKK67AYrFgtVp5+OGHv3ap2Ehg1ATSpLZ0taJThpI7LoqP86soLGtl5oSB\nJ9AJgiAIwsUyqJZ3QUEBl19+OatWrWL58uWsWrWKI0eODHfZhl1v8O6dtJYSqyM8VMnh8jZ8PrG/\ntyAIgvDNNKjg/fTTT/PXv/6VL7/8kj179vD000/zxBNPDHfZhl20NgoIbFDi9Xn517G3icqqxd7t\npqzeepFLJwiCIAyGz+9jR10+Npf9YhflghlU8JZKpWRlZQX/PnHixGC2tJHMqOkJ3l2tfFyxkX3N\nB2hRFAE+CstaBz5ZEARB+EYot1TxdsmH7Ky/sGm2L6ZBB++NGzdit9ux2+1s2LBhVATvMJUehVTO\nsfYSNtV8DoDb70IZZqewVARvQRCEkaB30nGn+/wm8/omG1TwfvTRR3nnnXdYunQpl112GR9++CH/\n+7//O9xlG3ZSiZQoTSTdnm7kUjnLUy4FIDapm6b2Lprav/6D4PV5KW4vw+vzDndxBUEQhDPo9nT3\n/Om4yCW5cAacbX7LLbcEZ5X7/X4yMzMBsNvt/OxnP+P1118f/hIOsxhtNI2dzdyUdQ2TIifyWfU2\n5GFmIIZ9J1q46pLUfs91eBy8ePQ1jreXsDRpITeMu+qClVsQBEEI6A3aDq/zIpfkwhkweJ+aHW20\nuiZjFTNipjLNOBmJREKMNpo2RwNa9UQ+3V1NVqaUgva9XJ2+ilBlSPA8i9PK3w69TJ29AQkSdtTl\nszhxPlGaiAHuJgiCIJxvouX9FbNnz75Q5bhoorVRwVnnAOMM6XxRv5tL5+vYsMXKS4XvYZc3YXZa\n+f6Uu5BKpJgdFp4+8DfaHWbmx88hIyyVfx1/m48r8rgr55aLWBtBEISxJ9jyHkPBe+TvfXmeZYWn\nA6AzdhCf7MIubwLgWFsxW2p2YHPZebbwBdodZq5IvZxvj7+eWbHTSNIlsL+5kOqO2otZfEEQhDFH\nBG+BzPAMAEot5URkBgJxSPMcwpR6PqrI448H/k5zl4nLkxdzRdoyJBIJUomU6zOvBOCDsvX4/V+f\n4CW/YR+vFL2F3dU5fJURBEEYA7p6u829IniPWWEqHbHaaErN5ZTbSwn1xdBaHU6Gdwl+v5/mrhYu\niZvFtRlX9EkRm2XIZGLkeEotFdTa6ge8R7mlijeL17Gv+QC/K/g/mjtbhrtagiAIo5ZDtLwFgHGG\nDDz+wNKvNTkriQrTkL/bxYrYq1mVejnfnnDDGXO7z4ubBcDh1mP9XrvL3c3LRW/g9/uZHTud1u42\nfl/wF8otVcNSF0EQhNGut+Xt9nnw+DwXuTQXhgjeZzCuZ9w7MTSeabETuWf1RAB2bpdwWcJSpJIz\n/9gmRmQhl8g40k/w9vv9vFm8DrPTwsrUy7hz4s3cnn0TDq+Td0s+HJ7KCIIgjHKntrgdnrGxXEwE\n7zOYFJXN7Njp3Dz+OiQSCVlJ4Vw5L4VWq4N/5p3A4/Wd8Ty1XM04QwZ19gbMDgsQyLm7sWorzx58\ngV/kP86BlsOkh6WyKvUyAObGzSRNn0KdvbHPB9Dj84judEEQhEHoOjV4j5FxbxG8z0AlU3LnxJtJ\nC0sJvnb1/DTS4vTsPd7Cr/+5n5pm2xnPnRwVaKX3tr73NBbwUUUeJ8ylAEyJyuHunFuQSU+ml00P\nS8GPn6pTZqpvrN7Gr/f8gUprzXmvnyAIwmjh8/v6NHy6RctbOJVcJuXHN+eyaGoctS12fv3P/Wwp\nqDvtuEmR2QAcaT2O2+dhfeUm5FI5j877Gb+Z/998b8qdGNThfc7pfUg4NVAfNhXhx8/upv3DWCtB\nEISRzel14efkCh9Hz/j3aCeC9xBoVHK+syqbB781lRCNgtc3lZwWwCM1BhJC4ygxl7GlZgdmp4VF\nCfMGzLyW3hu8O6oB6HDZqLM3AHCw+XCfCRiNnc0UtRVzpPUYx9tKxszkDEEQRq8KaxVvF39wVntE\ndH8lWI+VFKkDZlgTzmxKRiQ/vWUaT75xkNc3lSCTSViSmxB8f3LUROrtjXxSsRGVTMmKlKUDXk+n\nDCVKE0mltRqf38eJ9kAXu0auodPTxfH2kuA1n9j3DD7/yTH3xNB47pi4hoTQuOGprCAIwjDbXpfP\n/uZC5sXNIlmfOKRzexO0yKVyPD7PmEmRKlreZykuMoSf3JxLqEbBv/KK+WhXJV5fIKhO6Rn39uPn\nsqRFfXKi9ydNn0KXp5uWrtZg8L4u4woA9jcX4vf7ea/kI3x+H5clLeK6zCuZFTOdOnsDT+77M5tr\ntg9TTQVBEIZXe88E3w7XmecSDaQ3WBtUYcDYWestgvc5SDCG8uObczHoVHy4s5In3ziIydJNki4B\ngyqcUEUIS5MXDepa6WHJQKD76ER7CTpFKPPiZ2HURHLYVMTepgOUWMqZFDmB68et5vLkxXwn52a+\nP+UuQhRaPihbT52tYVjq6fA4+PvhlyloPjQs1xcEYWxrd5gBsLnsQz63t9vcoDb0/F0Eb2EQkmN0\nPHr3bGZOiKaszsr/vLSX3UXN/HDaWn4y8wdo5OpBXad30lp+w16sLhsTIsYhlUiZFTMNl8/N6yfe\nQyaRcf1Xth2dFJXNjT2vHTIdPb+V67G5ZgdHWo+zrXbnsFx/qCxO69dmsRMEYWTw+rxYnR3A2Qbv\nQLCOUAUmAo+VMW8RvM+DUI2C71+Tw3evzMYPvPjJcdZtakIj0Q/6GvEhsShlSio7AjPOsyOyAJgZ\nOw0Ar9/LkqT5xGiNp507MXICcomMQ61FwdccHgcvF71BibnsHGoGVqeNLbU7AKi21Z11l9T2unze\nL/vknMrS67Xj7/KHgr+MmSdsQRjNLE5rcLb42XSbdwVb3oHgPVa+F0TwPk8kEgnzJ8fx6N2zyUgI\nrAf/n5f2cqLaPKjzZVIZqbqk4N8nRIwDIEZrZFx4OuGqsGBil6/SyNWMjxhHvb2R1u42AHbW72Z/\ncyHvlnw0qI1S+rOhahMur4soTSQ+v48yS+WQr+H0uviwfENg9n3P2NbZ8vg8lFkqcPs8NHY2ndO1\nBEG4+Hq7zOHsgndvgyKiJ3iLMW/hrESHa/jZrdO5dkEaFpuL3715kHe3leH2nDkr26l6u87jQ2IJ\nU51std839bv8Ys6P0Mg1/Z471ZgDQKHpKG6vO9jF3dDZRPEQWt9ba3bwyJdP8u/yTznWVkx+w15i\ntEZuyroWgBJL+aCv1euQ6SgurwuA4z2T8c5Wja0Od8/yuHp74zldSxCEi6/9lAf6s+k27215R4gx\nb+FcyaRSrl6Qxs9vm05UuJpP99Twk7/ls257OW3W/j9YGeFpAEyMHN/ndaVM8bVj51OicpAg4ZCp\niJ3V+7C6bIw3ZAKwdZBj1XsaC1hX9gmm7jY+q97GXw79A5/fx9UZqxgXnoZMIqPEPPTgvaexIPj/\nJ9pLhnz+qcrMJ1v+9XbR8haEke7U4N3hPosxb3fgOzW8p8Ej0qOeByUlJVx++eW89tprp733+uuv\ns2bNGr797W/zm9/8ZjiLcdFkJITxyF2zWTk7GY/Hx/ovq/nPv+Xz29cK2Li3hlZr3+QCEyOyuDvn\nVlb20z0+EJ0ylPSwVCqt1aw7tgGpRMrt2TeRHpZCUdsJmnrypFudNho7m087/3h7Ca+deBeNXMNP\nZz3AHdlrGBeezqyYaUyNykEpU5IWlkydrYEud9egy2VxWik2l5GmTyZcFUaxuazPOvWhKrVWACBB\nIlregjAK9HabyyUybM6zWCrWE6y1Ci1qmWrMbEwybElaurq6+PWvf828efNOe89ut/OPf/yDzz77\nDLlczt13301hYSG5ubnDVZyLRqOSc9PSTK5ZmMbe483sOtxIaZ2V0jor72wrY3qWkRWzk8lMCEMi\nkTAjZupZ3yvXmEO5tRJTZxtzYmdgUIezNGkRFdZX2VKznZiQaNZXbsLtdbMq7XJWpV6GBAn7mg/y\ndvEHSJHwvcl3kqxLJFmXyJy4GX2unxWeQZmlklJLBVONkwZVpn1NB/HjZ07cDKqstexu2k+dvYFk\n3dASMQD4fD4qLFVEa6OQSWQ02Bvx+/1n3J5VEEaKbbVfEKbSMz16ysUuykXRG7wTdPFUd9Ti9Xn7\n7P3wdbrdgUaQRqZGLVePmW7zYQveSqWSF154gRdeeOG09xQKBQqFgq6uLrRaLd3d3YSFhQ1XUb4R\nVAoZC6fEs3BKPNZOFwdLTWw/2EBBsYmCYhP6ECXhIUrCdSpyM6OYmxODWjm0X89U4yTW9czovqxn\nfflUYw6R6gjyG/cBEKoIIUSuZUPlJsoslXh8biqs1Sikcu7M+TbjDOn9Xj/LkMmGqs2UmMv7Dd4+\nv4+C5kPEh8YSHxLL3qYDyCUypkdPRSNTs7tpPyfaSs8qeFdZ6nB4nUwPm4rL56Kxs5l2h5nIAVLP\nCsI3mcfnYV3px0SqDWM3eDvNhCpCiFJHUN1Ri81tJ1w1+HjQ7XEgl8pRyBSo5WpsZzHpbSQatuAt\nl8uRy898eZVKxf3338/ll1+OSqXiyiuvJC0tbbiK8o0TFqJkSW4Ci6fGU1xjYUtBHbUmO83mbmpa\n7Bwub+OdbWXMnxTHitlJRIX3P1HtVJGaCObFzSI8NDSYLlUqkbIy9TLeOPEel8TP4uqMVUiQ8Orx\ntznSehyAXONkrs9cTaTGMOD1U8OSUUjlA457r6/4jLzqrQAkhcbT0NlErnESIQot43tm0B83l7I8\n9dJB1elUJ1oDE+8yw9OwOjvYTyH19sbTgnedrYHdjfu5JvMKFFKRAVj45mpzmPHjp81hxul1oZIp\nL3aRLii/34/ZYSE2JAa9UgdAh9M2tODt7UYjC8wJ0sjUtHhMY6JH7qJ8s9ntdp577jny8vIIDQ3l\nzjvv5MSJE0yYMKHfcwwGLXL54LtSBsNo1J3X652N6Gg9C2cmB//eZu3msz015H1ZxZYDdXxeWM/S\nmUlctySTpJivL+9DxrtPe+0a41JWTVqIUqYIvvaLuP/gi+p9RGrDmRidNejyTjBmcKS5GJUO9Oq+\n5TnYeJS86q0YQyJJ0MVwqCnwcLB8/AKMRh1GdKSFJ1FhrUJvUKGSD+2L6lhJYKb67IzJ1Fkb+XcF\nWPztp/0e/1nyOXvrCpmTNoXpMZOHdI+L6ZvweRwOo7VecO51q2sMbEbkx49TaScxIuVrzrgwLtTv\nzOrowO3zEKc3EmuIhDqQaLxDur/T6yRUrcVo1KHXhuDr8BEeoUbZz/fLaPk8XpTgXV5eTlJSEhER\ngRbTzJkzOXr06IDB22we/CSrYk6vAAAgAElEQVSpwTAadZhM38zulcunxbNkSiz7jrfwcX4Vm/bW\nsGlvDQnGEGZkGZmbE0tshLbf8/uvW9+xoAkhge1Lh/JzSA1J4wjF7Co9GEwgA2B2WPjzvpeRS2Tc\nnX0ryfpEWtPaaepsJlmRFrxHpj6DSkstX5YdJucrs+oH4vf7OW4qw6AKh04Fod7Ak3lJcxWm6JPl\n9/g8HG4MPDQU1haTpEgd9D0upm/y5/FcjNZ6wfmpW1nTyV0Ji+oq0Hsv/hDQhfydVXcE6h8iDUXm\nDgTbOlMLSYrB37/T3U24KhyTyYbMFwhpNU2thKlOD9Ij8fPY38PGRVkqlpCQQHl5OQ5HIJgcPXqU\n1NTUi1GUbyy5TMq8SbE8ds8c/t81OeRmRtHc3s1Hu6r4r+d384e3DnKwxITHe/Yzt8/GpMjAA9ap\n2dz8fj8vF71Jp7uLG7OuDu4KFKWJYFJUdp/uq97kMweaDw0peUxzVws2p53M8DQkEgnhqjC0cg31\nnX1nnFdYq4LpEatO2R/9m8Dv959xpr8wdrV1twf/v3EMLn3sXSYWoQpHpwwFhpaoxe114/F5gt3m\n6p4/x8JysWFreR89epQnn3yS+vp65HI5GzduZOnSpSQmJrJs2TK++93vcscddyCTyZg2bRozZ84c\nrqKMaFKphNnZMczOjqHb6eFQeSufH2ygqMpMUZWZUI2CWdnRTEqLwOcDt8fLxEwvetX5HWLolRAa\nR5QmkqNtJ3B53ShlCmpsdZRbK8mJnMCC+LkDnp8RlkqE2sDupv24fC5unfAt1HLV1973RHtgvLt3\nLbxEIiEhNI4ySyUurwtlz1jh0bYTQGCsv6qjFp/fh1Qy9GfUbk83apn6vI6b5Tfs5Y3iddw7+Q5y\nBzlbXxjdTD0ZESGQUGms6Z1pHqE2BMe8h5KopatnZrlGEZgX1JsPYyxkWRu24D1p0iReffXVft+/\n+eabufnmm4fr9qOSRiVn7sRY5k6Mpa7FzvbCBvaeaGbbgXq2Hei7Ucfk9Eiunp9KRsL5ncUvkUiY\nZpzMpprPOd5ewlRjTnAm++LES7422ClkCn48435ePPoaB1oO09DZzHcm3kySLqHfc/x+P7sa9iCT\nSIPbrQLEh8ZRaqmgsbOZFH0gtWxRWzEKqYLJUdkcaDlMS5eJ2JCYIdWx3FLFnw8+x/KUS7kyffmQ\nzh3I7qb9gT8b943Y4O3wOCk0HWFGTK6YDHgetHa3oZapUctVY7JXpk/w7unmHkrL2+E5uUwMCDYE\nxsJyMZFhbYRKjA7l1uVZPP2D+Tx001RuWJzOzZeN47blWUzKiORIRRu/ebWAx18tYNeRRpxuLx6v\nj45OF91Ozzndu3eZWG/a0/1NhYSrwoKbqXydMJWeB6d9jyWJ82nqbObJfX/mnZIP6XJ3n/H4cmsV\nDZ1NzE6c1idtbEJoLHAyTWpbd2CMfbwhg3HhGQBUDrHr3OV189qJd/D4vWyu2X5WuZbPxOywUGEN\nTE461lZC5xAS3XxT+Pw+Xjn2Bq8ef4dd9XsudnFGPL/fT2t3O0ZNBPEhsVic1iElQPom8vv9HGg5\njN3VOajje/c6MKjD0SkC3eZn1/LumW3ek0JatLyFbzyZVMrk9Egmp0cGX1uzIpsvCmpYv7uaoop2\nyuqtvLzhBL6eMWaZVEJ2qoFZE6LRaZW0WR1Y7E4iw9QkGUNJMIYMuMY8RZ9IuCqMw63HGBeejsPr\nYHHiJUPqnpZJZXwr6xomR03k7ZIP2F6Xz66GvegUoWgVGiZHTWR12nIkEgk76vIBWJG5uM81epfD\n9QbvorZiAHIiJ5AaFmiJV3bUMC9+1qDLtb7yM1q6WonRRtPc1cKm6s+54SvbsJ6NgpbAXuixITE0\ndTZTaDrC/Pg5/R7f3GVid+N+LktaRKgy5Jzvfz5srNoWXF540HSYJUnzL3KJRrYOlw23z02kJpJI\njYFj7cU0dDaTGX7xls0WNBeyaf827p9yb3AMeijKrVX84+hrLE1aOKh/N+0OMwqpglBFCBKJhBC5\ndogt757saj1BWy3raXmPgW1BRfAepcYnGxifbMBk6Wbn4QaOVZlRKWRo1XJaLQ6OVrRztKK93/P1\nWgXGcA0JxhDGJxuYkGzAoAv8w5BKpOQaJ/F53S4+KF8PwNy4s5uzMCFiHP81+2G21uzgYMthujzd\nmLpayavagkIqZ17cLA6ajhAfEku2MZPW1pNP5XEhsUglUvY1HSQ7IouinvHunMgJhKvCUEgVVHUM\nvuVd1VHDlpodRGki+fGM+3h875/YWf8llyUvGtK60zM50HwYqUTKndlreHL/n9nfVNhv8Pb7/bx6\n7B0qO6o5ZDrK/VPv+do1+MOtqK2Y9ZWfYVCFE6rQUm6posNlC45TCkPXO95t1EQSGxINQONFDt75\nDfuo7WikuL20z2qSwaqwVAFQa6sf+MAe7Q4LEWpDcLhNp9INKUVq76Ykarm6z5+i5S2MeMZwDdcv\nyuD6RX1fbzF3caCkFZ/fT1SYGr1WicnSTW2LnYa2TlotDqqabJQ3dLDjUKBlOzHVwHUL08lICCPX\nOJnP63bR6e5iXHg60dqosy6jQipnRepSVqQuBQItkt/t/z8+rtjIifZSfH4fCxPmnTaerpIpuXn8\ndbxT/CF/O/wyUomU2JCYYNKWZF1iYPa5x/m1k+K8Pi+vH38PP35unXAjWoWWVamX8UbxOjZWbWPN\n+GvPun6t3W1U22rJjsgiWZ9IelgqpZYKLE4r4aow7K5OdJ6Ta1KL2k5Q2VFNmFJPc5eJPxT8H/fn\n3hPsafgqh8eJ1WklpicAnG+d7i7+WfQmMqmMeyffTrm1itrSjzlkKmJhQt8JihanlWcOPMfq9OXM\niLk46Y79fj8Or2PAXfi+CXq3743q6TYHLuo2t16flwprFQBVHbVnFbx7H5YbOpu+NlGKw+Ok09MV\nXJ0CoFeE0tTZjMfnQT6IORXdPcG7t+UtJqwJo160QcvKOcl9XpuQ0rd15/X5qG2xc7zazOGyNo5V\nmTlWVcCk9AhSYkJQSTQ4/d3MjumbA/1c6ZU6vj/lLv5Q8BdKLRWoZSpm9/NFMj9+Dsm6JF4uep3m\nLlNwKRtAalgS5dZKamx1ZBkyBrxnfuM+GjqbuCRuVvDYuXEz+ax6G/kNe7g8eXGf1q/L60IhVQxq\nNvqB5sMATI8O5K2fGZNLhbWK/c2F+Pw+NlRuIlJr4D+mriVMqeeTio1IkHB/7ncpbi9lXdkn/Png\n8/zP3P9Eqzg9IL1X+hF7mw7w6LyfYujZ0/hUFdZqPq/9guvHrf7aHgS/34/b5w7O3gfY1bCHTk8X\n12SsIkWfhE4ZyrrSjylsOXJa8N7beICW7lbyqrYG6zsYLq8bm8t2XlLdbqzexqeVm/jvOQ8TrTWe\n8/WGy8ngHWh5S5DQcBGXi1Xb6nD53ABD6rHq5ff7qew5r9PdRYfL1meOyleZnT3LxE75zPZ21dtc\n9jN+lr+qd2KaRv6VCWvnaamYw+WhuMZCTloEctk3a4qYCN5Cv2RSKamxelJj9ayak0JxjZkPdlQE\nu9zlcYnIIpp5c52dquwSkmJCcbq8uDw+dFoFxjANKqWMsjorxbUWup0eUmJ0JMeGEh6iQi6XIpdJ\nkMukKGRSdFoFWnUgC1x8aCzfnXQbfz/8CvMT5gS7w84kSRfPT2f9kMKWI31yrqfpA9mqqjpqBgze\nDo+D9RWfoZQpWZ2+4pT6y7gibRn/Ov42H1ds5Ds5gdURFqeVJ/Y+Q7hKz72T7xgw4NjdnexrPohM\nIiO3Z8/16dFTeK/0Iz4oCww5KGVKmuwmnj34AkuS5lNrb2BmTC4JoXEkhMbh9Lr5pHIjeVVbuH7c\n6j7X9/q8HDIdxev3Um6pPK211O4w89zhV7C7O1FIFdw+8aZ+y1ppreHd0n/TaG/ioenfJ1mfiNfn\nZXtdPiqZMhioI9QGUnRJlFjKsbs7CVWcHJM/0DO239DZRI2tjujoiWe816mqOmp4uehNzA4L/zX7\noWAX8tlweJxsqdmOx+/lWHvJNzx4B4atojSRKGVKIjURZ2yxbqr+nKauFm4Zf8OQNuwYqjLzyR37\nau0Ng2799mp3WPqMVzfYmwYM3r1r3A2qkw/FvTPO69rbMcQPJXhr6HZ6MFu8ANS2WvCm+5BJzz7g\n2rvdPP12IVVNNiamGvj+tZMIUSv6HOP3+2mzOuhyenB7fIRoFAMm0DqfRPAWBm18soGf3jodk6Ub\ns82JrWsSJXUW9tDM5oK6r78AcLza3O97MqmEZTOTuGp+KhqVnImR43l8/i/67GXudHnZdrAes83J\nkmnxxEUGAodKpjxtF7S0sEDPwsGWwzR2NlPcXsqMmFyuz1zd58txc812bG47V6YtO+3LZlbsNLbW\n7mRf8wGWJi0gSZfAW8UfYHPbsbntPLn/z9wz6fbTHg4+Ks9jV8Me7O7ArNtJkRPQKgL/qHXKUHIi\nJ3Ck9RhzY2dy/bjV7GjeySclW3ir+AMkSLgibVnwWpcnL+LLxr18XreLBQlz+gSkCmtVcNyvoqO6\nT/B2ed28cORf2N2daORq9jQVsCxl8WlL51xeF28Vf8CeppP7rr9RvI6fzPgBhaajWJxWFifO79MN\nnRs9iWpbLUdMx4ITAlu6TNTaG9ApQ7G57OQ37GVmRv/B2+ays6thD+srNwW3id3bdICrM1YGj2np\nMmFQhaOQKfq7TB+7G/cHfx7llkqWJH5zJ9W1drchlUgx9PSGxIfEcri1CJvb3mfN8ycVG/H4vWjl\nmvMyebI/JZbAngWzEqeyty6wb0DvEszBqOoIrKZI1SdT1VFDfWcj2ZH9r0CpszcAEBd68vMo8QRa\nzs/8ex8x8hamZgQ2aUruSQ3t9/sprrFgtjmZmhkVDN47D7SQv78Kr6wbzTQ4VtvC48UHuGd1dvA7\nopfT5aWqqQOFXEaoRo4+RHnaBF1bl4s/vFVITYudSL2aY1VmfvOvAu69aiJKuZRup5eiqnb2Hm+m\nse3kCgEJ8McHFqDXDn+OehG8hSGRSCREG7REGwKBaOaEaG66NJNjVe10dLpRK2Uo5FKsnS5Mlm66\nnB7S4/SMTw4nRK2gptlGTbOdTocbj9ePx+vD7fXh9fo4VmUmb28N+UcbmZUdg88XeF+nVRIVpkah\nkvPullI6Ol0AbN5fy8wJ0eRmRqFUSFGr5GQlhqHoyYEfrgojXBVGja2eGls9UomUrbU7CVPpuTw5\nMHPd7LCwuWYHYUodlyUvPq2+UomU6zNX8+fC53m/7BMWJszjSM8s+xkxubxb8m+eLXyBeybdFmz1\nH28rYWP1VkIVIUyKzCY2JPq05DV3TlyD1WkLtjJvz70Ba2cXO+u/ZG7cTGJOCdAKmYJrM6/kH0df\n44OyDXxvyp3B9w63Hgv+f2XPUjQIfMm9XfwBNbZ65sXNYnJUNs8f+RefVHzGPZNv71OWD8s3sKep\ngMTQeG4cdzX5jXvZ23SAHfVfUtB8CAkSliRe0uecXOMk/l3+KYWmI8HgfaDlCADXpK/ik8rP2N98\nCKfH1ec8v9/PZ9Xb2NN0gOauluDv6dvjr+elotfZ33yQq9JXIJFIqLHV8dS+Z5kRM5W7cm457Xfz\nVV6fl621O1FI5ShlSsotlYPaoGKgY6zODg60HGZRwrzz3uo1dbcRqTYErxsfEsPh1iIa7E3oIwLB\nalfDHjx+LwqpnK21O0nRJQ56LPpEeynR2igi1F8/2bF3vDtGG82s+EDwLm6rRC+JDk5U9fv9FBSb\n2Hu8mYmpEVwyKRalIlB2t8cXXJZ5SfwsqjpqzjgE4PH66HJ4CNHIg13zqT0PCGabky8OtEMcxMbI\naK1wkLe3hry9NYxPCmdqZhS7jzVR0xyYtKqUS4mc3AQK+KKwlYhQA5Myo9kLGMJkVJZ38OjL+1gy\nLQGdVoFcJqXG1EnB8WZcnr6ZKcNDlcRGaAlRK3B7fdSbOmnrcLBkWgK3LhvHuu0V5O2p4df/3N/n\nPIVcyrRxUUSGqVHIpUTp1eg0g3vQPFcieAvnTC6TMiVjcBPWemfBn4nb4yVvby3rv6xiSz8teZVS\nxtXzU0kwhrLhy2r2nWhh34mW4PuRejXXL05nzsQYuhweLglbTqWlFo0rHmeXjFL1ej4oW09bK9S1\ndVDpOYhf5cZemcPvKg6TmRDGkmkJfbq+xkdkMikym6Ntx6nqqEUhlXPLhBuJ1kYRFxLDXwpf5NXj\n75AYGk+YSs+7pf9GgoT/yL2XRF38GeuhkWv6tGQlEgk3ZV3D9OgppIWdvjnFNONkMsLSONxaRHF7\nGeMjMgE42nocpUxJjNZInb0xuDNVVUcNu5v2k6xLZE3WtcilclL0SRw0HaHGVhfckrW4vYztdfnE\nhsTw4xn3o5ApiA2Jpqj1BB+Wb8Dj8zApMvu07udorZH4kFhOtJfS1NlMbEgMB1sOI5fImGqcRGt3\nG3nVW9lTd5DskJOt72pbLR9V5KGUKcmOyCIzPJ0FCXMIVYSQa5zMnqYCKqzVZISnkle1FT9+9jcX\nsjRp4de2Ag+1FtHmaGdhwjzsLjsHTUcC66i1kf2es77iM75o2MPPZj14Wi5sh8fJXw79g3p7IyqZ\niku+Zsmhx+vD7fGhUX3916rD48Du7iRJl0Cb1UGLuQtHR+Azt6eqmFBPHCqlhK01u1BIlORKVlPg\n/5iXj77D57s7uG52LhnxYfj9flrM3XR0uUiL0wfHZYvaivnroX8gl8iJdObQVpbAtIwYls9KJj7q\n9KWHNbZ6nF4X4wzppBkCn78PCw7yVpmHiakG5k6MZfexJo5VBXrO9heb+GBnBTlpEdSbOqk3daLK\nKQKtBKM/AxkyihqreeJAAZ0OD50ON10OTzBo6kMUkFOFTqHH1aWgzGTln3knsLmlqOJg4fQILr1q\nIUWV7Ww5UMexKjPFtRYkEpg53khidCj5R5po67QhC4cVM9K55pIslHIp+z+XYoyUc+O1k/hn3gk+\n21fbp65xkVomp0cikQS6xi12F01tXZyoMYPED34pEgmsnJ3Mty7NCPzbvDST1FgdR8rbUCpkqBQy\nkmJCyc2MGtTveziI4C18YyjkMq66JJVLpyXQau1GIZMik0np6HTRau1GrpAzISks2CU1c7yRE9Vm\nmi3duN0+ms1d7DjUwAsfH+OtLaXYutw9Vw4HAl1bEm0uquw97LCsBxkglRDSmQGdKVR32Kho6OCz\nfbXkpEWQkxpBXKSWyDA100MXUtR2ArfPzUzdIurr/expqeR4tRmHcwLSlCM8svU5tM4E7OEm0hST\n8XXp4JR4UGeyU1jayvjkcDITwk5r7Ukl0n7H5iUSCTdmXcVT+57l7ZIP+PmsB2l3mGnpbmVqVA4G\nlYFaWz01HbWMM2RwoCUwSe7KtGXBLuer01fybOELrCv9mDsn3oxGruG1E+8ilUi5I/um4HE6ZSjX\nZl7J6yfeBeDSpAVnLNPK1KW8VPQGfzv0Mnfm3EydvYHJUdloFRrmxc8ir3orWyt2kT35ZPDe3Rjo\nmr9n0u3kRI7H6/Mh7fk5zIqZxp6mAvY1H0QjV3PIdJRwVRgWp5UPyzbwwLS1/baQ/X4/m6u3I0HC\n0qQFFLUVc9B0hHJrJUZtJH6/n621O0nTp2BrDeFAiQm1SkaBdA/dfhvvH9vM1emrUCqkeLx+XB4P\n71W9E8whkF+3H5UthTpTJ3UmOw2tnWhUCiL0KgyhKmpbAp8dt9fHsplJXLMg7bQv9RpbHW+cWMe3\nxl2D1xMIsjW1Xn6SF8hjgMKBeoqMPW272LHTi1RjR5lpx9OUzI6aLqSGSajGHaRCvZnH37GSEh6H\n2e7Eag/0boRqFEzPiiI2Qsv2zk8AcLukNCsP4c8sYeex2ew41EhWUjipsToSokJQKWU4XF4O2w4A\nYKrT8tv8Y/gT5EhCLGQmhvVMVG0HJExKj+DqS9I4VN7KtgP17C5qRiGXkhKnpUltxdep44lXD6PK\nCcGjaae1zoJWrUCrlhMXFUKIWo5KIeNEYwPQjaNZz8927Q7+jObOTOIQBdhcdpQKGdOyjEzLMlJn\nslNcY2FKRiTGni2SV1+SyhO799PQLeHGReODeSY0MjXdHgczJ0STnWqgrsWO0+2jsauRGePSiFL3\nfXjx+X3satjLx+VfEKuN4f4p9yKXS04bL+9NU92fbo8Dm8t2weZZiOAtfOOEahSEntL1FBuhJSsp\n/LQdgSQSCdmpEWSfcu7K2cm8v6OCY1XtTEw1kJkQRlJ0KGGhKvQhSuxdbg42xrKnYwvj9Flcl30Z\nUdrAhDOP18eBEhNbCuooqmynqLLvOnh57DgkWhs796rZSaCLWALEGcfRabfiCq3BrjHjdys4VmDk\nkV37yEmL4PIZiRSWtbLjUAO9e7HERGjJSgyjxdxNY1snSCTotUrCdUoSo0JJidURHxUSmP3t8SGT\nSTCEGlmUeAnb63bx/L4PsNukIINDB2Q4nXZU4+DVL/YwVQ97JYfQyNTEKJLZe7yZtg4HsyckMjFi\nPMfai/lV/hOEK8Mxu8wsT76UFH0SHV0uqho7aGjtotMRQYQ/Bb/Ei7VJR6O/E4fLS1N7F1a7i/R4\nPbkJU1iV2synVVt49uALAEwzTgECk7CywjM4Ziqlwd5EQ52URrONvc6D6JV6/B2R/N/2IxSWtiKR\nBFL/GvQKlCka9jcV0tppBcBZmY0qopoSSzmfHt/PZeOmo1Kc7L72eD3k1x5ie10+Ta5awr3JvJPX\nhNntACO88eVuPmhxERZroV7/OVJHOJ2HA0MYErUd9ZTA52lf6152btaCN/C5kyeUoEiowNsRgUTq\npcJfybGd+8EdmH+hVclptzmpaLAGyxIfraJbYuGzfbXsO9HCtHFRNLR2Ut/aSahGgSKxBJOqnj/u\n+weu+lQUyWBpk5OdYmBcYhhatQKTX0u+fQNhkw4j9SnpBlZlLiZzbjxJ0aEcMCfxXulHaCfto7po\nJjppJLMmRBOiUXCwxMSOQ41Ide2ospvxmo0YrZcQk1PDcQ4yfb4Na2ksJbUWSmotfT7byqxSZOFQ\nWOhD4rETkxSDVV7PA2smUNVi5qXiF8nWT+GeGZcikUjITAzjynkptHc4iTZoqO+s56n9PiYa01BN\njKE5JJYmSnj0+zkkhsWe9m98T4OXf52AhJAE4ifHotMqSYgKIXuchkP5H56WqCXRGEqisW/SGKlE\ngl/qRi1X90kQpZarcHgCSVpC1ArGJxuo7qjl+f2v0qScyp3jTw7BNHY280rRm8Hx9/KOClqcTSQp\n+0/X/FVllkq+qN9DoekIHp+Hxxf84oLkPxDBWxhVosI1rL06p9/3o8M1pMfP5QZO30BFLpMGn66b\nzV3UNttpbOvEbHOiVSvQazNRKmS40314vD6MYRompBgI1ShweHJ5Yt8zmLrbuDJtBZFJWew81Njn\nISAuUsvyWUkU11jYX2xiZ3sXEiAqXI1SIafV2k2dyT5g8hyJTIMyR0ORfz9+twaJFMJ9SYTo5TRQ\nSIurng2HD6HO6cDXlsBP/3YyjemHOytZOmMBCw1p7G3Zh5k2fJ06PlqnYIt6J/Zu91fuFngseo5j\nnIlGJSc9IZrw8FQs8iqkSImSpgbHkBclzqfEUs7vtr+O9UgusogmlJkOOqviePqLQM9AIJufjC6H\nh8ZWBxJVNPLYao5bjuHr1NHZEoG7XYYyp5GPKz7lg/UdpMeFExWmpra7ijbdPlAFelW81giaKlNp\ndJkACZoIOT5tOw6XFyuHkQE+tYVZU0JYnjuBgvY9bG+BcGk0FlpIm9RGRPdkrOoy6tUVKLw6EroW\n06GqwiY5yIw5HhbFTyXRGEp4qBKjUUdpZRttHQ6M4Sr+ceJlyq2VLEq/lvy9TrYeqA/+fi12J15H\nIzIV+GVOFEklANy+ZBoLU08dw05CX+4gr3oryGBixHiuzT25J/2loQuQS+W8Vfw+EdMO8tD0/0d8\nT5rg25ZlUVZv5d2a12lwwtq515Ibn4nfP5dffVlBhaOI39x8LX6vnPrWTupNdjxePyqlhPdatxIi\nj+Anty4gIyWSfx9382lVPRWWKvIat+KkiyL7fmzuZcHApFbKiY8KhJDe8e7ZKROYMzeHzTWtfFBW\nQouzhUROD951nYFhsZvnzmLcKb1NXl9gtvhXg7fX5+V4ewkqmQq9SodeGYq6p4Wt+cpKFLVcHcyZ\nDoGW9Tsl/8aPn4NNRXw78+RGRu+VfESdvYE5sTNID0vhzeL3+bJxX3CvBb/fT7vD0m+SpBPtpTxb\nGHhwNWoiWZQwL5jmdbiJ4C0IZxBj0BJjGPySD7VczX/krqXcWsnMmFykEilzJ8ZSXm9l19EmkqJD\nWTQ1DplUyuLcBG5zuIOtFqVCFuxV6HJ4qG2xUdVko8XcjUwmQSGX4nb7MNudWOxOFN4FVEs2IVF3\nk6pP5id3Byba/WLXdlzGTtKTPRyxQbg3hbjMKMYlhaFSyFj/ZTUb9/RmvppFWoqU2DAD5iQvZpuT\ntDg9aXE6kqJDCdUo0KjkdHa7qWmxU2eyo1HKiYnQotMqKK61cKS8jaIKM0jGoczoxu0I4Td7D6EP\nUSIhMGNXnhWJK6yZCZOdeMI7qHdCnGw8CTmxLJ2eQHq8PtgV3uXw8FmRnk09E+8Wxy/i+pUL8Pr8\nPF9opURyFHXudqpaYqnqciM31oNfQrgziylhM5icnkLEYjUhajlatZy/Ha7keHsJ994Sxd8Ot6OU\nKnH5XGRM7CIjPoxPmqsA+P/t3XlU1XXewPH35S5clsvOvQiyCC6oLIqKkopmZuNYNmIuh9DHmbG0\n1aemzDqc7DweLc2xGpvz1CntmVGcMuWUTZZLhdKIFlmYKCqKsgmyiOz77/njwk3k4prShc/rv/u7\nP+T78ce5n/vdPt8l0Qt4PX09VdqTTIoM4V9ZB3HWOvHs6EWYnIxUNobx0rc/UaM/R3jwdMszV6lU\nuBvscTfYs+3kDk5fytW1lZMAABPYSURBVAGg2eM0ry2axcXqBvp6OWOvU9Pa2srS1K/R2bkR7jWY\nb8+nARDs1bnwzrTgKeRXF3K0LMvqlMV4vzGoVWqSsj5mQ2YSL4x8Cp1ah52dCq1LJYUN5wh1H0CU\n34C2hqoZ5zuGf+fs4mDRD0zsO5b+fq70bzu06NTF0zSVNDLUewBBPi54uOgJcjHv1PjwRDKXGqvw\n0LtTXn+Rb/K+5cGQqYB5y2TOpVyMjl5kt8Ue1LbDo73oTGF1EVHGiE4xnKvMQ4UKf0PfDtfVdmqc\ntI5caqjssJBwZ84e8xeay2jtNDS1NncqXqRX66lvbrCcKPh90Y+crcw139/SxImL2YR7DaGqsZoT\nF7MJcglg/pA5tLS28HnOHr4v+pEZIdPQqrV8cnone3P3MXdQXKeaBmBeUAjmaaBh3mG/6imE1yLJ\nW4hfiaeDe6dv6CF+rlZPdjPPA3Zeleqo11x1UV+7LVmV/KfwEJFev4wyBLsG8sOFDE7VHkWv1vM/\ns+/vcPLXuPA+fH3YvM1ubLiPZfvNtQwO6ryPPXqwyVzJrLGFuoZmquvGcK6oikxDOafyL6HV2OHt\n5oqH4yQy2U6F6w9UNlSbv2xMmmT19zjqNTwYNZwT6d+iKK3MjhpvGQ59JGoWn51xIb34R1r7nAXM\nte0TBs+yLL67Un+3fhwvP8nmtrn7eUNm83+Z/+KHCxnE9o3hdEUOAQY/jI5eTOw7lp1n97IlazsO\nGj1PDnvEUrHORWcg1GMAx8tPUlJb1mkB3PdFP/JN/rf4OJlQq+z4qeQoM/rfT4jvL8+wvKGCupY6\nBnsOYPag6VQ0VnC2Mhdvh84LPe1UdiwMm0d+daHVxYtgXtFdUF1ISv5/+PjkDh4e/BA1TbV80lau\nuL1aYbtxfqP58uxe9ucfINYvxvL/2tTazNaTnwLmNQft2hcHXmqswtvBk7+MeIKVh9axPz+NKYET\naW5t4Y0f/pfS+l9GiRw1Dhjb4mkfDWg/5rSwuoiaphoGuIfQ0tpCblUBvs4+Visf9nMJ5GjZcX4u\nPUaE91AqGi7xVV4qrjoDY/qMoqqxisrGaiobq6hqrO50Qp+Dxh4FhcYW81qAT07vRGunIWHwbD7I\n3MKRkmOEew0ho+QoCgrDjeaRDbWdmtE+I9iTm0JGaSbeDp58lbsfgO2ndhDiGmSJC6C2qY4jpccw\nORrveOIGSd5C2KSHBjxAoEtfRl72gduvLXnXt9QzyhTV6chOnVbdqarerVCpVDjYa3Cw1+DhoifA\nZGB8ZMfV9d7eBjYcLGNXW69pTJ+rV+NTqVQ8O+JxgA7zmI5aB+YM+gNxA+4ns/Q4ja1NjDBGXnX7\nVoiruUZ4VWM1/s6+DPcO55BHOkfLskgtOEiL0sLQtop8E/zH8lXefhRF4fHIP+F/xS6BkaZhHC8/\nSXrxT0ztd4/lelFNMUlZ29Cr9TwaNo+zlXn88/hH7Cv4D3H9fymok1tlHiYOMPRFbadmccQCmlqb\n0XWxf12r1naZuNv9of80TlfkcOD8d9irdaQX/0RVUzVDPAcxwC24w70GnTNRpki+KzpMVvkphngO\nAmDX2a8orClinO9oBrgHd7jf6OhFWd1F/jT0YQw6Zyb5j+fTM1/wVe5+jpWfpLS+nFGmKHRqLcW1\nFwjzHGxJYK46F5w0jhRWnyejJJMPMpNobm3hLyMeR2Onoam1ybJF7Eoz+v+eY+Un2HZqB6EeA9mZ\ns4em1iamBU+/6mE+7doLOpXXV1hOBvx90GSijBFsz97B0bLjtCqt/Ni2tbF9nQZATJ+R7MlN4duC\ng9Q01aKgMDlgAntz9/FB5haeH/mU5Zn9WHKE5tZmon2i7njiBjkSVAibpFPrGOs7GvvLypgGX/Zh\n396b+C34XdAkPPTuaO20jLiOkqlaO02XZ4Vr7TQMM4YT7RN1zX3XgS7+qFXmeyYHTEClUllKtn6e\nsweAIW3J21nrxPMjn+LF6GcIdg3q9G9FeoehsdPwXfEPlnlZRVH414lkmlqbSBg8C5OTkShTJAad\nMwcKv7MsmoJfDupoHyVQqVRdJu7rpbXT8Kewh7FX6/gm/1vqW+p5MGQqi8MXWE0m7QVrdp/7hsJq\ncwW8Xee+wd3ejT/0n9bp/kfC5lsq7QGM7zsGvVrPF2e/4lxlHtE+UfzXkDnEh87kmajHuDdwouVn\nVSoVvs4+lNSV8d7P/0SFCgWFzcc/JrvCPMTe1dY/HycTk/zHU1Z/kS1Z2zlQ+D0mRyNjfK7v8KP2\n5L06/W8cKvoBo4MX9wZOxE5lR1SfcCobqzhWdoKTFacJdPHvMFpmcjJazh4orClirO9oZvSfRqxf\nDIU1RSRn/9ty73dF5hX6XZVuvt0keQvRQ/g590Frp8FerWPIdZ6tfifo1Dr+e/hinhvxhKXK3J35\nvVoivIfS19mX4W3zrhHeQy09PyeNY4feXx8nU5cH7Dho9Iw0DuNCbSnbsz8DYN/Zg2RX5BDpNdTy\nZUlrpyHWL4a65voOFetyK8097/aFUL8Wo6M3fw5LYIzPSF6KfpYpgXd3+aUm0MWfkLbEtPK7daz+\n/m+0Kq3Eh87stOgLzEPf7VUKwVyboH3ed5B7fx4OfeiqPc72IWYnrSNLohYR63cXRbUX+OzMlwCW\neXVrpgbdg6vOhe+LD6Og8GDI7667SI6bzlwlUa+258HgqSwd9bRlgdpIP/PfwdaTn9KqtFqdj7+r\nj3k/v6vOhRn9fw/AjP734+vkQ2pBGjtz9lBWV052RQ4D3IKvqwjO7SDD5kL0EJq2eT2Nnea6y4ne\nKebezZ3/kFsYlmBZuATmJDzUYxAZpZkM9hx4Q2fQzxo4ndyqfPblH8BN58rXBano1DoeGji9w33j\n/WLYdfZrvslLZbzfGHOt8KoCvBw8rR4sc6uGeoZahv+vZXHEAtKLMzhbmUteVQFDPUMtQ+jXY1q/\ne/F19iHCa8g1656P6TOSSw1VTA++D5OTER9HI0fLjlNefxF7tY4+Tl3vmdZr9MQNuJ8PMrcQ7BpI\nhFfXO0iuNNF/LL7OfQj1GNBhZAogwhSKRqWmrG2ufrh35xGqEaZh5FSeI9pnhKWQkk6t5bHIP/Lm\n4Xf5PGcPP5UcBSDa59c9lOlGSPIWogcZ2U3HcP6WXZmg7/KNJqM084b/r/QaPYsiFrAm/W98euYL\nAGb0n9ap52XQORPtE8WB89+TUZKJv8GPmuZaS1W87uSodSS2bwyxxNzUz2vVWqJ9oq7r3gBDXx65\nrBSvXqMnPnQmb//0PkEuAdf84jTCGImdyo5+LgE3NKfsoHEg0tt6stdr9Qx078+x8hMEGvytHiqk\nU2uJD32o03UPvTtLhi/irR/foaD6PFo7TbdOT8mwuRCiVwnzGsya8a8Q7nXtE8+u5OXgwSNh87BT\n2RHo1pe7+1qvPjc5YAIqVOw5l9JhsVpvN9hjII9H/pm5g2Zc817zGoWI6zoa9Ea0J/YoU+ch82vx\ndHBnyfDFBBr8meQfa3W64U6RnrcQotdxuoW59wHuISwfs5SgPiaqK64sbGNmcjIS6T2Un0qO8k1e\nKvDrz3fbqqE3MEx/O8T0GYWT1omIm/jyBuYEvnTUU79yq26c9LyFEOIGeTl44KC9eq+rffX1mbai\nMwGSvH8T1HZqhhvDb+vZ6HeCJG8hhLgNglwCLPutvfQed3Slvej5JHkLIcRtcm/g3UDXe5qFuFky\n5y2EELfJEI+B/HFoPP2usqdZiJshyVsIIW4TlUol2/fEbSHD5kIIIYSNkeQthBBC2BhJ3kIIIYSN\nkeQthBBC2BiVoihKdzdCCCGEENdPet5CCCGEjZHkLYQQQtgYSd5CCCGEjZHkLYQQQtgYSd5CCCGE\njZHkLYQQQtiYXpm8V61axZw5c5g7dy5Hjhzp7ubcsjVr1jBnzhxmzpzJ7t27OX/+PPPmzSM+Pp4l\nS5bQ2NjY3U28afX19UyePJnk5OQeFdeOHTuYPn06cXFxpKSk9IjYampqePLJJ5k3bx5z584lNTWV\nrKws5s6dy9y5c1m+fHl3N/GGnTx5ksmTJ7N582aALp/Tjh07mDlzJrNmzeLjjz/uziZfN2uxLViw\ngISEBBYsWEBJSQlge7FdGVe71NRUBg0aZHlta3F1ovQyhw4dUh599FFFURQlOztbmT17dje36Nak\npaUpCxcuVBRFUcrLy5UJEyYoy5YtU3bu3KkoiqL89a9/VZKSkrqzibdk3bp1SlxcnLJ9+/YeE1d5\nebkyZcoUpaqqSikuLlYSExN7RGybNm1S1q5dqyiKohQVFSn33XefkpCQoGRkZCiKoijPPvuskpKS\n0p1NvCE1NTVKQkKCkpiYqGzatElRFMXqc6qpqVGmTJmiVFZWKnV1dcq0adOUixcvdmfTr8labEuX\nLlU+//xzRVEUZfPmzcrq1attLjZrcSmKotTX1ysJCQnK2LFjLffZUlzW9Lqed1paGpMnTwYgJCSE\nS5cuUV1d3c2tunmjRo3irbfeAsDFxYW6ujoOHTrEPffcA8Ddd99NWlpadzbxpp0+fZrs7GwmTpwI\n0GPiSktLIyYmBmdnZ4xGIytWrOgRsbm7u1NRUQFAZWUlbm5uFBQUEBERAdheXDqdjvfeew+j0Wi5\nZu05ZWRkEB4ejsFgQK/XExUVxeHDh7ur2dfFWmzLly/nvvvuA355lrYWm7W4AN555x3i4+PR6XQA\nNheXNb0ueZeWluLu7m557eHhYRkeskVqtRpHR0cAtm3bRmxsLHV1dZY/Uk9PT5uNb/Xq1Sxbtszy\nuqfElZ+fT319PYsXLyY+Pp60tLQeEdu0adMoLCzk3nvvJSEhgaVLl+Li4mJ539bi0mg06PX6Dtes\nPafS0lI8PDws99jCZ4q12BwdHVGr1bS0tLBlyxYeeOABm4vNWlw5OTlkZWUxdepUyzVbi8uaXn+e\nt9JDqsPu3buXbdu2sXHjRqZMmWK5bqvxffLJJwwbNgx/f3+r79tqXO0qKip4++23KSwsZP78+R3i\nsdXYPv30U3x9fdmwYQNZWVk88cQTGAwGy/u2GldXuorHluNsaWlh6dKljBkzhpiYGD777LMO79ti\nbK+++iqJiYlXvccW4+p1ydtoNFJaWmp5feHCBby9vbuxRbcuNTWVd955h/fffx+DwYCjoyP19fXo\n9XqKi4s7DSHZgpSUFPLy8khJSaGoqAidTtcj4gJzj2348OFoNBoCAgJwcnJCrVbbfGyHDx9m3Lhx\nAISGhtLQ0EBzc7PlfVuN63LW/gatfaYMGzasG1t581588UUCAwN58sknAeufl7YUW3FxMWfOnOG5\n554DzO1PSEjgqaeesum4oBcOm48dO5Zdu3YBkJmZidFoxNnZuZtbdfOqqqpYs2YN7777Lm5ubgDc\nddddlhh3797N+PHju7OJN+XNN99k+/btbN26lVmzZvH444/3iLgAxo0bx8GDB2ltbeXixYvU1tb2\niNgCAwPJyMgAoKCgACcnJ0JCQkhPTwdsN67LWXtOkZGR/Pzzz1RWVlJTU8Phw4cZOXJkN7f0xu3Y\nsQOtVsvTTz9tuWbrsZlMJvbu3cvWrVvZunUrRqORzZs323xc0EtPFVu7di3p6emoVCqWL19OaGho\ndzfppn300UesX7+efv36Wa699tprJCYm0tDQgK+vL6+++iparbYbW3lr1q9fj5+fH+PGjeOFF17o\nEXF9+OGHbNu2DYDHHnuM8PBwm4+tpqaGl156ibKyMpqbm1myZAne3t68/PLLtLa2EhkZyYsvvtjd\nzbxuR48eZfXq1RQUFKDRaDCZTKxdu5Zly5Z1ek5ffvklGzZsQKVSkZCQwPTp07u7+VdlLbaysjLs\n7e0tnZmQkBBeeeUVm4rNWlzr16+3dGwmTZrE119/DWBTcVnTK5O3EEIIYct63bC5EEIIYeskeQsh\nhBA2RpK3EEIIYWMkeQshhBA2RpK3EEIIYWMkeQshbllycrKlEIYQ4vaT5C2EEELYmF5XHlWI3mzT\npk188cUXtLS0EBwczMKFC1m0aBGxsbFkZWUB8MYbb2AymUhJSeHvf/87er0eBwcHVqxYgclkIiMj\ng1WrVqHVanF1dWX16tUAVFdX89xzz3H69Gl8fX15++23UalU3RmuED2W9LyF6CWOHDnCnj17SEpK\n4qOPPsJgMHDgwAHy8vKIi4tjy5YtREdHs3HjRurq6khMTGT9+vVs2rSJ2NhY3nzzTQCef/55VqxY\nwebNmxk1ahT79u0DIDs7mxUrVpCcnMypU6fIzMzsznCF6NGk5y1EL3Ho0CFyc3OZP38+ALW1tRQX\nF+Pm5kZYWBgAUVFR/OMf/+Ds2bN4enri4+MDQHR0NB9++CHl5eVUVlYycOBAABYsWACY57zDw8Nx\ncHAAzDWlq6qq7nCEQvQekryF6CV0Oh2TJk3i5ZdftlzLz88nLi7O8lpRFFQqVafh7suvd1VRWa1W\nd/oZIcTtIcPmQvQSUVFR7N+/n5qaGgCSkpIoKSnh0qVLHDt2DDAf6zlo0CCCgoIoKyujsLAQgLS0\nNCIjI3F3d8fNzY0jR44AsHHjRpKSkronICF6Mel5C9FLhIeH8/DDDzNv3jzs7e0xGo2MHj0ak8lE\ncnIyr732GoqisG7dOvR6PStXruSZZ56xnKW+cuVKAF5//XVWrVqFRqPBYDDw+uuvs3v37m6OToje\nRU4VE6IXy8/PJz4+nv3793d3U4QQN0CGzYUQQggbIz1vIYQQwsZIz1sIIYSwMZK8hRBCCBsjyVsI\nIYSwMZK8hRBCCBsjyVsIIYSwMZK8hRBCCBvz/67espGXTjeOAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f1232fcf8d0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "colab_type": "code",
        "id": "YYn-LSmlVFg6",
        "outputId": "dc8e7993-9bd6-4e9b-d83a-3e146094754d"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "plt.subplot(211)  \n",
        "plt.plot(history.history['acc'])  \n",
        "plt.plot(history.history['val_acc'])  \n",
        "plt.title('model accuracy')  \n",
        "plt.ylabel('accuracy')  \n",
        "plt.xlabel('epoch')  \n",
        "plt.legend(['train', 'test'], loc='upper left')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f1232a2d198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAADECAYAAAC2lamMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWwghhFAZKd5CCCGEyvx//HkSw0ggeLwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f1233912518>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sum-8hWpU0ZR"
      },
      "cell_type": "code",
      "source": [
        "#model.save(\"C:\\\\Users\\\\Manasa\\\\Desktop\\\\Srikanth Assignment\\\\cifar.h5\")\n",
        "\n",
        "model.save('model2.h5')\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"model2.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tSndPHEnDh_H"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import model_from_json\n",
        "from keras.models import load_model\n",
        "\n",
        "# serialize model to JSON\n",
        "#  the keras model which is trained is defined as 'model' in this example\n",
        "model_json = model.to_json()\n",
        "\n",
        "\n",
        "with open(\"model_n.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model_nu.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "C4lNx9pXAxj3"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vKCowD2M7Bmn"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5Xnhrd1b6keA"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "cuVkfD7YgW8u"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "FTJdUJVkJLWJ"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nbzKmSVlIrfx"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-_afOcsIt67y"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "zTrq5-EDEhgq"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "EjAvhjGTEiCm"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "je9nD6Qgt3ZY"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qKaaNstdD-U4"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "p2M_vmR9D0Oi"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OnprQiO1t1jj"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "s0IlHWyptwK3"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-jC7Oo7HtsJ7"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Py0WQKLHrrUp"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xzkowKeHpEh0"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gWLOn3ObnzEK"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yUh5H4qwnopA"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NjLsTBRw-Fyc"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}