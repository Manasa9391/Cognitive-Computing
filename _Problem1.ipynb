{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Copy of Problem1.ipynb","version":"0.3.2","provenance":[{"file_id":"1tmrD_CWJ3PP5gJHGt-Zfl8NB19g2AXXs","timestamp":1539060646764},{"file_id":"https://github.com/Manasa9391/Cognitive-Computing/blob/master/Problem1.ipynb","timestamp":1539042335022}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"aJnMU23BsTpF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b44e8ae8-24fd-48eb-b0b5-6fe553733838","executionInfo":{"status":"ok","timestamp":1539091517307,"user_tz":240,"elapsed":1784,"user":{"displayName":"Manasa Singhekar","photoUrl":"","userId":"13090086925392106704"}}},"cell_type":"code","source":["from __future__ import print_function\n","\n","import keras\n","\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","#from keras.optimizers import RMSpro\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"1yy7RMnP3va9","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.datasets import cifar10\n","import numpy as np\n","from keras.utils import np_utils\n","from keras.models import Sequential\n","from keras.layers.core import Dense, Dropout, Flatten,Activation\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras.callbacks import ModelCheckpoint\n","from keras.preprocessing.image import ImageDataGenerator"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dXSNq8Gv96Ty","colab_type":"code","colab":{}},"cell_type":"code","source":["def save_history(history, result_file):\n","    loss = history.history['loss']\n","    acc = history.history['acc']\n","    val_loss = history.history['val_loss']\n","    val_acc = history.history['val_acc']\n","    nb_epoch = len(acc)\n","\n","    with open(result_file, \"w\") as fp:\n","        fp.write(\"epoch\\tloss\\tacc\\tval_loss\\tval_acc\\n\")\n","        for i in range(nb_epoch):\n","            fp.write(\"%d\\t%f\\t%f\\t%f\\t%f\\n\" %\n","                     (i, loss[i], acc[i], val_loss[i], val_acc[i]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qD46C2OG51rL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":3995},"outputId":"c2d4c2bf-ea3b-4983-af95-9e986b30577b","executionInfo":{"status":"ok","timestamp":1539096463011,"user_tz":240,"elapsed":491672,"user":{"displayName":"Manasa Singhekar","photoUrl":"","userId":"13090086925392106704"}}},"cell_type":"code","source":["if __name__ == '__main__':\n","    nb_epoch = 100\n","    batch_size = 128\n","    nb_classes = 10\n","\n","    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n","\n","    X_train = X_train.reshape(50000, 32 * 32 * 3)\n","    X_test = X_test.reshape(10000, 32 * 32 * 3)\n","\n","    X_train = X_train.astype('float32')\n","    X_test = X_test.astype('float32')\n","    X_train /= 255.0\n","    X_test /= 255.0\n","\n","    Y_train = np_utils.to_categorical(y_train, nb_classes)\n","    Y_test = np_utils.to_categorical(y_test, nb_classes)\n","\n","    # MLP\n","    model = Sequential()\n","    model.add(Dense(1024, input_shape=(3072, )))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(512))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(512))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(10))\n","    model.add(Activation('softmax'))\n","\n","    model.compile(loss='categorical_crossentropy',\n","                  optimizer='adam',\n","                  metrics=['accuracy'])\n","    model.summary()\n","\n","    # training\n","    history = model.fit(X_train, Y_train,\n","                        batch_size=batch_size,\n","                        nb_epoch=nb_epoch,\n","                        verbose=1,\n","                        validation_data=(X_test, Y_test))\n","\n","    save_history(history, 'history.txt')\n","\n","    loss, acc = model.evaluate(X_test, Y_test, verbose=0)\n","    print('Test loss:', loss)\n","    print('Test acc:', acc)\n","    \n"],"execution_count":19,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_13 (Dense)             (None, 1024)              3146752   \n","_________________________________________________________________\n","activation_13 (Activation)   (None, 1024)              0         \n","_________________________________________________________________\n","dropout_10 (Dropout)         (None, 1024)              0         \n","_________________________________________________________________\n","dense_14 (Dense)             (None, 512)               524800    \n","_________________________________________________________________\n","activation_14 (Activation)   (None, 512)               0         \n","_________________________________________________________________\n","dropout_11 (Dropout)         (None, 512)               0         \n","_________________________________________________________________\n","dense_15 (Dense)             (None, 512)               262656    \n","_________________________________________________________________\n","activation_15 (Activation)   (None, 512)               0         \n","_________________________________________________________________\n","dropout_12 (Dropout)         (None, 512)               0         \n","_________________________________________________________________\n","dense_16 (Dense)             (None, 10)                5130      \n","_________________________________________________________________\n","activation_16 (Activation)   (None, 10)                0         \n","=================================================================\n","Total params: 3,939,338\n","Trainable params: 3,939,338\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n","  warnings.warn('The `nb_epoch` argument in `fit` '\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 50000 samples, validate on 10000 samples\n","Epoch 1/100\n","50000/50000 [==============================] - 6s 113us/step - loss: 1.9677 - acc: 0.2824 - val_loss: 1.7424 - val_acc: 0.3779\n","Epoch 2/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.7999 - acc: 0.3524 - val_loss: 1.7272 - val_acc: 0.3676\n","Epoch 3/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.7424 - acc: 0.3723 - val_loss: 1.6550 - val_acc: 0.4147\n","Epoch 4/100\n","50000/50000 [==============================] - 5s 99us/step - loss: 1.7042 - acc: 0.3851 - val_loss: 1.6009 - val_acc: 0.4351\n","Epoch 5/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.6652 - acc: 0.4017 - val_loss: 1.5777 - val_acc: 0.4415\n","Epoch 6/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.6492 - acc: 0.4042 - val_loss: 1.5864 - val_acc: 0.4376\n","Epoch 7/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.6300 - acc: 0.4129 - val_loss: 1.5899 - val_acc: 0.4286\n","Epoch 8/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.6072 - acc: 0.4232 - val_loss: 1.5274 - val_acc: 0.4658\n","Epoch 9/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.5945 - acc: 0.4260 - val_loss: 1.5276 - val_acc: 0.4580\n","Epoch 10/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.5825 - acc: 0.4297 - val_loss: 1.5152 - val_acc: 0.4605\n","Epoch 11/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.5646 - acc: 0.4361 - val_loss: 1.4888 - val_acc: 0.4735\n","Epoch 12/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.5549 - acc: 0.4410 - val_loss: 1.4820 - val_acc: 0.4714\n","Epoch 13/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.5460 - acc: 0.4418 - val_loss: 1.4953 - val_acc: 0.4697\n","Epoch 14/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.5357 - acc: 0.4476 - val_loss: 1.5195 - val_acc: 0.4589\n","Epoch 15/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.5368 - acc: 0.4440 - val_loss: 1.4926 - val_acc: 0.4716\n","Epoch 16/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.5130 - acc: 0.4576 - val_loss: 1.4750 - val_acc: 0.4763\n","Epoch 17/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.5027 - acc: 0.4589 - val_loss: 1.4653 - val_acc: 0.4810\n","Epoch 18/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.4959 - acc: 0.4633 - val_loss: 1.4690 - val_acc: 0.4786\n","Epoch 19/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.4864 - acc: 0.4648 - val_loss: 1.4510 - val_acc: 0.4866\n","Epoch 20/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.4870 - acc: 0.4657 - val_loss: 1.4424 - val_acc: 0.4845\n","Epoch 21/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.4754 - acc: 0.4701 - val_loss: 1.4261 - val_acc: 0.4957\n","Epoch 22/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.4638 - acc: 0.4708 - val_loss: 1.4355 - val_acc: 0.4905\n","Epoch 23/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.4580 - acc: 0.4747 - val_loss: 1.4368 - val_acc: 0.4849\n","Epoch 24/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.4574 - acc: 0.4782 - val_loss: 1.4418 - val_acc: 0.4852\n","Epoch 25/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.4459 - acc: 0.4810 - val_loss: 1.4336 - val_acc: 0.4954\n","Epoch 26/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.4366 - acc: 0.4828 - val_loss: 1.4294 - val_acc: 0.4917\n","Epoch 27/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.4390 - acc: 0.4784 - val_loss: 1.4150 - val_acc: 0.4919\n","Epoch 28/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.4309 - acc: 0.4845 - val_loss: 1.4265 - val_acc: 0.4822\n","Epoch 29/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.4274 - acc: 0.4861 - val_loss: 1.4365 - val_acc: 0.4955\n","Epoch 30/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.4148 - acc: 0.4927 - val_loss: 1.4267 - val_acc: 0.4907\n","Epoch 31/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.4118 - acc: 0.4938 - val_loss: 1.4132 - val_acc: 0.4979\n","Epoch 32/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.4047 - acc: 0.4949 - val_loss: 1.4348 - val_acc: 0.4957\n","Epoch 33/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.4056 - acc: 0.4939 - val_loss: 1.4144 - val_acc: 0.5007\n","Epoch 34/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3921 - acc: 0.4975 - val_loss: 1.4018 - val_acc: 0.5094\n","Epoch 35/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3966 - acc: 0.4956 - val_loss: 1.3984 - val_acc: 0.5069\n","Epoch 36/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3837 - acc: 0.5014 - val_loss: 1.4106 - val_acc: 0.5033\n","Epoch 37/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3834 - acc: 0.4996 - val_loss: 1.3990 - val_acc: 0.5000\n","Epoch 38/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3769 - acc: 0.5045 - val_loss: 1.4063 - val_acc: 0.4989\n","Epoch 39/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3875 - acc: 0.5025 - val_loss: 1.3896 - val_acc: 0.5063\n","Epoch 40/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3712 - acc: 0.5052 - val_loss: 1.4205 - val_acc: 0.4935\n","Epoch 41/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3710 - acc: 0.5089 - val_loss: 1.3956 - val_acc: 0.5049\n","Epoch 42/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3646 - acc: 0.5095 - val_loss: 1.4014 - val_acc: 0.5020\n","Epoch 43/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3675 - acc: 0.5063 - val_loss: 1.3744 - val_acc: 0.5169\n","Epoch 44/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3620 - acc: 0.5089 - val_loss: 1.3948 - val_acc: 0.5062\n","Epoch 45/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3530 - acc: 0.5110 - val_loss: 1.3994 - val_acc: 0.5019\n","Epoch 46/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3521 - acc: 0.5138 - val_loss: 1.3910 - val_acc: 0.5076\n","Epoch 47/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3481 - acc: 0.5133 - val_loss: 1.3957 - val_acc: 0.5035\n","Epoch 48/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3517 - acc: 0.5146 - val_loss: 1.4109 - val_acc: 0.5018\n","Epoch 49/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3391 - acc: 0.5164 - val_loss: 1.4244 - val_acc: 0.4998\n","Epoch 50/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3415 - acc: 0.5146 - val_loss: 1.3891 - val_acc: 0.5071\n","Epoch 51/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3321 - acc: 0.5192 - val_loss: 1.3869 - val_acc: 0.5086\n","Epoch 52/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3313 - acc: 0.5229 - val_loss: 1.4273 - val_acc: 0.4935\n","Epoch 53/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3303 - acc: 0.5219 - val_loss: 1.3970 - val_acc: 0.5063\n","Epoch 54/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3196 - acc: 0.5222 - val_loss: 1.3750 - val_acc: 0.5167\n","Epoch 55/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3254 - acc: 0.5220 - val_loss: 1.3763 - val_acc: 0.5126\n","Epoch 56/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3186 - acc: 0.5267 - val_loss: 1.3854 - val_acc: 0.5094\n","Epoch 57/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3202 - acc: 0.5231 - val_loss: 1.4190 - val_acc: 0.4978\n","Epoch 58/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3163 - acc: 0.5267 - val_loss: 1.4010 - val_acc: 0.5018\n","Epoch 59/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3130 - acc: 0.5286 - val_loss: 1.3879 - val_acc: 0.5122\n","Epoch 60/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3139 - acc: 0.5252 - val_loss: 1.3925 - val_acc: 0.5089\n","Epoch 61/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3067 - acc: 0.5291 - val_loss: 1.3894 - val_acc: 0.5087\n","Epoch 62/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3014 - acc: 0.5318 - val_loss: 1.3927 - val_acc: 0.4982\n","Epoch 63/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2983 - acc: 0.5307 - val_loss: 1.3810 - val_acc: 0.5020\n","Epoch 64/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2965 - acc: 0.5328 - val_loss: 1.3793 - val_acc: 0.5136\n","Epoch 65/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2875 - acc: 0.5351 - val_loss: 1.3723 - val_acc: 0.5167\n","Epoch 66/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2874 - acc: 0.5371 - val_loss: 1.3912 - val_acc: 0.5091\n","Epoch 67/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2859 - acc: 0.5372 - val_loss: 1.3795 - val_acc: 0.5139\n","Epoch 68/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2858 - acc: 0.5345 - val_loss: 1.3864 - val_acc: 0.5067\n","Epoch 69/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2848 - acc: 0.5370 - val_loss: 1.3883 - val_acc: 0.5060\n","Epoch 70/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2834 - acc: 0.5363 - val_loss: 1.3951 - val_acc: 0.5091\n","Epoch 71/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2818 - acc: 0.5385 - val_loss: 1.4036 - val_acc: 0.5060\n","Epoch 72/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2818 - acc: 0.5378 - val_loss: 1.3898 - val_acc: 0.5077\n","Epoch 73/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2743 - acc: 0.5388 - val_loss: 1.3845 - val_acc: 0.5119\n","Epoch 74/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2762 - acc: 0.5386 - val_loss: 1.4170 - val_acc: 0.4996\n","Epoch 75/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2788 - acc: 0.5395 - val_loss: 1.4121 - val_acc: 0.5050\n","Epoch 76/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2638 - acc: 0.5440 - val_loss: 1.3697 - val_acc: 0.5191\n","Epoch 77/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2645 - acc: 0.5414 - val_loss: 1.3785 - val_acc: 0.5121\n","Epoch 78/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2591 - acc: 0.5447 - val_loss: 1.3840 - val_acc: 0.5065\n","Epoch 79/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2601 - acc: 0.5479 - val_loss: 1.3909 - val_acc: 0.5077\n","Epoch 80/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2627 - acc: 0.5467 - val_loss: 1.4045 - val_acc: 0.5076\n","Epoch 81/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2537 - acc: 0.5476 - val_loss: 1.3927 - val_acc: 0.5128\n","Epoch 82/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2600 - acc: 0.5431 - val_loss: 1.3654 - val_acc: 0.5148\n","Epoch 83/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2599 - acc: 0.5452 - val_loss: 1.3859 - val_acc: 0.5161\n","Epoch 84/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2542 - acc: 0.5471 - val_loss: 1.3844 - val_acc: 0.5095\n","Epoch 85/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2607 - acc: 0.5445 - val_loss: 1.3863 - val_acc: 0.5195\n","Epoch 86/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2547 - acc: 0.5492 - val_loss: 1.3787 - val_acc: 0.5156\n","Epoch 87/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2473 - acc: 0.5504 - val_loss: 1.4050 - val_acc: 0.5035\n","Epoch 88/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2393 - acc: 0.5512 - val_loss: 1.3805 - val_acc: 0.5165\n","Epoch 89/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2446 - acc: 0.5509 - val_loss: 1.3919 - val_acc: 0.5121\n","Epoch 90/100\n","50000/50000 [==============================] - 5s 99us/step - loss: 1.2471 - acc: 0.5514 - val_loss: 1.3817 - val_acc: 0.5126\n","Epoch 91/100\n","50000/50000 [==============================] - 5s 99us/step - loss: 1.2362 - acc: 0.5547 - val_loss: 1.3923 - val_acc: 0.5114\n","Epoch 92/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2389 - acc: 0.5527 - val_loss: 1.3920 - val_acc: 0.5055\n","Epoch 93/100\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2338 - acc: 0.5540 - val_loss: 1.3966 - val_acc: 0.5043\n","Epoch 94/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2473 - acc: 0.5507 - val_loss: 1.4062 - val_acc: 0.5084\n","Epoch 95/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2374 - acc: 0.5539 - val_loss: 1.3897 - val_acc: 0.5107\n","Epoch 96/100\n","50000/50000 [==============================] - 5s 99us/step - loss: 1.2386 - acc: 0.5536 - val_loss: 1.3902 - val_acc: 0.5080\n","Epoch 97/100\n","50000/50000 [==============================] - 5s 99us/step - loss: 1.2304 - acc: 0.5565 - val_loss: 1.4233 - val_acc: 0.5046\n","Epoch 98/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2275 - acc: 0.5578 - val_loss: 1.4082 - val_acc: 0.5038\n","Epoch 99/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2319 - acc: 0.5570 - val_loss: 1.4021 - val_acc: 0.5062\n","Epoch 100/100\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2284 - acc: 0.5580 - val_loss: 1.4027 - val_acc: 0.5113\n","Test loss: 1.402669591331482\n","Test acc: 0.5113\n"],"name":"stdout"}]},{"metadata":{"id":"_U1eErx6RMVf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":213},"outputId":"2833662c-605e-4872-a7f0-948e138cb546","executionInfo":{"status":"ok","timestamp":1539096658004,"user_tz":240,"elapsed":558,"user":{"displayName":"Manasa Singhekar","photoUrl":"","userId":"13090086925392106704"}}},"cell_type":"code","source":["plt.subplot(212)  \n","plt.plot(history.history['loss'])  \n","plt.plot(history.history['val_loss'])  \n","plt.title('model loss')  \n","plt.ylabel('loss')  \n","plt.xlabel('epoch')  \n","plt.legend(['train', 'test'], loc='upper left')  \n","plt.show()"],"execution_count":24,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAe8AAADECAYAAAC2lamMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n","text/plain":["<matplotlib.figure.Figure at 0x7f459c6d7e48>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"XbNwRYl1RO2U","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"tzEi3L5MvQgK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"c2eaac4b-c78b-44f5-cf9a-efe18e34d5f4","executionInfo":{"status":"ok","timestamp":1539096695537,"user_tz":240,"elapsed":4700,"user":{"displayName":"Manasa Singhekar","photoUrl":"","userId":"13090086925392106704"}}},"cell_type":"code","source":["loss1, acc1 = model.evaluate(X_train, Y_train, verbose=0)\n","print('Train loss:', loss1)\n","print('Train acc:', acc1)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Train loss: 0.9948181167602539\n","Train acc: 0.66084\n"],"name":"stdout"}]},{"metadata":{"id":"ecXUpuxeQvgn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":230},"outputId":"98dc5858-1ee6-4c14-8992-71c93253459d","executionInfo":{"status":"ok","timestamp":1539096953947,"user_tz":240,"elapsed":568,"user":{"displayName":"Manasa Singhekar","photoUrl":"","userId":"13090086925392106704"}}},"cell_type":"code","source":["\n","import matplotlib.pyplot as plt \n","plt.subplot(211)  \n","plt.plot(history.history['acc'])  \n","plt.plot(history.history['val_acc'])  \n","plt.title('model accuracy')  \n","plt.ylabel('accuracy')  \n","plt.xlabel('epoch')  \n","plt.legend(['train', 'test'], loc='upper left')"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7f459c4874a8>"]},"metadata":{"tags":[]},"execution_count":29},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAe8AAADECAYAAAC2lamMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<matplotlib.figure.Figure at 0x7f459c5065c0>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"OIyeKJc0N6yZ","colab_type":"code","colab":{}},"cell_type":"code","source":["# Modify the following parameters and discuss the effect of changing parameters on loss and\n","#accuracy\n","#1. No of epochs--200"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zBo-_DCDPRpF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":7395},"outputId":"47a8722a-7fe1-407c-e535-8a226610fed7","executionInfo":{"status":"ok","timestamp":1539097955716,"user_tz":240,"elapsed":980032,"user":{"displayName":"Manasa Singhekar","photoUrl":"","userId":"13090086925392106704"}}},"cell_type":"code","source":["if __name__ == '__main__':\n","    nb_epoch = 200\n","    batch_size = 128\n","    nb_classes = 10\n","\n","    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n","\n","    X_train = X_train.reshape(50000, 32 * 32 * 3)\n","    X_test = X_test.reshape(10000, 32 * 32 * 3)\n","\n","    X_train = X_train.astype('float32')\n","    X_test = X_test.astype('float32')\n","    X_train /= 255.0\n","    X_test /= 255.0\n","\n","    Y_train = np_utils.to_categorical(y_train, nb_classes)\n","    Y_test = np_utils.to_categorical(y_test, nb_classes)\n","\n","    # MLP\n","    model = Sequential()\n","    model.add(Dense(1024, input_shape=(3072, )))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(512))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(512))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(10))\n","    model.add(Activation('softmax'))\n","\n","    model.compile(loss='categorical_crossentropy',\n","                  optimizer='adam',\n","                  metrics=['accuracy'])\n","    model.summary()\n","\n","    # training\n","    history = model.fit(X_train, Y_train,\n","                        batch_size=batch_size,\n","                        nb_epoch=nb_epoch,\n","                        verbose=1,\n","                        validation_data=(X_test, Y_test))\n","\n","    save_history(history, 'history.txt')\n","\n","    loss, acc = model.evaluate(X_test, Y_test, verbose=0)\n","    print('Test loss:', loss)\n","    print('Test acc:', acc)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_17 (Dense)             (None, 1024)              3146752   \n","_________________________________________________________________\n","activation_17 (Activation)   (None, 1024)              0         \n","_________________________________________________________________\n","dropout_13 (Dropout)         (None, 1024)              0         \n","_________________________________________________________________\n","dense_18 (Dense)             (None, 512)               524800    \n","_________________________________________________________________\n","activation_18 (Activation)   (None, 512)               0         \n","_________________________________________________________________\n","dropout_14 (Dropout)         (None, 512)               0         \n","_________________________________________________________________\n","dense_19 (Dense)             (None, 512)               262656    \n","_________________________________________________________________\n","activation_19 (Activation)   (None, 512)               0         \n","_________________________________________________________________\n","dropout_15 (Dropout)         (None, 512)               0         \n","_________________________________________________________________\n","dense_20 (Dense)             (None, 10)                5130      \n","_________________________________________________________________\n","activation_20 (Activation)   (None, 10)                0         \n","=================================================================\n","Total params: 3,939,338\n","Trainable params: 3,939,338\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n","  warnings.warn('The `nb_epoch` argument in `fit` '\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 50000 samples, validate on 10000 samples\n","Epoch 1/200\n","50000/50000 [==============================] - 6s 115us/step - loss: 1.9779 - acc: 0.2756 - val_loss: 1.7709 - val_acc: 0.3640\n","Epoch 2/200\n","50000/50000 [==============================] - 5s 100us/step - loss: 1.8080 - acc: 0.3463 - val_loss: 1.6860 - val_acc: 0.3973\n","Epoch 3/200\n","50000/50000 [==============================] - 5s 99us/step - loss: 1.7443 - acc: 0.3707 - val_loss: 1.6654 - val_acc: 0.4064\n","Epoch 4/200\n","50000/50000 [==============================] - 5s 99us/step - loss: 1.7169 - acc: 0.3819 - val_loss: 1.6399 - val_acc: 0.4202\n","Epoch 5/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.6790 - acc: 0.3968 - val_loss: 1.5903 - val_acc: 0.4302\n","Epoch 6/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.6555 - acc: 0.4056 - val_loss: 1.6126 - val_acc: 0.4249\n","Epoch 7/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.6337 - acc: 0.4101 - val_loss: 1.5560 - val_acc: 0.4499\n","Epoch 8/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.6070 - acc: 0.4205 - val_loss: 1.5339 - val_acc: 0.4592\n","Epoch 9/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.5968 - acc: 0.4258 - val_loss: 1.5103 - val_acc: 0.4613\n","Epoch 10/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.5832 - acc: 0.4298 - val_loss: 1.5278 - val_acc: 0.4577\n","Epoch 11/200\n","50000/50000 [==============================] - 5s 99us/step - loss: 1.5697 - acc: 0.4351 - val_loss: 1.5251 - val_acc: 0.4569\n","Epoch 12/200\n","50000/50000 [==============================] - 5s 99us/step - loss: 1.5624 - acc: 0.4388 - val_loss: 1.5062 - val_acc: 0.4690\n","Epoch 13/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.5525 - acc: 0.4394 - val_loss: 1.4822 - val_acc: 0.4697\n","Epoch 14/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.5342 - acc: 0.4476 - val_loss: 1.4672 - val_acc: 0.4755\n","Epoch 15/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.5315 - acc: 0.4480 - val_loss: 1.4904 - val_acc: 0.4721\n","Epoch 16/200\n","50000/50000 [==============================] - 5s 99us/step - loss: 1.5291 - acc: 0.4483 - val_loss: 1.5117 - val_acc: 0.4655\n","Epoch 17/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.5149 - acc: 0.4552 - val_loss: 1.4911 - val_acc: 0.4682\n","Epoch 18/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.5101 - acc: 0.4563 - val_loss: 1.4510 - val_acc: 0.4824\n","Epoch 19/200\n","50000/50000 [==============================] - 5s 99us/step - loss: 1.5085 - acc: 0.4542 - val_loss: 1.4723 - val_acc: 0.4802\n","Epoch 20/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.4963 - acc: 0.4618 - val_loss: 1.4426 - val_acc: 0.4935\n","Epoch 21/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.4881 - acc: 0.4632 - val_loss: 1.4360 - val_acc: 0.4964\n","Epoch 22/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.4819 - acc: 0.4660 - val_loss: 1.4720 - val_acc: 0.4753\n","Epoch 23/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.4822 - acc: 0.4653 - val_loss: 1.4359 - val_acc: 0.4874\n","Epoch 24/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.4696 - acc: 0.4734 - val_loss: 1.4853 - val_acc: 0.4765\n","Epoch 25/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.4719 - acc: 0.4712 - val_loss: 1.4523 - val_acc: 0.4863\n","Epoch 26/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.4613 - acc: 0.4734 - val_loss: 1.4408 - val_acc: 0.4884\n","Epoch 27/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.4559 - acc: 0.4770 - val_loss: 1.4332 - val_acc: 0.4919\n","Epoch 28/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.4496 - acc: 0.4778 - val_loss: 1.4471 - val_acc: 0.4874\n","Epoch 29/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.4514 - acc: 0.4776 - val_loss: 1.4313 - val_acc: 0.4959\n","Epoch 30/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.4378 - acc: 0.4823 - val_loss: 1.4286 - val_acc: 0.4922\n","Epoch 31/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.4346 - acc: 0.4825 - val_loss: 1.4631 - val_acc: 0.4811\n","Epoch 32/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.4341 - acc: 0.4832 - val_loss: 1.4138 - val_acc: 0.4971\n","Epoch 33/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.4281 - acc: 0.4856 - val_loss: 1.4375 - val_acc: 0.4923\n","Epoch 34/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.4252 - acc: 0.4866 - val_loss: 1.4100 - val_acc: 0.5008\n","Epoch 35/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.4207 - acc: 0.4893 - val_loss: 1.4389 - val_acc: 0.4895\n","Epoch 36/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.4221 - acc: 0.4888 - val_loss: 1.4120 - val_acc: 0.5013\n","Epoch 37/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.4196 - acc: 0.4894 - val_loss: 1.4196 - val_acc: 0.4935\n","Epoch 38/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.4102 - acc: 0.4943 - val_loss: 1.4332 - val_acc: 0.4881\n","Epoch 39/200\n","50000/50000 [==============================] - 5s 99us/step - loss: 1.4129 - acc: 0.4907 - val_loss: 1.4240 - val_acc: 0.4988\n","Epoch 40/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.4015 - acc: 0.4931 - val_loss: 1.4374 - val_acc: 0.4870\n","Epoch 41/200\n","50000/50000 [==============================] - 5s 99us/step - loss: 1.3990 - acc: 0.4961 - val_loss: 1.4100 - val_acc: 0.5021\n","Epoch 42/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3988 - acc: 0.4950 - val_loss: 1.4029 - val_acc: 0.5062\n","Epoch 43/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3957 - acc: 0.4966 - val_loss: 1.4056 - val_acc: 0.5059\n","Epoch 44/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3933 - acc: 0.4971 - val_loss: 1.4128 - val_acc: 0.5020\n","Epoch 45/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3838 - acc: 0.5008 - val_loss: 1.4116 - val_acc: 0.5045\n","Epoch 46/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3879 - acc: 0.4983 - val_loss: 1.4575 - val_acc: 0.4812\n","Epoch 47/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3840 - acc: 0.5018 - val_loss: 1.3989 - val_acc: 0.5076\n","Epoch 48/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3776 - acc: 0.5037 - val_loss: 1.4070 - val_acc: 0.5041\n","Epoch 49/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3774 - acc: 0.5029 - val_loss: 1.4333 - val_acc: 0.4979\n","Epoch 50/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3818 - acc: 0.5012 - val_loss: 1.3872 - val_acc: 0.5140\n","Epoch 51/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3698 - acc: 0.5051 - val_loss: 1.4228 - val_acc: 0.5015\n","Epoch 52/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3647 - acc: 0.5057 - val_loss: 1.4239 - val_acc: 0.4965\n","Epoch 53/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3649 - acc: 0.5095 - val_loss: 1.4091 - val_acc: 0.5083\n","Epoch 54/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3623 - acc: 0.5074 - val_loss: 1.3927 - val_acc: 0.5089\n","Epoch 55/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3571 - acc: 0.5107 - val_loss: 1.4202 - val_acc: 0.4994\n","Epoch 56/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3645 - acc: 0.5081 - val_loss: 1.4165 - val_acc: 0.4982\n","Epoch 57/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3593 - acc: 0.5076 - val_loss: 1.3877 - val_acc: 0.5115\n","Epoch 58/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3518 - acc: 0.5096 - val_loss: 1.3925 - val_acc: 0.5110\n","Epoch 59/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3491 - acc: 0.5132 - val_loss: 1.3928 - val_acc: 0.5063\n","Epoch 60/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3512 - acc: 0.5137 - val_loss: 1.4021 - val_acc: 0.5030\n","Epoch 61/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3509 - acc: 0.5124 - val_loss: 1.4080 - val_acc: 0.5014\n","Epoch 62/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3486 - acc: 0.5123 - val_loss: 1.3896 - val_acc: 0.5082\n","Epoch 63/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3416 - acc: 0.5174 - val_loss: 1.3969 - val_acc: 0.5001\n","Epoch 64/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3433 - acc: 0.5153 - val_loss: 1.4045 - val_acc: 0.5032\n","Epoch 65/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3291 - acc: 0.5219 - val_loss: 1.3931 - val_acc: 0.5005\n","Epoch 66/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3294 - acc: 0.5194 - val_loss: 1.3977 - val_acc: 0.5058\n","Epoch 67/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3310 - acc: 0.5185 - val_loss: 1.3728 - val_acc: 0.5177\n","Epoch 68/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3430 - acc: 0.5155 - val_loss: 1.4143 - val_acc: 0.5002\n","Epoch 69/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3295 - acc: 0.5228 - val_loss: 1.4148 - val_acc: 0.5016\n","Epoch 70/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3304 - acc: 0.5196 - val_loss: 1.4006 - val_acc: 0.5043\n","Epoch 71/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3215 - acc: 0.5238 - val_loss: 1.3972 - val_acc: 0.5078\n","Epoch 72/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3236 - acc: 0.5203 - val_loss: 1.3953 - val_acc: 0.5073\n","Epoch 73/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3241 - acc: 0.5214 - val_loss: 1.3987 - val_acc: 0.5024\n","Epoch 74/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3236 - acc: 0.5234 - val_loss: 1.4074 - val_acc: 0.5031\n","Epoch 75/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3104 - acc: 0.5262 - val_loss: 1.3816 - val_acc: 0.5126\n","Epoch 76/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3113 - acc: 0.5254 - val_loss: 1.3967 - val_acc: 0.5072\n","Epoch 77/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3069 - acc: 0.5275 - val_loss: 1.3925 - val_acc: 0.5073\n","Epoch 78/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3145 - acc: 0.5282 - val_loss: 1.4010 - val_acc: 0.5039\n","Epoch 79/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3079 - acc: 0.5290 - val_loss: 1.3847 - val_acc: 0.5111\n","Epoch 80/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3086 - acc: 0.5268 - val_loss: 1.3797 - val_acc: 0.5124\n","Epoch 81/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3066 - acc: 0.5297 - val_loss: 1.4179 - val_acc: 0.5000\n","Epoch 82/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3095 - acc: 0.5306 - val_loss: 1.3885 - val_acc: 0.5107\n","Epoch 83/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3017 - acc: 0.5315 - val_loss: 1.3899 - val_acc: 0.5064\n","Epoch 84/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3033 - acc: 0.5320 - val_loss: 1.3784 - val_acc: 0.5157\n","Epoch 85/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3045 - acc: 0.5298 - val_loss: 1.3813 - val_acc: 0.5100\n","Epoch 86/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2991 - acc: 0.5289 - val_loss: 1.4149 - val_acc: 0.5004\n","Epoch 87/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.3024 - acc: 0.5291 - val_loss: 1.3827 - val_acc: 0.5165\n","Epoch 88/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.3003 - acc: 0.5297 - val_loss: 1.3951 - val_acc: 0.5068\n","Epoch 89/200\n","50000/50000 [==============================] - 5s 99us/step - loss: 1.2890 - acc: 0.5353 - val_loss: 1.3808 - val_acc: 0.5137\n","Epoch 90/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2903 - acc: 0.5330 - val_loss: 1.3874 - val_acc: 0.5108\n","Epoch 91/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2908 - acc: 0.5324 - val_loss: 1.3896 - val_acc: 0.5079\n","Epoch 92/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2889 - acc: 0.5369 - val_loss: 1.3901 - val_acc: 0.5058\n","Epoch 93/200\n","50000/50000 [==============================] - 5s 99us/step - loss: 1.2780 - acc: 0.5377 - val_loss: 1.3810 - val_acc: 0.5145\n","Epoch 94/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2854 - acc: 0.5353 - val_loss: 1.3871 - val_acc: 0.5111\n","Epoch 95/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2843 - acc: 0.5377 - val_loss: 1.3842 - val_acc: 0.5095\n","Epoch 96/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2754 - acc: 0.5394 - val_loss: 1.4007 - val_acc: 0.5041\n","Epoch 97/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2806 - acc: 0.5392 - val_loss: 1.3948 - val_acc: 0.5104\n","Epoch 98/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2810 - acc: 0.5387 - val_loss: 1.3962 - val_acc: 0.5067\n","Epoch 99/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2693 - acc: 0.5412 - val_loss: 1.3957 - val_acc: 0.5075\n","Epoch 100/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2762 - acc: 0.5385 - val_loss: 1.4033 - val_acc: 0.5086\n","Epoch 101/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2714 - acc: 0.5411 - val_loss: 1.3823 - val_acc: 0.5132\n","Epoch 102/200\n","50000/50000 [==============================] - 5s 99us/step - loss: 1.2807 - acc: 0.5399 - val_loss: 1.3979 - val_acc: 0.5073\n","Epoch 103/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2801 - acc: 0.5376 - val_loss: 1.3919 - val_acc: 0.5106\n","Epoch 104/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2698 - acc: 0.5414 - val_loss: 1.3943 - val_acc: 0.5091\n","Epoch 105/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2688 - acc: 0.5402 - val_loss: 1.3789 - val_acc: 0.5184\n","Epoch 106/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2685 - acc: 0.5413 - val_loss: 1.3892 - val_acc: 0.5096\n","Epoch 107/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2637 - acc: 0.5445 - val_loss: 1.3759 - val_acc: 0.5210\n","Epoch 108/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2691 - acc: 0.5411 - val_loss: 1.3828 - val_acc: 0.5109\n","Epoch 109/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2633 - acc: 0.5458 - val_loss: 1.3961 - val_acc: 0.5150\n","Epoch 110/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2657 - acc: 0.5435 - val_loss: 1.4039 - val_acc: 0.5029\n","Epoch 111/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2607 - acc: 0.5463 - val_loss: 1.4151 - val_acc: 0.5030\n","Epoch 112/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2651 - acc: 0.5445 - val_loss: 1.4023 - val_acc: 0.5083\n","Epoch 113/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2554 - acc: 0.5472 - val_loss: 1.3846 - val_acc: 0.5146\n","Epoch 114/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2536 - acc: 0.5463 - val_loss: 1.3878 - val_acc: 0.5126\n","Epoch 115/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2552 - acc: 0.5486 - val_loss: 1.4453 - val_acc: 0.4943\n","Epoch 116/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2527 - acc: 0.5481 - val_loss: 1.3844 - val_acc: 0.5125\n","Epoch 117/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2528 - acc: 0.5485 - val_loss: 1.3843 - val_acc: 0.5112\n","Epoch 118/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2490 - acc: 0.5526 - val_loss: 1.4126 - val_acc: 0.5054\n","Epoch 119/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2576 - acc: 0.5469 - val_loss: 1.3968 - val_acc: 0.5155\n","Epoch 120/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2498 - acc: 0.5488 - val_loss: 1.4000 - val_acc: 0.5052\n","Epoch 121/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2524 - acc: 0.5499 - val_loss: 1.3850 - val_acc: 0.5101\n","Epoch 122/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2471 - acc: 0.5508 - val_loss: 1.3908 - val_acc: 0.5121\n","Epoch 123/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2484 - acc: 0.5489 - val_loss: 1.4009 - val_acc: 0.5123\n","Epoch 124/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2530 - acc: 0.5484 - val_loss: 1.3984 - val_acc: 0.5100\n","Epoch 125/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2436 - acc: 0.5521 - val_loss: 1.3968 - val_acc: 0.5146\n","Epoch 126/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2460 - acc: 0.5494 - val_loss: 1.4030 - val_acc: 0.5079\n","Epoch 127/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2400 - acc: 0.5501 - val_loss: 1.3923 - val_acc: 0.5172\n","Epoch 128/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2501 - acc: 0.5508 - val_loss: 1.3962 - val_acc: 0.5109\n","Epoch 129/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2394 - acc: 0.5535 - val_loss: 1.3938 - val_acc: 0.5144\n","Epoch 130/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2376 - acc: 0.5530 - val_loss: 1.3948 - val_acc: 0.5069\n","Epoch 131/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2420 - acc: 0.5541 - val_loss: 1.4013 - val_acc: 0.5090\n","Epoch 132/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2367 - acc: 0.5547 - val_loss: 1.4200 - val_acc: 0.5036\n","Epoch 133/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2350 - acc: 0.5550 - val_loss: 1.4022 - val_acc: 0.5104\n","Epoch 134/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2419 - acc: 0.5518 - val_loss: 1.3897 - val_acc: 0.5150\n","Epoch 135/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2278 - acc: 0.5562 - val_loss: 1.3884 - val_acc: 0.5111\n","Epoch 136/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2452 - acc: 0.5514 - val_loss: 1.3911 - val_acc: 0.5128\n","Epoch 137/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2334 - acc: 0.5547 - val_loss: 1.4110 - val_acc: 0.5074\n","Epoch 138/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2369 - acc: 0.5547 - val_loss: 1.3997 - val_acc: 0.5143\n","Epoch 139/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2205 - acc: 0.5584 - val_loss: 1.3878 - val_acc: 0.5137\n","Epoch 140/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2273 - acc: 0.5608 - val_loss: 1.4166 - val_acc: 0.5055\n","Epoch 141/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2207 - acc: 0.5572 - val_loss: 1.3928 - val_acc: 0.5139\n","Epoch 142/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2252 - acc: 0.5577 - val_loss: 1.3946 - val_acc: 0.5156\n","Epoch 143/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2330 - acc: 0.5544 - val_loss: 1.4119 - val_acc: 0.5042\n","Epoch 144/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2274 - acc: 0.5584 - val_loss: 1.3956 - val_acc: 0.5116\n","Epoch 145/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2129 - acc: 0.5607 - val_loss: 1.3895 - val_acc: 0.5174\n","Epoch 146/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2170 - acc: 0.5617 - val_loss: 1.3785 - val_acc: 0.5169\n","Epoch 147/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2227 - acc: 0.5603 - val_loss: 1.3958 - val_acc: 0.5136\n","Epoch 148/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2098 - acc: 0.5660 - val_loss: 1.3975 - val_acc: 0.5121\n","Epoch 149/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2207 - acc: 0.5594 - val_loss: 1.4031 - val_acc: 0.5041\n","Epoch 150/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2211 - acc: 0.5604 - val_loss: 1.4030 - val_acc: 0.5121\n","Epoch 151/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2100 - acc: 0.5623 - val_loss: 1.4007 - val_acc: 0.5099\n","Epoch 152/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2234 - acc: 0.5562 - val_loss: 1.3848 - val_acc: 0.5142\n","Epoch 153/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2138 - acc: 0.5619 - val_loss: 1.3887 - val_acc: 0.5153\n","Epoch 154/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2175 - acc: 0.5595 - val_loss: 1.3872 - val_acc: 0.5157\n","Epoch 155/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2158 - acc: 0.5612 - val_loss: 1.3999 - val_acc: 0.5094\n","Epoch 156/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2128 - acc: 0.5628 - val_loss: 1.3881 - val_acc: 0.5182\n","Epoch 157/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2197 - acc: 0.5602 - val_loss: 1.4074 - val_acc: 0.5086\n","Epoch 158/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2137 - acc: 0.5625 - val_loss: 1.3952 - val_acc: 0.5157\n","Epoch 159/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2136 - acc: 0.5623 - val_loss: 1.4018 - val_acc: 0.5089\n","Epoch 160/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1999 - acc: 0.5668 - val_loss: 1.4000 - val_acc: 0.5087\n","Epoch 161/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2047 - acc: 0.5670 - val_loss: 1.3965 - val_acc: 0.5102\n","Epoch 162/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2054 - acc: 0.5638 - val_loss: 1.4027 - val_acc: 0.5097\n","Epoch 163/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2100 - acc: 0.5652 - val_loss: 1.4076 - val_acc: 0.5087\n","Epoch 164/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2033 - acc: 0.5680 - val_loss: 1.3963 - val_acc: 0.5133\n","Epoch 165/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2114 - acc: 0.5627 - val_loss: 1.3977 - val_acc: 0.5125\n","Epoch 166/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2019 - acc: 0.5671 - val_loss: 1.4123 - val_acc: 0.5055\n","Epoch 167/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.1987 - acc: 0.5675 - val_loss: 1.4117 - val_acc: 0.5100\n","Epoch 168/200\n","50000/50000 [==============================] - 5s 99us/step - loss: 1.2041 - acc: 0.5673 - val_loss: 1.3981 - val_acc: 0.5120\n","Epoch 169/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1967 - acc: 0.5689 - val_loss: 1.4241 - val_acc: 0.5053\n","Epoch 170/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2091 - acc: 0.5664 - val_loss: 1.3822 - val_acc: 0.5183\n","Epoch 171/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2093 - acc: 0.5649 - val_loss: 1.4015 - val_acc: 0.5056\n","Epoch 172/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2037 - acc: 0.5649 - val_loss: 1.3891 - val_acc: 0.5165\n","Epoch 173/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1966 - acc: 0.5692 - val_loss: 1.3988 - val_acc: 0.5098\n","Epoch 174/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1970 - acc: 0.5696 - val_loss: 1.4097 - val_acc: 0.5098\n","Epoch 175/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.2002 - acc: 0.5687 - val_loss: 1.4087 - val_acc: 0.5106\n","Epoch 176/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1993 - acc: 0.5646 - val_loss: 1.4080 - val_acc: 0.5124\n","Epoch 177/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.2033 - acc: 0.5662 - val_loss: 1.4035 - val_acc: 0.5092\n","Epoch 178/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1958 - acc: 0.5712 - val_loss: 1.4272 - val_acc: 0.5003\n","Epoch 179/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1984 - acc: 0.5686 - val_loss: 1.3900 - val_acc: 0.5138\n","Epoch 180/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.1857 - acc: 0.5725 - val_loss: 1.4155 - val_acc: 0.5069\n","Epoch 181/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1851 - acc: 0.5722 - val_loss: 1.4117 - val_acc: 0.5064\n","Epoch 182/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1990 - acc: 0.5683 - val_loss: 1.4058 - val_acc: 0.5086\n","Epoch 183/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1804 - acc: 0.5765 - val_loss: 1.3960 - val_acc: 0.5118\n","Epoch 184/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1963 - acc: 0.5700 - val_loss: 1.4072 - val_acc: 0.5094\n","Epoch 185/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1950 - acc: 0.5711 - val_loss: 1.4581 - val_acc: 0.4897\n","Epoch 186/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1874 - acc: 0.5731 - val_loss: 1.3886 - val_acc: 0.5150\n","Epoch 187/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1887 - acc: 0.5728 - val_loss: 1.4220 - val_acc: 0.5052\n","Epoch 188/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1819 - acc: 0.5737 - val_loss: 1.3914 - val_acc: 0.5161\n","Epoch 189/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1930 - acc: 0.5702 - val_loss: 1.3990 - val_acc: 0.5096\n","Epoch 190/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1828 - acc: 0.5772 - val_loss: 1.3944 - val_acc: 0.5115\n","Epoch 191/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1725 - acc: 0.5783 - val_loss: 1.3955 - val_acc: 0.5113\n","Epoch 192/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1837 - acc: 0.5727 - val_loss: 1.3914 - val_acc: 0.5150\n","Epoch 193/200\n","50000/50000 [==============================] - 5s 98us/step - loss: 1.1796 - acc: 0.5740 - val_loss: 1.3932 - val_acc: 0.5113\n","Epoch 194/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1894 - acc: 0.5727 - val_loss: 1.3950 - val_acc: 0.5175\n","Epoch 195/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1782 - acc: 0.5762 - val_loss: 1.4120 - val_acc: 0.5101\n","Epoch 196/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1813 - acc: 0.5737 - val_loss: 1.4070 - val_acc: 0.5141\n","Epoch 197/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1776 - acc: 0.5779 - val_loss: 1.3980 - val_acc: 0.5121\n","Epoch 198/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1833 - acc: 0.5735 - val_loss: 1.4180 - val_acc: 0.5054\n","Epoch 199/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1750 - acc: 0.5747 - val_loss: 1.4063 - val_acc: 0.5105\n","Epoch 200/200\n","50000/50000 [==============================] - 5s 97us/step - loss: 1.1782 - acc: 0.5779 - val_loss: 1.4170 - val_acc: 0.5003\n","Test loss: 1.4170141353607177\n","Test acc: 0.5003\n"],"name":"stdout"}]},{"metadata":{"id":"REvWVPqGvBEG","colab_type":"code","colab":{}},"cell_type":"code","source":["#2. Batch size"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WVSSsFe9u-cM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":3995},"outputId":"ede52cf4-187b-468c-d93b-535dc3565ff2","executionInfo":{"status":"ok","timestamp":1539098803053,"user_tz":240,"elapsed":283511,"user":{"displayName":"Manasa Singhekar","photoUrl":"","userId":"13090086925392106704"}}},"cell_type":"code","source":["if __name__ == '__main__':\n","    nb_epoch = 100\n","    batch_size = 256\n","    nb_classes = 10\n","\n","    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n","\n","    X_train = X_train.reshape(50000, 32 * 32 * 3)\n","    X_test = X_test.reshape(10000, 32 * 32 * 3)\n","\n","    X_train = X_train.astype('float32')\n","    X_test = X_test.astype('float32')\n","    X_train /= 255.0\n","    X_test /= 255.0\n","\n","    Y_train = np_utils.to_categorical(y_train, nb_classes)\n","    Y_test = np_utils.to_categorical(y_test, nb_classes)\n","\n","    # MLP\n","    model = Sequential()\n","    model.add(Dense(1024, input_shape=(3072, )))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(512))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(512))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(10))\n","    model.add(Activation('softmax'))\n","\n","    model.compile(loss='categorical_crossentropy',\n","                  optimizer='adam',\n","                  metrics=['accuracy'])\n","    model.summary()\n","\n","    # training\n","    history = model.fit(X_train, Y_train,\n","                        batch_size=batch_size,\n","                        nb_epoch=nb_epoch,\n","                        verbose=1,\n","                        validation_data=(X_test, Y_test))\n","\n","    save_history(history, 'history.txt')\n","\n","    loss, acc = model.evaluate(X_test, Y_test, verbose=0)\n","    print('Test loss:', loss)\n","    print('Test acc:', acc)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_21 (Dense)             (None, 1024)              3146752   \n","_________________________________________________________________\n","activation_21 (Activation)   (None, 1024)              0         \n","_________________________________________________________________\n","dropout_16 (Dropout)         (None, 1024)              0         \n","_________________________________________________________________\n","dense_22 (Dense)             (None, 512)               524800    \n","_________________________________________________________________\n","activation_22 (Activation)   (None, 512)               0         \n","_________________________________________________________________\n","dropout_17 (Dropout)         (None, 512)               0         \n","_________________________________________________________________\n","dense_23 (Dense)             (None, 512)               262656    \n","_________________________________________________________________\n","activation_23 (Activation)   (None, 512)               0         \n","_________________________________________________________________\n","dropout_18 (Dropout)         (None, 512)               0         \n","_________________________________________________________________\n","dense_24 (Dense)             (None, 10)                5130      \n","_________________________________________________________________\n","activation_24 (Activation)   (None, 10)                0         \n","=================================================================\n","Total params: 3,939,338\n","Trainable params: 3,939,338\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n","  warnings.warn('The `nb_epoch` argument in `fit` '\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 50000 samples, validate on 10000 samples\n","Epoch 1/100\n","50000/50000 [==============================] - 4s 72us/step - loss: 1.9841 - acc: 0.2738 - val_loss: 1.7919 - val_acc: 0.3492\n","Epoch 2/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.7915 - acc: 0.3547 - val_loss: 1.6957 - val_acc: 0.3919\n","Epoch 3/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.7300 - acc: 0.3759 - val_loss: 1.6574 - val_acc: 0.4005\n","Epoch 4/100\n","50000/50000 [==============================] - 3s 57us/step - loss: 1.6909 - acc: 0.3901 - val_loss: 1.6272 - val_acc: 0.4149\n","Epoch 5/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.6486 - acc: 0.4085 - val_loss: 1.5489 - val_acc: 0.4563\n","Epoch 6/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.6193 - acc: 0.4168 - val_loss: 1.5461 - val_acc: 0.4505\n","Epoch 7/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.6026 - acc: 0.4227 - val_loss: 1.5425 - val_acc: 0.4503\n","Epoch 8/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.5742 - acc: 0.4338 - val_loss: 1.5079 - val_acc: 0.4634\n","Epoch 9/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.5609 - acc: 0.4373 - val_loss: 1.4910 - val_acc: 0.4656\n","Epoch 10/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.5301 - acc: 0.4490 - val_loss: 1.4996 - val_acc: 0.4667\n","Epoch 11/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.5346 - acc: 0.4478 - val_loss: 1.4840 - val_acc: 0.4740\n","Epoch 12/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.5234 - acc: 0.4494 - val_loss: 1.4552 - val_acc: 0.4802\n","Epoch 13/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.5028 - acc: 0.4581 - val_loss: 1.4539 - val_acc: 0.4780\n","Epoch 14/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.4954 - acc: 0.4635 - val_loss: 1.4627 - val_acc: 0.4817\n","Epoch 15/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.4771 - acc: 0.4694 - val_loss: 1.4453 - val_acc: 0.4869\n","Epoch 16/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.4773 - acc: 0.4694 - val_loss: 1.4366 - val_acc: 0.4867\n","Epoch 17/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.4684 - acc: 0.4685 - val_loss: 1.4425 - val_acc: 0.4772\n","Epoch 18/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.4536 - acc: 0.4769 - val_loss: 1.4097 - val_acc: 0.5002\n","Epoch 19/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.4467 - acc: 0.4793 - val_loss: 1.4314 - val_acc: 0.4885\n","Epoch 20/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.4370 - acc: 0.4825 - val_loss: 1.4205 - val_acc: 0.4922\n","Epoch 21/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.4267 - acc: 0.4848 - val_loss: 1.4149 - val_acc: 0.4912\n","Epoch 22/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.4181 - acc: 0.4893 - val_loss: 1.4315 - val_acc: 0.4865\n","Epoch 23/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.4215 - acc: 0.4897 - val_loss: 1.4169 - val_acc: 0.4957\n","Epoch 24/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.4066 - acc: 0.4928 - val_loss: 1.3865 - val_acc: 0.5074\n","Epoch 25/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.3975 - acc: 0.4955 - val_loss: 1.3878 - val_acc: 0.5080\n","Epoch 26/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.3912 - acc: 0.4997 - val_loss: 1.3920 - val_acc: 0.5004\n","Epoch 27/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.3798 - acc: 0.5033 - val_loss: 1.3882 - val_acc: 0.5065\n","Epoch 28/100\n","50000/50000 [==============================] - 3s 55us/step - loss: 1.3760 - acc: 0.5010 - val_loss: 1.4174 - val_acc: 0.4932\n","Epoch 29/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.3784 - acc: 0.5035 - val_loss: 1.3625 - val_acc: 0.5170\n","Epoch 30/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.3688 - acc: 0.5051 - val_loss: 1.3777 - val_acc: 0.5098\n","Epoch 31/100\n","50000/50000 [==============================] - 3s 55us/step - loss: 1.3636 - acc: 0.5087 - val_loss: 1.3522 - val_acc: 0.5223\n","Epoch 32/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.3557 - acc: 0.5105 - val_loss: 1.3755 - val_acc: 0.5122\n","Epoch 33/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.3558 - acc: 0.5118 - val_loss: 1.3726 - val_acc: 0.5080\n","Epoch 34/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.3456 - acc: 0.5166 - val_loss: 1.3698 - val_acc: 0.5124\n","Epoch 35/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.3425 - acc: 0.5169 - val_loss: 1.3836 - val_acc: 0.5092\n","Epoch 36/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.3355 - acc: 0.5215 - val_loss: 1.3765 - val_acc: 0.5080\n","Epoch 37/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.3325 - acc: 0.5183 - val_loss: 1.3703 - val_acc: 0.5138\n","Epoch 38/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.3270 - acc: 0.5198 - val_loss: 1.3537 - val_acc: 0.5204\n","Epoch 39/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.3150 - acc: 0.5240 - val_loss: 1.3983 - val_acc: 0.5021\n","Epoch 40/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.3139 - acc: 0.5237 - val_loss: 1.3844 - val_acc: 0.5045\n","Epoch 41/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.3116 - acc: 0.5285 - val_loss: 1.3677 - val_acc: 0.5157\n","Epoch 42/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.3051 - acc: 0.5321 - val_loss: 1.3575 - val_acc: 0.5163\n","Epoch 43/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.3065 - acc: 0.5277 - val_loss: 1.3587 - val_acc: 0.5148\n","Epoch 44/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.3019 - acc: 0.5319 - val_loss: 1.3551 - val_acc: 0.5162\n","Epoch 45/100\n","50000/50000 [==============================] - 3s 55us/step - loss: 1.2980 - acc: 0.5324 - val_loss: 1.3386 - val_acc: 0.5233\n","Epoch 46/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2884 - acc: 0.5364 - val_loss: 1.3790 - val_acc: 0.5124\n","Epoch 47/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2866 - acc: 0.5389 - val_loss: 1.3533 - val_acc: 0.5174\n","Epoch 48/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2869 - acc: 0.5357 - val_loss: 1.3543 - val_acc: 0.5152\n","Epoch 49/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2813 - acc: 0.5367 - val_loss: 1.3502 - val_acc: 0.5217\n","Epoch 50/100\n","50000/50000 [==============================] - 3s 55us/step - loss: 1.2800 - acc: 0.5396 - val_loss: 1.3663 - val_acc: 0.5146\n","Epoch 51/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2890 - acc: 0.5364 - val_loss: 1.3490 - val_acc: 0.5222\n","Epoch 52/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2699 - acc: 0.5428 - val_loss: 1.3354 - val_acc: 0.5252\n","Epoch 53/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2569 - acc: 0.5448 - val_loss: 1.3582 - val_acc: 0.5170\n","Epoch 54/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2589 - acc: 0.5480 - val_loss: 1.3738 - val_acc: 0.5148\n","Epoch 55/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2604 - acc: 0.5451 - val_loss: 1.3499 - val_acc: 0.5243\n","Epoch 56/100\n","50000/50000 [==============================] - 3s 58us/step - loss: 1.2592 - acc: 0.5475 - val_loss: 1.3635 - val_acc: 0.5239\n","Epoch 57/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2456 - acc: 0.5514 - val_loss: 1.3432 - val_acc: 0.5216\n","Epoch 58/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2508 - acc: 0.5481 - val_loss: 1.3481 - val_acc: 0.5225\n","Epoch 59/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2460 - acc: 0.5525 - val_loss: 1.3587 - val_acc: 0.5173\n","Epoch 60/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2341 - acc: 0.5548 - val_loss: 1.3438 - val_acc: 0.5231\n","Epoch 61/100\n","50000/50000 [==============================] - 3s 55us/step - loss: 1.2438 - acc: 0.5530 - val_loss: 1.3344 - val_acc: 0.5265\n","Epoch 62/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2372 - acc: 0.5529 - val_loss: 1.3633 - val_acc: 0.5158\n","Epoch 63/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2347 - acc: 0.5539 - val_loss: 1.3460 - val_acc: 0.5248\n","Epoch 64/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2403 - acc: 0.5522 - val_loss: 1.3670 - val_acc: 0.5159\n","Epoch 65/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2329 - acc: 0.5554 - val_loss: 1.3836 - val_acc: 0.5136\n","Epoch 66/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2213 - acc: 0.5578 - val_loss: 1.3460 - val_acc: 0.5262\n","Epoch 67/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2123 - acc: 0.5626 - val_loss: 1.3275 - val_acc: 0.5293\n","Epoch 68/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2183 - acc: 0.5598 - val_loss: 1.3599 - val_acc: 0.5152\n","Epoch 69/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2178 - acc: 0.5593 - val_loss: 1.3480 - val_acc: 0.5234\n","Epoch 70/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2157 - acc: 0.5607 - val_loss: 1.3705 - val_acc: 0.5160\n","Epoch 71/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2050 - acc: 0.5609 - val_loss: 1.3436 - val_acc: 0.5234\n","Epoch 72/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2054 - acc: 0.5649 - val_loss: 1.3593 - val_acc: 0.5190\n","Epoch 73/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2072 - acc: 0.5637 - val_loss: 1.3477 - val_acc: 0.5226\n","Epoch 74/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.2029 - acc: 0.5671 - val_loss: 1.3447 - val_acc: 0.5245\n","Epoch 75/100\n","50000/50000 [==============================] - 3s 57us/step - loss: 1.1983 - acc: 0.5691 - val_loss: 1.3574 - val_acc: 0.5214\n","Epoch 76/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.1992 - acc: 0.5668 - val_loss: 1.3507 - val_acc: 0.5186\n","Epoch 77/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.1874 - acc: 0.5690 - val_loss: 1.3541 - val_acc: 0.5242\n","Epoch 78/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.1943 - acc: 0.5704 - val_loss: 1.3486 - val_acc: 0.5253\n","Epoch 79/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.1921 - acc: 0.5679 - val_loss: 1.3864 - val_acc: 0.5159\n","Epoch 80/100\n","50000/50000 [==============================] - 3s 57us/step - loss: 1.1912 - acc: 0.5696 - val_loss: 1.3547 - val_acc: 0.5242\n","Epoch 81/100\n","50000/50000 [==============================] - 3s 55us/step - loss: 1.1827 - acc: 0.5709 - val_loss: 1.3466 - val_acc: 0.5228\n","Epoch 82/100\n","50000/50000 [==============================] - 3s 55us/step - loss: 1.1833 - acc: 0.5720 - val_loss: 1.3297 - val_acc: 0.5283\n","Epoch 83/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.1768 - acc: 0.5750 - val_loss: 1.3583 - val_acc: 0.5217\n","Epoch 84/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.1780 - acc: 0.5761 - val_loss: 1.3466 - val_acc: 0.5230\n","Epoch 85/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.1727 - acc: 0.5774 - val_loss: 1.3775 - val_acc: 0.5135\n","Epoch 86/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.1802 - acc: 0.5732 - val_loss: 1.3427 - val_acc: 0.5288\n","Epoch 87/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.1735 - acc: 0.5768 - val_loss: 1.3488 - val_acc: 0.5260\n","Epoch 88/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.1732 - acc: 0.5779 - val_loss: 1.3500 - val_acc: 0.5256\n","Epoch 89/100\n","50000/50000 [==============================] - 3s 55us/step - loss: 1.1725 - acc: 0.5776 - val_loss: 1.3506 - val_acc: 0.5266\n","Epoch 90/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.1601 - acc: 0.5819 - val_loss: 1.3719 - val_acc: 0.5109\n","Epoch 91/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.1676 - acc: 0.5768 - val_loss: 1.3618 - val_acc: 0.5280\n","Epoch 92/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.1570 - acc: 0.5828 - val_loss: 1.3610 - val_acc: 0.5205\n","Epoch 93/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.1601 - acc: 0.5785 - val_loss: 1.3573 - val_acc: 0.5249\n","Epoch 94/100\n","50000/50000 [==============================] - 3s 55us/step - loss: 1.1674 - acc: 0.5795 - val_loss: 1.3734 - val_acc: 0.5214\n","Epoch 95/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.1535 - acc: 0.5826 - val_loss: 1.3476 - val_acc: 0.5280\n","Epoch 96/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.1567 - acc: 0.5798 - val_loss: 1.3796 - val_acc: 0.5216\n","Epoch 97/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.1525 - acc: 0.5836 - val_loss: 1.3485 - val_acc: 0.5280\n","Epoch 98/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.1460 - acc: 0.5855 - val_loss: 1.3737 - val_acc: 0.5216\n","Epoch 99/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.1429 - acc: 0.5852 - val_loss: 1.3624 - val_acc: 0.5252\n","Epoch 100/100\n","50000/50000 [==============================] - 3s 56us/step - loss: 1.1473 - acc: 0.5856 - val_loss: 1.3557 - val_acc: 0.5273\n","Test loss: 1.3556572998046874\n","Test acc: 0.5273\n"],"name":"stdout"}]},{"metadata":{"id":"3dE_ij-quuku","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2295},"outputId":"e5ee7ddd-f815-47af-c459-8b6df7a902ec","executionInfo":{"status":"ok","timestamp":1539099540363,"user_tz":240,"elapsed":93074,"user":{"displayName":"Manasa Singhekar","photoUrl":"","userId":"13090086925392106704"}}},"cell_type":"code","source":["if __name__ == '__main__':\n","    nb_epoch = 50\n","    batch_size = 512\n","    nb_classes = 10\n","\n","    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n","\n","    X_train = X_train.reshape(50000, 32 * 32 * 3)\n","    X_test = X_test.reshape(10000, 32 * 32 * 3)\n","\n","    X_train = X_train.astype('float32')\n","    X_test = X_test.astype('float32')\n","    X_train /= 255.0\n","    X_test /= 255.0\n","\n","    Y_train = np_utils.to_categorical(y_train, nb_classes)\n","    Y_test = np_utils.to_categorical(y_test, nb_classes)\n","\n","    # MLP\n","    model = Sequential()\n","    model.add(Dense(1024, input_shape=(3072, )))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(512))\n","    model.add(Activation('sigmoid'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(512))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(10))\n","    model.add(Activation('softmax'))\n","\n","    model.compile(loss='categorical_crossentropy',\n","                  optimizer='adam',\n","                  metrics=['accuracy'])\n","    model.summary()\n","\n","    # training\n","    history = model.fit(X_train, Y_train,\n","                        batch_size=batch_size,\n","                        nb_epoch=nb_epoch,\n","                        verbose=1,\n","                        validation_data=(X_test, Y_test))\n","\n","    save_history(history, 'history.txt')\n","\n","    loss, acc = model.evaluate(X_test, Y_test, verbose=0)\n","    print('Test loss:', loss)\n","    print('Test acc:', acc)"],"execution_count":32,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_25 (Dense)             (None, 1024)              3146752   \n","_________________________________________________________________\n","activation_25 (Activation)   (None, 1024)              0         \n","_________________________________________________________________\n","dropout_19 (Dropout)         (None, 1024)              0         \n","_________________________________________________________________\n","dense_26 (Dense)             (None, 512)               524800    \n","_________________________________________________________________\n","activation_26 (Activation)   (None, 512)               0         \n","_________________________________________________________________\n","dropout_20 (Dropout)         (None, 512)               0         \n","_________________________________________________________________\n","dense_27 (Dense)             (None, 512)               262656    \n","_________________________________________________________________\n","activation_27 (Activation)   (None, 512)               0         \n","_________________________________________________________________\n","dropout_21 (Dropout)         (None, 512)               0         \n","_________________________________________________________________\n","dense_28 (Dense)             (None, 10)                5130      \n","_________________________________________________________________\n","activation_28 (Activation)   (None, 10)                0         \n","=================================================================\n","Total params: 3,939,338\n","Trainable params: 3,939,338\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n","  warnings.warn('The `nb_epoch` argument in `fit` '\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 50000 samples, validate on 10000 samples\n","Epoch 1/50\n","50000/50000 [==============================] - 3s 55us/step - loss: 2.0255 - acc: 0.2561 - val_loss: 1.7797 - val_acc: 0.3696\n","Epoch 2/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.7723 - acc: 0.3585 - val_loss: 1.6729 - val_acc: 0.4028\n","Epoch 3/50\n","50000/50000 [==============================] - 2s 35us/step - loss: 1.6870 - acc: 0.3916 - val_loss: 1.6160 - val_acc: 0.4201\n","Epoch 4/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.6330 - acc: 0.4153 - val_loss: 1.5577 - val_acc: 0.4503\n","Epoch 5/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.5836 - acc: 0.4345 - val_loss: 1.5172 - val_acc: 0.4553\n","Epoch 6/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.5465 - acc: 0.4435 - val_loss: 1.4825 - val_acc: 0.4672\n","Epoch 7/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.5140 - acc: 0.4563 - val_loss: 1.4696 - val_acc: 0.4738\n","Epoch 8/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.4833 - acc: 0.4702 - val_loss: 1.4531 - val_acc: 0.4821\n","Epoch 9/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.4688 - acc: 0.4731 - val_loss: 1.4486 - val_acc: 0.4785\n","Epoch 10/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.4437 - acc: 0.4823 - val_loss: 1.4122 - val_acc: 0.4950\n","Epoch 11/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.4281 - acc: 0.4857 - val_loss: 1.3910 - val_acc: 0.5019\n","Epoch 12/50\n","50000/50000 [==============================] - 2s 35us/step - loss: 1.4145 - acc: 0.4929 - val_loss: 1.4085 - val_acc: 0.5000\n","Epoch 13/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.3987 - acc: 0.4964 - val_loss: 1.3766 - val_acc: 0.5045\n","Epoch 14/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.3808 - acc: 0.5055 - val_loss: 1.3900 - val_acc: 0.5050\n","Epoch 15/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.3644 - acc: 0.5126 - val_loss: 1.3485 - val_acc: 0.5152\n","Epoch 16/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.3505 - acc: 0.5143 - val_loss: 1.3502 - val_acc: 0.5158\n","Epoch 17/50\n","50000/50000 [==============================] - 2s 35us/step - loss: 1.3336 - acc: 0.5211 - val_loss: 1.3573 - val_acc: 0.5143\n","Epoch 18/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.3153 - acc: 0.5277 - val_loss: 1.3382 - val_acc: 0.5230\n","Epoch 19/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.3025 - acc: 0.5325 - val_loss: 1.3168 - val_acc: 0.5327\n","Epoch 20/50\n","50000/50000 [==============================] - 2s 35us/step - loss: 1.2895 - acc: 0.5383 - val_loss: 1.3140 - val_acc: 0.5268\n","Epoch 21/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.2750 - acc: 0.5424 - val_loss: 1.3189 - val_acc: 0.5273\n","Epoch 22/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.2631 - acc: 0.5500 - val_loss: 1.2929 - val_acc: 0.5411\n","Epoch 23/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.2480 - acc: 0.5507 - val_loss: 1.2907 - val_acc: 0.5413\n","Epoch 24/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.2376 - acc: 0.5573 - val_loss: 1.2816 - val_acc: 0.5492\n","Epoch 25/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.2319 - acc: 0.5549 - val_loss: 1.2876 - val_acc: 0.5377\n","Epoch 26/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.2186 - acc: 0.5606 - val_loss: 1.2774 - val_acc: 0.5438\n","Epoch 27/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.2008 - acc: 0.5672 - val_loss: 1.2910 - val_acc: 0.5399\n","Epoch 28/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.1865 - acc: 0.5750 - val_loss: 1.2621 - val_acc: 0.5510\n","Epoch 29/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.1802 - acc: 0.5754 - val_loss: 1.2810 - val_acc: 0.5444\n","Epoch 30/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.1683 - acc: 0.5794 - val_loss: 1.2649 - val_acc: 0.5471\n","Epoch 31/50\n","50000/50000 [==============================] - 2s 35us/step - loss: 1.1524 - acc: 0.5849 - val_loss: 1.2702 - val_acc: 0.5397\n","Epoch 32/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.1428 - acc: 0.5882 - val_loss: 1.2594 - val_acc: 0.5533\n","Epoch 33/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.1343 - acc: 0.5931 - val_loss: 1.2795 - val_acc: 0.5416\n","Epoch 34/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.1375 - acc: 0.5927 - val_loss: 1.2613 - val_acc: 0.5524\n","Epoch 35/50\n","50000/50000 [==============================] - 2s 35us/step - loss: 1.1234 - acc: 0.5944 - val_loss: 1.2645 - val_acc: 0.5482\n","Epoch 36/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.1079 - acc: 0.6017 - val_loss: 1.2518 - val_acc: 0.5540\n","Epoch 37/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.0978 - acc: 0.6062 - val_loss: 1.2667 - val_acc: 0.5579\n","Epoch 38/50\n","50000/50000 [==============================] - 2s 35us/step - loss: 1.0983 - acc: 0.6060 - val_loss: 1.2522 - val_acc: 0.5538\n","Epoch 39/50\n","50000/50000 [==============================] - 2s 35us/step - loss: 1.0855 - acc: 0.6100 - val_loss: 1.2604 - val_acc: 0.5546\n","Epoch 40/50\n","50000/50000 [==============================] - 2s 35us/step - loss: 1.0735 - acc: 0.6145 - val_loss: 1.2421 - val_acc: 0.5580\n","Epoch 41/50\n","50000/50000 [==============================] - 2s 35us/step - loss: 1.0610 - acc: 0.6178 - val_loss: 1.2431 - val_acc: 0.5599\n","Epoch 42/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.0543 - acc: 0.6200 - val_loss: 1.2324 - val_acc: 0.5646\n","Epoch 43/50\n","50000/50000 [==============================] - 2s 35us/step - loss: 1.0459 - acc: 0.6263 - val_loss: 1.2259 - val_acc: 0.5676\n","Epoch 44/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.0277 - acc: 0.6286 - val_loss: 1.2581 - val_acc: 0.5562\n","Epoch 45/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.0216 - acc: 0.6307 - val_loss: 1.2367 - val_acc: 0.5661\n","Epoch 46/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 1.0093 - acc: 0.6374 - val_loss: 1.2491 - val_acc: 0.5597\n","Epoch 47/50\n","50000/50000 [==============================] - 2s 35us/step - loss: 1.0133 - acc: 0.6326 - val_loss: 1.2388 - val_acc: 0.5624\n","Epoch 48/50\n","50000/50000 [==============================] - 2s 34us/step - loss: 1.0001 - acc: 0.6367 - val_loss: 1.2445 - val_acc: 0.5630\n","Epoch 49/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 0.9864 - acc: 0.6443 - val_loss: 1.2491 - val_acc: 0.5614\n","Epoch 50/50\n","50000/50000 [==============================] - 2s 36us/step - loss: 0.9740 - acc: 0.6513 - val_loss: 1.2429 - val_acc: 0.5627\n","Test loss: 1.242882384300232\n","Test acc: 0.5627\n"],"name":"stdout"}]},{"metadata":{"id":"7uWiicnyKBlF","colab_type":"code","colab":{}},"cell_type":"code","source":["#3. Network configuration\n","#a. Number of neurons in a layer\n","#b. Number of layers"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XIIiDNa8D5Sd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":10693},"outputId":"c8ca5205-62ec-482c-fa44-42192b723d8b","executionInfo":{"status":"ok","timestamp":1539104790051,"user_tz":240,"elapsed":1375126,"user":{"displayName":"Manasa Singhekar","photoUrl":"","userId":"13090086925392106704"}}},"cell_type":"code","source":["if __name__ == '__main__':\n","    nb_epoch = 300\n","    batch_size = 128\n","    nb_classes = 10\n","\n","    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n","\n","    X_train = X_train.reshape(50000, 32 * 32 * 3)\n","    X_test = X_test.reshape(10000, 32 * 32 * 3)\n","\n","    X_train = X_train.astype('float32')\n","    X_test = X_test.astype('float32')\n","    X_train /= 255.0\n","    X_test /= 255.0\n","\n","    Y_train = np_utils.to_categorical(y_train, nb_classes)\n","    Y_test = np_utils.to_categorical(y_test, nb_classes)\n","\n","    # MLP\n","    model = Sequential()\n","    model.add(Dense(1024, input_shape=(3072, )))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(512))\n","    model.add(Activation('sigmoid'))\n","    model.add(Dropout(0.2))\n","    #model.add(Dense(512))\n","   # model.add(Activation('relu'))\n","    #model.add(Dropout(0.2))\n","    model.add(Dense(10))\n","    model.add(Activation('softmax'))\n","\n","    model.compile(loss='categorical_crossentropy',\n","                  optimizer='adam',\n","                  metrics=['accuracy'])\n","    model.summary()\n","\n","    # training\n","    history = model.fit(X_train, Y_train,\n","                        batch_size=batch_size,\n","                        nb_epoch=nb_epoch,\n","                        verbose=1,\n","                        validation_data=(X_test, Y_test))\n","\n","    save_history(history, 'history.txt')\n","\n","    loss, acc = model.evaluate(X_test, Y_test, verbose=0)\n","    print('Test loss:', loss)\n","    print('Test acc:', acc)"],"execution_count":36,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_35 (Dense)             (None, 1024)              3146752   \n","_________________________________________________________________\n","activation_35 (Activation)   (None, 1024)              0         \n","_________________________________________________________________\n","dropout_26 (Dropout)         (None, 1024)              0         \n","_________________________________________________________________\n","dense_36 (Dense)             (None, 512)               524800    \n","_________________________________________________________________\n","activation_36 (Activation)   (None, 512)               0         \n","_________________________________________________________________\n","dropout_27 (Dropout)         (None, 512)               0         \n","_________________________________________________________________\n","dense_37 (Dense)             (None, 10)                5130      \n","_________________________________________________________________\n","activation_37 (Activation)   (None, 10)                0         \n","=================================================================\n","Total params: 3,676,682\n","Trainable params: 3,676,682\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n","  warnings.warn('The `nb_epoch` argument in `fit` '\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 50000 samples, validate on 10000 samples\n","Epoch 1/300\n","50000/50000 [==============================] - 6s 112us/step - loss: 1.9232 - acc: 0.3015 - val_loss: 1.6987 - val_acc: 0.3884\n","Epoch 2/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.7115 - acc: 0.3833 - val_loss: 1.6034 - val_acc: 0.4340\n","Epoch 3/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.6318 - acc: 0.4134 - val_loss: 1.5854 - val_acc: 0.4348\n","Epoch 4/300\n","50000/50000 [==============================] - 5s 93us/step - loss: 1.5881 - acc: 0.4302 - val_loss: 1.5187 - val_acc: 0.4560\n","Epoch 5/300\n","50000/50000 [==============================] - 5s 93us/step - loss: 1.5494 - acc: 0.4432 - val_loss: 1.4792 - val_acc: 0.4739\n","Epoch 6/300\n","50000/50000 [==============================] - 5s 94us/step - loss: 1.5194 - acc: 0.4544 - val_loss: 1.4631 - val_acc: 0.4769\n","Epoch 7/300\n","50000/50000 [==============================] - 5s 93us/step - loss: 1.4918 - acc: 0.4642 - val_loss: 1.4401 - val_acc: 0.4824\n","Epoch 8/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.4668 - acc: 0.4732 - val_loss: 1.4229 - val_acc: 0.4896\n","Epoch 9/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.4478 - acc: 0.4802 - val_loss: 1.4284 - val_acc: 0.4869\n","Epoch 10/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.4248 - acc: 0.4904 - val_loss: 1.4071 - val_acc: 0.4905\n","Epoch 11/300\n","50000/50000 [==============================] - 5s 93us/step - loss: 1.4125 - acc: 0.4934 - val_loss: 1.3775 - val_acc: 0.5124\n","Epoch 12/300\n","50000/50000 [==============================] - 5s 93us/step - loss: 1.3889 - acc: 0.5001 - val_loss: 1.3678 - val_acc: 0.5070\n","Epoch 13/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.3815 - acc: 0.5060 - val_loss: 1.3598 - val_acc: 0.5210\n","Epoch 14/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.3681 - acc: 0.5106 - val_loss: 1.3508 - val_acc: 0.5188\n","Epoch 15/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.3414 - acc: 0.5200 - val_loss: 1.3623 - val_acc: 0.5155\n","Epoch 16/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.3380 - acc: 0.5190 - val_loss: 1.3694 - val_acc: 0.5130\n","Epoch 17/300\n","50000/50000 [==============================] - 5s 93us/step - loss: 1.3237 - acc: 0.5261 - val_loss: 1.3371 - val_acc: 0.5231\n","Epoch 18/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.3080 - acc: 0.5294 - val_loss: 1.3523 - val_acc: 0.5172\n","Epoch 19/300\n","50000/50000 [==============================] - 5s 93us/step - loss: 1.3094 - acc: 0.5316 - val_loss: 1.3367 - val_acc: 0.5269\n","Epoch 20/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.2859 - acc: 0.5406 - val_loss: 1.3080 - val_acc: 0.5332\n","Epoch 21/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.2816 - acc: 0.5417 - val_loss: 1.3266 - val_acc: 0.5251\n","Epoch 22/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.2676 - acc: 0.5467 - val_loss: 1.3328 - val_acc: 0.5255\n","Epoch 23/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.2554 - acc: 0.5513 - val_loss: 1.3141 - val_acc: 0.5389\n","Epoch 24/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.2465 - acc: 0.5534 - val_loss: 1.3182 - val_acc: 0.5309\n","Epoch 25/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.2471 - acc: 0.5541 - val_loss: 1.3311 - val_acc: 0.5230\n","Epoch 26/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.2348 - acc: 0.5559 - val_loss: 1.2936 - val_acc: 0.5385\n","Epoch 27/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 1.2201 - acc: 0.5622 - val_loss: 1.3157 - val_acc: 0.5309\n","Epoch 28/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.2142 - acc: 0.5649 - val_loss: 1.2934 - val_acc: 0.5406\n","Epoch 29/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.2065 - acc: 0.5691 - val_loss: 1.3010 - val_acc: 0.5372\n","Epoch 30/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 1.1968 - acc: 0.5716 - val_loss: 1.2797 - val_acc: 0.5448\n","Epoch 31/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.1886 - acc: 0.5745 - val_loss: 1.2827 - val_acc: 0.5384\n","Epoch 32/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.1786 - acc: 0.5780 - val_loss: 1.2929 - val_acc: 0.5378\n","Epoch 33/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.1790 - acc: 0.5788 - val_loss: 1.3153 - val_acc: 0.5327\n","Epoch 34/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.1708 - acc: 0.5838 - val_loss: 1.2889 - val_acc: 0.5409\n","Epoch 35/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.1590 - acc: 0.5865 - val_loss: 1.2790 - val_acc: 0.5413\n","Epoch 36/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.1528 - acc: 0.5863 - val_loss: 1.2799 - val_acc: 0.5473\n","Epoch 37/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.1493 - acc: 0.5890 - val_loss: 1.2746 - val_acc: 0.5459\n","Epoch 38/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.1433 - acc: 0.5911 - val_loss: 1.2988 - val_acc: 0.5369\n","Epoch 39/300\n","50000/50000 [==============================] - 5s 93us/step - loss: 1.1393 - acc: 0.5909 - val_loss: 1.2769 - val_acc: 0.5454\n","Epoch 40/300\n","50000/50000 [==============================] - 5s 93us/step - loss: 1.1289 - acc: 0.5973 - val_loss: 1.2709 - val_acc: 0.5455\n","Epoch 41/300\n","50000/50000 [==============================] - 5s 94us/step - loss: 1.1215 - acc: 0.5973 - val_loss: 1.2758 - val_acc: 0.5465\n","Epoch 42/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 1.1212 - acc: 0.5991 - val_loss: 1.2849 - val_acc: 0.5430\n","Epoch 43/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.1125 - acc: 0.6021 - val_loss: 1.2730 - val_acc: 0.5511\n","Epoch 44/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.1150 - acc: 0.6007 - val_loss: 1.2753 - val_acc: 0.5488\n","Epoch 45/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.1062 - acc: 0.6053 - val_loss: 1.2985 - val_acc: 0.5398\n","Epoch 46/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.0938 - acc: 0.6080 - val_loss: 1.2699 - val_acc: 0.5481\n","Epoch 47/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.0927 - acc: 0.6096 - val_loss: 1.2781 - val_acc: 0.5426\n","Epoch 48/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.0860 - acc: 0.6139 - val_loss: 1.2803 - val_acc: 0.5491\n","Epoch 49/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.0837 - acc: 0.6109 - val_loss: 1.2667 - val_acc: 0.5517\n","Epoch 50/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.0767 - acc: 0.6157 - val_loss: 1.2803 - val_acc: 0.5481\n","Epoch 51/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.0686 - acc: 0.6183 - val_loss: 1.2862 - val_acc: 0.5437\n","Epoch 52/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.0715 - acc: 0.6173 - val_loss: 1.2796 - val_acc: 0.5490\n","Epoch 53/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.0663 - acc: 0.6181 - val_loss: 1.2915 - val_acc: 0.5559\n","Epoch 54/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.0621 - acc: 0.6208 - val_loss: 1.2838 - val_acc: 0.5521\n","Epoch 55/300\n","50000/50000 [==============================] - 5s 93us/step - loss: 1.0525 - acc: 0.6217 - val_loss: 1.2833 - val_acc: 0.5527\n","Epoch 56/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.0566 - acc: 0.6194 - val_loss: 1.2956 - val_acc: 0.5465\n","Epoch 57/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.0428 - acc: 0.6289 - val_loss: 1.2806 - val_acc: 0.5499\n","Epoch 58/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.0511 - acc: 0.6257 - val_loss: 1.2775 - val_acc: 0.5506\n","Epoch 59/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.0444 - acc: 0.6281 - val_loss: 1.2816 - val_acc: 0.5519\n","Epoch 60/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.0355 - acc: 0.6270 - val_loss: 1.2938 - val_acc: 0.5455\n","Epoch 61/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.0345 - acc: 0.6304 - val_loss: 1.2744 - val_acc: 0.5553\n","Epoch 62/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.0285 - acc: 0.6324 - val_loss: 1.2910 - val_acc: 0.5490\n","Epoch 63/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.0328 - acc: 0.6308 - val_loss: 1.2764 - val_acc: 0.5534\n","Epoch 64/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.0187 - acc: 0.6339 - val_loss: 1.2882 - val_acc: 0.5489\n","Epoch 65/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 1.0134 - acc: 0.6377 - val_loss: 1.2915 - val_acc: 0.5492\n","Epoch 66/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.0115 - acc: 0.6398 - val_loss: 1.2780 - val_acc: 0.5532\n","Epoch 67/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.0063 - acc: 0.6395 - val_loss: 1.2833 - val_acc: 0.5474\n","Epoch 68/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.0034 - acc: 0.6409 - val_loss: 1.3118 - val_acc: 0.5380\n","Epoch 69/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 1.0017 - acc: 0.6411 - val_loss: 1.2773 - val_acc: 0.5562\n","Epoch 70/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.9974 - acc: 0.6428 - val_loss: 1.2826 - val_acc: 0.5503\n","Epoch 71/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.9948 - acc: 0.6430 - val_loss: 1.2872 - val_acc: 0.5519\n","Epoch 72/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.9996 - acc: 0.6456 - val_loss: 1.2950 - val_acc: 0.5518\n","Epoch 73/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.9882 - acc: 0.6491 - val_loss: 1.3124 - val_acc: 0.5480\n","Epoch 74/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.9844 - acc: 0.6478 - val_loss: 1.2858 - val_acc: 0.5509\n","Epoch 75/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.9835 - acc: 0.6482 - val_loss: 1.3031 - val_acc: 0.5458\n","Epoch 76/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.9830 - acc: 0.6476 - val_loss: 1.2966 - val_acc: 0.5515\n","Epoch 77/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.9745 - acc: 0.6539 - val_loss: 1.2951 - val_acc: 0.5551\n","Epoch 78/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.9777 - acc: 0.6511 - val_loss: 1.3007 - val_acc: 0.5506\n","Epoch 79/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.9660 - acc: 0.6545 - val_loss: 1.3044 - val_acc: 0.5518\n","Epoch 80/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.9688 - acc: 0.6533 - val_loss: 1.3005 - val_acc: 0.5472\n","Epoch 81/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.9636 - acc: 0.6571 - val_loss: 1.3214 - val_acc: 0.5401\n","Epoch 82/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.9632 - acc: 0.6564 - val_loss: 1.3006 - val_acc: 0.5507\n","Epoch 83/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.9542 - acc: 0.6601 - val_loss: 1.2976 - val_acc: 0.5576\n","Epoch 84/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.9645 - acc: 0.6579 - val_loss: 1.3131 - val_acc: 0.5540\n","Epoch 85/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.9515 - acc: 0.6614 - val_loss: 1.2945 - val_acc: 0.5532\n","Epoch 86/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.9430 - acc: 0.6647 - val_loss: 1.3248 - val_acc: 0.5445\n","Epoch 87/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.9493 - acc: 0.6612 - val_loss: 1.2952 - val_acc: 0.5500\n","Epoch 88/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.9400 - acc: 0.6638 - val_loss: 1.3074 - val_acc: 0.5516\n","Epoch 89/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.9496 - acc: 0.6615 - val_loss: 1.3302 - val_acc: 0.5446\n","Epoch 90/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.9319 - acc: 0.6698 - val_loss: 1.2951 - val_acc: 0.5523\n","Epoch 91/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.9422 - acc: 0.6630 - val_loss: 1.3376 - val_acc: 0.5411\n","Epoch 92/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.9367 - acc: 0.6650 - val_loss: 1.3018 - val_acc: 0.5460\n","Epoch 93/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.9320 - acc: 0.6664 - val_loss: 1.3225 - val_acc: 0.5491\n","Epoch 94/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.9340 - acc: 0.6672 - val_loss: 1.3116 - val_acc: 0.5525\n","Epoch 95/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.9330 - acc: 0.6674 - val_loss: 1.3137 - val_acc: 0.5524\n","Epoch 96/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.9312 - acc: 0.6696 - val_loss: 1.3212 - val_acc: 0.5523\n","Epoch 97/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.9243 - acc: 0.6699 - val_loss: 1.3220 - val_acc: 0.5473\n","Epoch 98/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.9241 - acc: 0.6706 - val_loss: 1.3164 - val_acc: 0.5499\n","Epoch 99/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.9143 - acc: 0.6747 - val_loss: 1.3263 - val_acc: 0.5476\n","Epoch 100/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.9119 - acc: 0.6737 - val_loss: 1.3256 - val_acc: 0.5521\n","Epoch 101/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.9192 - acc: 0.6711 - val_loss: 1.3258 - val_acc: 0.5503\n","Epoch 102/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.9196 - acc: 0.6699 - val_loss: 1.3221 - val_acc: 0.5509\n","Epoch 103/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.9082 - acc: 0.6746 - val_loss: 1.3198 - val_acc: 0.5526\n","Epoch 104/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.9059 - acc: 0.6788 - val_loss: 1.3144 - val_acc: 0.5500\n","Epoch 105/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.9099 - acc: 0.6757 - val_loss: 1.3288 - val_acc: 0.5445\n","Epoch 106/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.9090 - acc: 0.6755 - val_loss: 1.3286 - val_acc: 0.5509\n","Epoch 107/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.9037 - acc: 0.6788 - val_loss: 1.3151 - val_acc: 0.5539\n","Epoch 108/300\n","50000/50000 [==============================] - 4s 90us/step - loss: 0.9089 - acc: 0.6776 - val_loss: 1.3196 - val_acc: 0.5531\n","Epoch 109/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8924 - acc: 0.6823 - val_loss: 1.3260 - val_acc: 0.5450\n","Epoch 110/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.8938 - acc: 0.6801 - val_loss: 1.3554 - val_acc: 0.5471\n","Epoch 111/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8961 - acc: 0.6820 - val_loss: 1.3398 - val_acc: 0.5407\n","Epoch 112/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8922 - acc: 0.6832 - val_loss: 1.3214 - val_acc: 0.5549\n","Epoch 113/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.8889 - acc: 0.6840 - val_loss: 1.3281 - val_acc: 0.5531\n","Epoch 114/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.8880 - acc: 0.6839 - val_loss: 1.3314 - val_acc: 0.5469\n","Epoch 115/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8859 - acc: 0.6836 - val_loss: 1.3410 - val_acc: 0.5498\n","Epoch 116/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.8866 - acc: 0.6844 - val_loss: 1.3465 - val_acc: 0.5502\n","Epoch 117/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8799 - acc: 0.6867 - val_loss: 1.3387 - val_acc: 0.5491\n","Epoch 118/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8801 - acc: 0.6842 - val_loss: 1.3362 - val_acc: 0.5483\n","Epoch 119/300\n","50000/50000 [==============================] - 4s 90us/step - loss: 0.8751 - acc: 0.6905 - val_loss: 1.3598 - val_acc: 0.5462\n","Epoch 120/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.8816 - acc: 0.6859 - val_loss: 1.3544 - val_acc: 0.5476\n","Epoch 121/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8775 - acc: 0.6887 - val_loss: 1.3478 - val_acc: 0.5495\n","Epoch 122/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.8783 - acc: 0.6892 - val_loss: 1.3581 - val_acc: 0.5352\n","Epoch 123/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8660 - acc: 0.6919 - val_loss: 1.3427 - val_acc: 0.5481\n","Epoch 124/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.8634 - acc: 0.6925 - val_loss: 1.3647 - val_acc: 0.5515\n","Epoch 125/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8644 - acc: 0.6909 - val_loss: 1.3583 - val_acc: 0.5469\n","Epoch 126/300\n","50000/50000 [==============================] - 4s 90us/step - loss: 0.8589 - acc: 0.6938 - val_loss: 1.3304 - val_acc: 0.5502\n","Epoch 127/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.8688 - acc: 0.6903 - val_loss: 1.3513 - val_acc: 0.5476\n","Epoch 128/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.8628 - acc: 0.6919 - val_loss: 1.3419 - val_acc: 0.5498\n","Epoch 129/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.8591 - acc: 0.6937 - val_loss: 1.3496 - val_acc: 0.5496\n","Epoch 130/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8558 - acc: 0.6952 - val_loss: 1.3237 - val_acc: 0.5486\n","Epoch 131/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.8679 - acc: 0.6931 - val_loss: 1.3596 - val_acc: 0.5456\n","Epoch 132/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8514 - acc: 0.6976 - val_loss: 1.3607 - val_acc: 0.5493\n","Epoch 133/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8590 - acc: 0.6937 - val_loss: 1.3670 - val_acc: 0.5460\n","Epoch 134/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8584 - acc: 0.6945 - val_loss: 1.3669 - val_acc: 0.5504\n","Epoch 135/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8506 - acc: 0.6979 - val_loss: 1.3442 - val_acc: 0.5442\n","Epoch 136/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8471 - acc: 0.6970 - val_loss: 1.3540 - val_acc: 0.5483\n","Epoch 137/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8458 - acc: 0.7003 - val_loss: 1.3660 - val_acc: 0.5481\n","Epoch 138/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.8478 - acc: 0.6944 - val_loss: 1.3801 - val_acc: 0.5393\n","Epoch 139/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8532 - acc: 0.6971 - val_loss: 1.3785 - val_acc: 0.5478\n","Epoch 140/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8425 - acc: 0.7001 - val_loss: 1.3814 - val_acc: 0.5465\n","Epoch 141/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8420 - acc: 0.7030 - val_loss: 1.3644 - val_acc: 0.5488\n","Epoch 142/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.8348 - acc: 0.7021 - val_loss: 1.3569 - val_acc: 0.5455\n","Epoch 143/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.8352 - acc: 0.7016 - val_loss: 1.3659 - val_acc: 0.5477\n","Epoch 144/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.8444 - acc: 0.6993 - val_loss: 1.3578 - val_acc: 0.5489\n","Epoch 145/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.8437 - acc: 0.7011 - val_loss: 1.3543 - val_acc: 0.5500\n","Epoch 146/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8369 - acc: 0.7045 - val_loss: 1.3699 - val_acc: 0.5446\n","Epoch 147/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8368 - acc: 0.7024 - val_loss: 1.3995 - val_acc: 0.5429\n","Epoch 148/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8202 - acc: 0.7075 - val_loss: 1.3871 - val_acc: 0.5386\n","Epoch 149/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8261 - acc: 0.7043 - val_loss: 1.3739 - val_acc: 0.5468\n","Epoch 150/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8229 - acc: 0.7062 - val_loss: 1.3612 - val_acc: 0.5443\n","Epoch 151/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8301 - acc: 0.7056 - val_loss: 1.3487 - val_acc: 0.5538\n","Epoch 152/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8341 - acc: 0.7022 - val_loss: 1.3960 - val_acc: 0.5490\n","Epoch 153/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8279 - acc: 0.7070 - val_loss: 1.4053 - val_acc: 0.5442\n","Epoch 154/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.8219 - acc: 0.7076 - val_loss: 1.3585 - val_acc: 0.5486\n","Epoch 155/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8234 - acc: 0.7040 - val_loss: 1.3619 - val_acc: 0.5497\n","Epoch 156/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8184 - acc: 0.7095 - val_loss: 1.3812 - val_acc: 0.5452\n","Epoch 157/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8200 - acc: 0.7076 - val_loss: 1.3932 - val_acc: 0.5427\n","Epoch 158/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.8158 - acc: 0.7109 - val_loss: 1.3824 - val_acc: 0.5500\n","Epoch 159/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8231 - acc: 0.7067 - val_loss: 1.3674 - val_acc: 0.5467\n","Epoch 160/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8174 - acc: 0.7081 - val_loss: 1.4010 - val_acc: 0.5469\n","Epoch 161/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8158 - acc: 0.7124 - val_loss: 1.4017 - val_acc: 0.5448\n","Epoch 162/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.8182 - acc: 0.7105 - val_loss: 1.3627 - val_acc: 0.5458\n","Epoch 163/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.8082 - acc: 0.7138 - val_loss: 1.3948 - val_acc: 0.5443\n","Epoch 164/300\n","50000/50000 [==============================] - 5s 93us/step - loss: 0.8084 - acc: 0.7130 - val_loss: 1.4090 - val_acc: 0.5399\n","Epoch 165/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8083 - acc: 0.7133 - val_loss: 1.3840 - val_acc: 0.5463\n","Epoch 166/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8067 - acc: 0.7129 - val_loss: 1.3878 - val_acc: 0.5433\n","Epoch 167/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8075 - acc: 0.7137 - val_loss: 1.3807 - val_acc: 0.5492\n","Epoch 168/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8127 - acc: 0.7109 - val_loss: 1.3891 - val_acc: 0.5491\n","Epoch 169/300\n","50000/50000 [==============================] - 5s 93us/step - loss: 0.8003 - acc: 0.7174 - val_loss: 1.4142 - val_acc: 0.5464\n","Epoch 170/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7969 - acc: 0.7166 - val_loss: 1.3615 - val_acc: 0.5490\n","Epoch 171/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.8066 - acc: 0.7137 - val_loss: 1.4001 - val_acc: 0.5412\n","Epoch 172/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7915 - acc: 0.7193 - val_loss: 1.3984 - val_acc: 0.5435\n","Epoch 173/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.8009 - acc: 0.7162 - val_loss: 1.3855 - val_acc: 0.5474\n","Epoch 174/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.8008 - acc: 0.7176 - val_loss: 1.3890 - val_acc: 0.5454\n","Epoch 175/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.8006 - acc: 0.7156 - val_loss: 1.3988 - val_acc: 0.5455\n","Epoch 176/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.8002 - acc: 0.7163 - val_loss: 1.3968 - val_acc: 0.5406\n","Epoch 177/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7993 - acc: 0.7153 - val_loss: 1.3981 - val_acc: 0.5437\n","Epoch 178/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7961 - acc: 0.7171 - val_loss: 1.3773 - val_acc: 0.5446\n","Epoch 179/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7981 - acc: 0.7170 - val_loss: 1.4131 - val_acc: 0.5411\n","Epoch 180/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7995 - acc: 0.7168 - val_loss: 1.3805 - val_acc: 0.5456\n","Epoch 181/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7955 - acc: 0.7181 - val_loss: 1.4072 - val_acc: 0.5429\n","Epoch 182/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7883 - acc: 0.7196 - val_loss: 1.4280 - val_acc: 0.5477\n","Epoch 183/300\n","50000/50000 [==============================] - 4s 90us/step - loss: 0.7933 - acc: 0.7191 - val_loss: 1.4197 - val_acc: 0.5397\n","Epoch 184/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.7924 - acc: 0.7183 - val_loss: 1.4110 - val_acc: 0.5454\n","Epoch 185/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.8003 - acc: 0.7145 - val_loss: 1.3817 - val_acc: 0.5492\n","Epoch 186/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7800 - acc: 0.7231 - val_loss: 1.3998 - val_acc: 0.5421\n","Epoch 187/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.7894 - acc: 0.7218 - val_loss: 1.4182 - val_acc: 0.5447\n","Epoch 188/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.7797 - acc: 0.7229 - val_loss: 1.3983 - val_acc: 0.5410\n","Epoch 189/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.7841 - acc: 0.7218 - val_loss: 1.3916 - val_acc: 0.5490\n","Epoch 190/300\n","50000/50000 [==============================] - 4s 90us/step - loss: 0.7734 - acc: 0.7260 - val_loss: 1.3966 - val_acc: 0.5498\n","Epoch 191/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7803 - acc: 0.7208 - val_loss: 1.4374 - val_acc: 0.5422\n","Epoch 192/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7785 - acc: 0.7240 - val_loss: 1.4026 - val_acc: 0.5369\n","Epoch 193/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7807 - acc: 0.7220 - val_loss: 1.4048 - val_acc: 0.5493\n","Epoch 194/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7762 - acc: 0.7243 - val_loss: 1.4121 - val_acc: 0.5461\n","Epoch 195/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7795 - acc: 0.7231 - val_loss: 1.4181 - val_acc: 0.5494\n","Epoch 196/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7694 - acc: 0.7270 - val_loss: 1.4031 - val_acc: 0.5440\n","Epoch 197/300\n","50000/50000 [==============================] - 5s 93us/step - loss: 0.7726 - acc: 0.7255 - val_loss: 1.4213 - val_acc: 0.5447\n","Epoch 198/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7729 - acc: 0.7250 - val_loss: 1.4291 - val_acc: 0.5431\n","Epoch 199/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7685 - acc: 0.7280 - val_loss: 1.4828 - val_acc: 0.5323\n","Epoch 200/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7747 - acc: 0.7236 - val_loss: 1.4289 - val_acc: 0.5441\n","Epoch 201/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7765 - acc: 0.7235 - val_loss: 1.3937 - val_acc: 0.5440\n","Epoch 202/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7645 - acc: 0.7277 - val_loss: 1.4340 - val_acc: 0.5370\n","Epoch 203/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7720 - acc: 0.7248 - val_loss: 1.4094 - val_acc: 0.5485\n","Epoch 204/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7655 - acc: 0.7297 - val_loss: 1.4156 - val_acc: 0.5460\n","Epoch 205/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7775 - acc: 0.7268 - val_loss: 1.4230 - val_acc: 0.5445\n","Epoch 206/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7567 - acc: 0.7298 - val_loss: 1.4122 - val_acc: 0.5495\n","Epoch 207/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7725 - acc: 0.7235 - val_loss: 1.4284 - val_acc: 0.5401\n","Epoch 208/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7691 - acc: 0.7272 - val_loss: 1.4260 - val_acc: 0.5481\n","Epoch 209/300\n","50000/50000 [==============================] - 5s 93us/step - loss: 0.7654 - acc: 0.7290 - val_loss: 1.4342 - val_acc: 0.5433\n","Epoch 210/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7610 - acc: 0.7295 - val_loss: 1.4090 - val_acc: 0.5467\n","Epoch 211/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7636 - acc: 0.7301 - val_loss: 1.4469 - val_acc: 0.5402\n","Epoch 212/300\n","50000/50000 [==============================] - 5s 94us/step - loss: 0.7585 - acc: 0.7319 - val_loss: 1.4556 - val_acc: 0.5379\n","Epoch 213/300\n","50000/50000 [==============================] - 5s 93us/step - loss: 0.7649 - acc: 0.7306 - val_loss: 1.4365 - val_acc: 0.5437\n","Epoch 214/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7662 - acc: 0.7297 - val_loss: 1.4339 - val_acc: 0.5387\n","Epoch 215/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7559 - acc: 0.7336 - val_loss: 1.4340 - val_acc: 0.5404\n","Epoch 216/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7538 - acc: 0.7323 - val_loss: 1.4248 - val_acc: 0.5444\n","Epoch 217/300\n","50000/50000 [==============================] - 5s 93us/step - loss: 0.7535 - acc: 0.7342 - val_loss: 1.4357 - val_acc: 0.5499\n","Epoch 218/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7515 - acc: 0.7335 - val_loss: 1.4413 - val_acc: 0.5420\n","Epoch 219/300\n","50000/50000 [==============================] - 5s 93us/step - loss: 0.7515 - acc: 0.7321 - val_loss: 1.4221 - val_acc: 0.5457\n","Epoch 220/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7522 - acc: 0.7343 - val_loss: 1.4749 - val_acc: 0.5382\n","Epoch 221/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7414 - acc: 0.7361 - val_loss: 1.4237 - val_acc: 0.5479\n","Epoch 222/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7505 - acc: 0.7348 - val_loss: 1.4651 - val_acc: 0.5413\n","Epoch 223/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7535 - acc: 0.7331 - val_loss: 1.4236 - val_acc: 0.5426\n","Epoch 224/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7488 - acc: 0.7333 - val_loss: 1.4351 - val_acc: 0.5433\n","Epoch 225/300\n","50000/50000 [==============================] - 5s 95us/step - loss: 0.7549 - acc: 0.7326 - val_loss: 1.4291 - val_acc: 0.5487\n","Epoch 226/300\n","50000/50000 [==============================] - 5s 95us/step - loss: 0.7394 - acc: 0.7383 - val_loss: 1.4338 - val_acc: 0.5461\n","Epoch 227/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7433 - acc: 0.7356 - val_loss: 1.4311 - val_acc: 0.5387\n","Epoch 228/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7475 - acc: 0.7343 - val_loss: 1.4631 - val_acc: 0.5357\n","Epoch 229/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7583 - acc: 0.7337 - val_loss: 1.4532 - val_acc: 0.5407\n","Epoch 230/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7384 - acc: 0.7392 - val_loss: 1.4482 - val_acc: 0.5476\n","Epoch 231/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7442 - acc: 0.7379 - val_loss: 1.4448 - val_acc: 0.5352\n","Epoch 232/300\n","50000/50000 [==============================] - 5s 93us/step - loss: 0.7481 - acc: 0.7341 - val_loss: 1.4577 - val_acc: 0.5440\n","Epoch 233/300\n","50000/50000 [==============================] - 5s 94us/step - loss: 0.7450 - acc: 0.7353 - val_loss: 1.4561 - val_acc: 0.5455\n","Epoch 234/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7403 - acc: 0.7371 - val_loss: 1.4280 - val_acc: 0.5428\n","Epoch 235/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7369 - acc: 0.7379 - val_loss: 1.4625 - val_acc: 0.5494\n","Epoch 236/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7345 - acc: 0.7394 - val_loss: 1.4611 - val_acc: 0.5389\n","Epoch 237/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7339 - acc: 0.7394 - val_loss: 1.4596 - val_acc: 0.5407\n","Epoch 238/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7409 - acc: 0.7359 - val_loss: 1.4627 - val_acc: 0.5449\n","Epoch 239/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7370 - acc: 0.7405 - val_loss: 1.4141 - val_acc: 0.5445\n","Epoch 240/300\n","50000/50000 [==============================] - 5s 94us/step - loss: 0.7510 - acc: 0.7374 - val_loss: 1.4124 - val_acc: 0.5498\n","Epoch 241/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7333 - acc: 0.7397 - val_loss: 1.4632 - val_acc: 0.5462\n","Epoch 242/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7362 - acc: 0.7408 - val_loss: 1.4564 - val_acc: 0.5439\n","Epoch 243/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7313 - acc: 0.7400 - val_loss: 1.4674 - val_acc: 0.5459\n","Epoch 244/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7360 - acc: 0.7398 - val_loss: 1.4360 - val_acc: 0.5455\n","Epoch 245/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7353 - acc: 0.7374 - val_loss: 1.4500 - val_acc: 0.5391\n","Epoch 246/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7359 - acc: 0.7379 - val_loss: 1.4297 - val_acc: 0.5364\n","Epoch 247/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7331 - acc: 0.7402 - val_loss: 1.4911 - val_acc: 0.5396\n","Epoch 248/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7337 - acc: 0.7384 - val_loss: 1.4395 - val_acc: 0.5429\n","Epoch 249/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.7255 - acc: 0.7430 - val_loss: 1.4128 - val_acc: 0.5476\n","Epoch 250/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7303 - acc: 0.7421 - val_loss: 1.4476 - val_acc: 0.5410\n","Epoch 251/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7212 - acc: 0.7435 - val_loss: 1.4446 - val_acc: 0.5412\n","Epoch 252/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7271 - acc: 0.7412 - val_loss: 1.4440 - val_acc: 0.5443\n","Epoch 253/300\n","50000/50000 [==============================] - 5s 94us/step - loss: 0.7188 - acc: 0.7451 - val_loss: 1.4624 - val_acc: 0.5428\n","Epoch 254/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.7251 - acc: 0.7432 - val_loss: 1.4557 - val_acc: 0.5438\n","Epoch 255/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7255 - acc: 0.7425 - val_loss: 1.4723 - val_acc: 0.5433\n","Epoch 256/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7239 - acc: 0.7439 - val_loss: 1.4774 - val_acc: 0.5337\n","Epoch 257/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.7208 - acc: 0.7438 - val_loss: 1.4573 - val_acc: 0.5461\n","Epoch 258/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7226 - acc: 0.7438 - val_loss: 1.4645 - val_acc: 0.5365\n","Epoch 259/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7189 - acc: 0.7468 - val_loss: 1.4406 - val_acc: 0.5493\n","Epoch 260/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7196 - acc: 0.7449 - val_loss: 1.4537 - val_acc: 0.5456\n","Epoch 261/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7148 - acc: 0.7453 - val_loss: 1.4866 - val_acc: 0.5416\n","Epoch 262/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7228 - acc: 0.7454 - val_loss: 1.4808 - val_acc: 0.5441\n","Epoch 263/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7152 - acc: 0.7474 - val_loss: 1.4443 - val_acc: 0.5388\n","Epoch 264/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7229 - acc: 0.7440 - val_loss: 1.4557 - val_acc: 0.5403\n","Epoch 265/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7184 - acc: 0.7444 - val_loss: 1.4490 - val_acc: 0.5428\n","Epoch 266/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7231 - acc: 0.7450 - val_loss: 1.4764 - val_acc: 0.5443\n","Epoch 267/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7180 - acc: 0.7471 - val_loss: 1.4802 - val_acc: 0.5471\n","Epoch 268/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7146 - acc: 0.7473 - val_loss: 1.4717 - val_acc: 0.5430\n","Epoch 269/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7249 - acc: 0.7399 - val_loss: 1.4795 - val_acc: 0.5395\n","Epoch 270/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7164 - acc: 0.7451 - val_loss: 1.4903 - val_acc: 0.5415\n","Epoch 271/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7188 - acc: 0.7469 - val_loss: 1.4940 - val_acc: 0.5378\n","Epoch 272/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7163 - acc: 0.7467 - val_loss: 1.5054 - val_acc: 0.5443\n","Epoch 273/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7156 - acc: 0.7471 - val_loss: 1.4885 - val_acc: 0.5418\n","Epoch 274/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7158 - acc: 0.7474 - val_loss: 1.4409 - val_acc: 0.5440\n","Epoch 275/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7091 - acc: 0.7508 - val_loss: 1.4607 - val_acc: 0.5385\n","Epoch 276/300\n","50000/50000 [==============================] - 5s 92us/step - loss: 0.7117 - acc: 0.7480 - val_loss: 1.5038 - val_acc: 0.5374\n","Epoch 277/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7110 - acc: 0.7492 - val_loss: 1.4602 - val_acc: 0.5401\n","Epoch 278/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7025 - acc: 0.7522 - val_loss: 1.4604 - val_acc: 0.5432\n","Epoch 279/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7075 - acc: 0.7494 - val_loss: 1.4613 - val_acc: 0.5452\n","Epoch 280/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7133 - acc: 0.7489 - val_loss: 1.4923 - val_acc: 0.5373\n","Epoch 281/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7116 - acc: 0.7475 - val_loss: 1.4475 - val_acc: 0.5423\n","Epoch 282/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.7161 - acc: 0.7483 - val_loss: 1.4453 - val_acc: 0.5460\n","Epoch 283/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.7006 - acc: 0.7525 - val_loss: 1.4597 - val_acc: 0.5457\n","Epoch 284/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7064 - acc: 0.7530 - val_loss: 1.4632 - val_acc: 0.5368\n","Epoch 285/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7162 - acc: 0.7471 - val_loss: 1.4629 - val_acc: 0.5461\n","Epoch 286/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7022 - acc: 0.7516 - val_loss: 1.4881 - val_acc: 0.5481\n","Epoch 287/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7034 - acc: 0.7517 - val_loss: 1.4929 - val_acc: 0.5477\n","Epoch 288/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.6983 - acc: 0.7523 - val_loss: 1.4893 - val_acc: 0.5400\n","Epoch 289/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7020 - acc: 0.7507 - val_loss: 1.4803 - val_acc: 0.5440\n","Epoch 290/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7089 - acc: 0.7510 - val_loss: 1.4925 - val_acc: 0.5395\n","Epoch 291/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7000 - acc: 0.7528 - val_loss: 1.4551 - val_acc: 0.5433\n","Epoch 292/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.6974 - acc: 0.7533 - val_loss: 1.5058 - val_acc: 0.5395\n","Epoch 293/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.6941 - acc: 0.7547 - val_loss: 1.4532 - val_acc: 0.5470\n","Epoch 294/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.7009 - acc: 0.7527 - val_loss: 1.4897 - val_acc: 0.5415\n","Epoch 295/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.6939 - acc: 0.7529 - val_loss: 1.4623 - val_acc: 0.5459\n","Epoch 296/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.6961 - acc: 0.7549 - val_loss: 1.4726 - val_acc: 0.5415\n","Epoch 297/300\n","50000/50000 [==============================] - 4s 90us/step - loss: 0.6963 - acc: 0.7549 - val_loss: 1.5148 - val_acc: 0.5450\n","Epoch 298/300\n","50000/50000 [==============================] - 5s 91us/step - loss: 0.7051 - acc: 0.7520 - val_loss: 1.4682 - val_acc: 0.5451\n","Epoch 299/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.6923 - acc: 0.7544 - val_loss: 1.4751 - val_acc: 0.5434\n","Epoch 300/300\n","50000/50000 [==============================] - 5s 90us/step - loss: 0.6956 - acc: 0.7552 - val_loss: 1.4695 - val_acc: 0.5437\n","Test loss: 1.469488694190979\n","Test acc: 0.5437\n"],"name":"stdout"}]},{"metadata":{"id":"k_z8dia5tlnc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":4097},"outputId":"e63a6182-0962-4a94-ea5e-35873847e2a6","executionInfo":{"status":"ok","timestamp":1539107192860,"user_tz":240,"elapsed":549435,"user":{"displayName":"Manasa Singhekar","photoUrl":"","userId":"13090086925392106704"}}},"cell_type":"code","source":["if __name__ == '__main__':\n","    nb_epoch = 100\n","    batch_size = 128\n","    nb_classes = 10\n","\n","    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n","\n","    X_train = X_train.reshape(50000, 32 * 32 * 3)\n","    X_test = X_test.reshape(10000, 32 * 32 * 3)\n","\n","    X_train = X_train.astype('float32')\n","    X_test = X_test.astype('float32')\n","    X_train /= 255.0\n","    X_test /= 255.0\n","\n","    Y_train = np_utils.to_categorical(y_train, nb_classes)\n","    Y_test = np_utils.to_categorical(y_test, nb_classes)\n","\n","    # MLP\n","    model = Sequential()\n","    model.add(Dense(1024, input_shape=(3072, )))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(512))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(512))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(128))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(10))\n","    model.add(Activation('softmax'))\n","\n","    model.compile(loss='categorical_crossentropy',\n","                  optimizer='adam',\n","                  metrics=['accuracy'])\n","    model.summary()\n","\n","    # training\n","    history = model.fit(X_train, Y_train,\n","                        batch_size=batch_size,\n","                        nb_epoch=nb_epoch,\n","                        verbose=1,\n","                        validation_data=(X_test, Y_test))\n","\n","    save_history(history, 'history.txt')\n","\n","    loss, acc = model.evaluate(X_test, Y_test, verbose=0)\n","    print('Test loss:', loss)\n","    print('Test acc:', acc)"],"execution_count":37,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_38 (Dense)             (None, 1024)              3146752   \n","_________________________________________________________________\n","activation_38 (Activation)   (None, 1024)              0         \n","_________________________________________________________________\n","dropout_28 (Dropout)         (None, 1024)              0         \n","_________________________________________________________________\n","dense_39 (Dense)             (None, 512)               524800    \n","_________________________________________________________________\n","activation_39 (Activation)   (None, 512)               0         \n","_________________________________________________________________\n","dropout_29 (Dropout)         (None, 512)               0         \n","_________________________________________________________________\n","dense_40 (Dense)             (None, 512)               262656    \n","_________________________________________________________________\n","activation_40 (Activation)   (None, 512)               0         \n","_________________________________________________________________\n","dropout_30 (Dropout)         (None, 512)               0         \n","_________________________________________________________________\n","dense_41 (Dense)             (None, 128)               65664     \n","_________________________________________________________________\n","activation_41 (Activation)   (None, 128)               0         \n","_________________________________________________________________\n","dropout_31 (Dropout)         (None, 128)               0         \n","_________________________________________________________________\n","dense_42 (Dense)             (None, 10)                1290      \n","_________________________________________________________________\n","activation_42 (Activation)   (None, 10)                0         \n","=================================================================\n","Total params: 4,001,162\n","Trainable params: 4,001,162\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n","  warnings.warn('The `nb_epoch` argument in `fit` '\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 50000 samples, validate on 10000 samples\n","Epoch 1/100\n","50000/50000 [==============================] - 7s 134us/step - loss: 2.0317 - acc: 0.2454 - val_loss: 1.8362 - val_acc: 0.3387\n","Epoch 2/100\n","50000/50000 [==============================] - 6s 110us/step - loss: 1.8520 - acc: 0.3277 - val_loss: 1.7303 - val_acc: 0.3807\n","Epoch 3/100\n","50000/50000 [==============================] - 5s 110us/step - loss: 1.7915 - acc: 0.3542 - val_loss: 1.6932 - val_acc: 0.3956\n","Epoch 4/100\n","50000/50000 [==============================] - 6s 110us/step - loss: 1.7498 - acc: 0.3703 - val_loss: 1.6833 - val_acc: 0.4040\n","Epoch 5/100\n","50000/50000 [==============================] - 6s 110us/step - loss: 1.7240 - acc: 0.3796 - val_loss: 1.6287 - val_acc: 0.4165\n","Epoch 6/100\n","50000/50000 [==============================] - 5s 110us/step - loss: 1.6900 - acc: 0.3932 - val_loss: 1.6255 - val_acc: 0.4182\n","Epoch 7/100\n","50000/50000 [==============================] - 5s 110us/step - loss: 1.6730 - acc: 0.3984 - val_loss: 1.5750 - val_acc: 0.4382\n","Epoch 8/100\n","50000/50000 [==============================] - 6s 110us/step - loss: 1.6525 - acc: 0.4065 - val_loss: 1.5828 - val_acc: 0.4340\n","Epoch 9/100\n","50000/50000 [==============================] - 5s 110us/step - loss: 1.6376 - acc: 0.4134 - val_loss: 1.5713 - val_acc: 0.4314\n","Epoch 10/100\n","50000/50000 [==============================] - 5s 110us/step - loss: 1.6244 - acc: 0.4190 - val_loss: 1.5529 - val_acc: 0.4548\n","Epoch 11/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.6189 - acc: 0.4190 - val_loss: 1.5207 - val_acc: 0.4559\n","Epoch 12/100\n","50000/50000 [==============================] - 5s 110us/step - loss: 1.5988 - acc: 0.4254 - val_loss: 1.5580 - val_acc: 0.4496\n","Epoch 13/100\n","50000/50000 [==============================] - 5s 110us/step - loss: 1.5857 - acc: 0.4310 - val_loss: 1.5384 - val_acc: 0.4513\n","Epoch 14/100\n","50000/50000 [==============================] - 5s 110us/step - loss: 1.5725 - acc: 0.4352 - val_loss: 1.5248 - val_acc: 0.4579\n","Epoch 15/100\n","50000/50000 [==============================] - 5s 110us/step - loss: 1.5650 - acc: 0.4381 - val_loss: 1.5346 - val_acc: 0.4451\n","Epoch 16/100\n","50000/50000 [==============================] - 5s 110us/step - loss: 1.5560 - acc: 0.4402 - val_loss: 1.5169 - val_acc: 0.4589\n","Epoch 17/100\n","50000/50000 [==============================] - 5s 110us/step - loss: 1.5548 - acc: 0.4436 - val_loss: 1.5224 - val_acc: 0.4599\n","Epoch 18/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.5434 - acc: 0.4473 - val_loss: 1.4831 - val_acc: 0.4694\n","Epoch 19/100\n","50000/50000 [==============================] - 5s 110us/step - loss: 1.5346 - acc: 0.4490 - val_loss: 1.4774 - val_acc: 0.4765\n","Epoch 20/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.5295 - acc: 0.4536 - val_loss: 1.4605 - val_acc: 0.4776\n","Epoch 21/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.5281 - acc: 0.4540 - val_loss: 1.5003 - val_acc: 0.4667\n","Epoch 22/100\n","50000/50000 [==============================] - 5s 110us/step - loss: 1.5059 - acc: 0.4575 - val_loss: 1.4755 - val_acc: 0.4798\n","Epoch 23/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.5115 - acc: 0.4556 - val_loss: 1.4599 - val_acc: 0.4887\n","Epoch 24/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.5007 - acc: 0.4598 - val_loss: 1.4670 - val_acc: 0.4865\n","Epoch 25/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.4954 - acc: 0.4628 - val_loss: 1.4834 - val_acc: 0.4738\n","Epoch 26/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.4916 - acc: 0.4621 - val_loss: 1.4723 - val_acc: 0.4763\n","Epoch 27/100\n","50000/50000 [==============================] - 5s 110us/step - loss: 1.4821 - acc: 0.4670 - val_loss: 1.4477 - val_acc: 0.4809\n","Epoch 28/100\n","50000/50000 [==============================] - 5s 110us/step - loss: 1.4757 - acc: 0.4703 - val_loss: 1.4507 - val_acc: 0.4883\n","Epoch 29/100\n","50000/50000 [==============================] - 6s 110us/step - loss: 1.4721 - acc: 0.4673 - val_loss: 1.4363 - val_acc: 0.4874\n","Epoch 30/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.4730 - acc: 0.4704 - val_loss: 1.4438 - val_acc: 0.4945\n","Epoch 31/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.4733 - acc: 0.4712 - val_loss: 1.4426 - val_acc: 0.4904\n","Epoch 32/100\n","50000/50000 [==============================] - 5s 108us/step - loss: 1.4556 - acc: 0.4739 - val_loss: 1.4437 - val_acc: 0.4925\n","Epoch 33/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.4550 - acc: 0.4753 - val_loss: 1.4644 - val_acc: 0.4751\n","Epoch 34/100\n","50000/50000 [==============================] - 5s 108us/step - loss: 1.4567 - acc: 0.4753 - val_loss: 1.4274 - val_acc: 0.4930\n","Epoch 35/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.4486 - acc: 0.4800 - val_loss: 1.4367 - val_acc: 0.4862\n","Epoch 36/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.4423 - acc: 0.4821 - val_loss: 1.4385 - val_acc: 0.4914\n","Epoch 37/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.4395 - acc: 0.4834 - val_loss: 1.4309 - val_acc: 0.4935\n","Epoch 38/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.4367 - acc: 0.4820 - val_loss: 1.4246 - val_acc: 0.4922\n","Epoch 39/100\n","50000/50000 [==============================] - 5s 108us/step - loss: 1.4313 - acc: 0.4837 - val_loss: 1.4055 - val_acc: 0.4973\n","Epoch 40/100\n","50000/50000 [==============================] - 5s 108us/step - loss: 1.4363 - acc: 0.4820 - val_loss: 1.4227 - val_acc: 0.4982\n","Epoch 41/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.4235 - acc: 0.4896 - val_loss: 1.4276 - val_acc: 0.4884\n","Epoch 42/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.4172 - acc: 0.4888 - val_loss: 1.4366 - val_acc: 0.4966\n","Epoch 43/100\n","50000/50000 [==============================] - 5s 107us/step - loss: 1.4207 - acc: 0.4869 - val_loss: 1.4225 - val_acc: 0.4941\n","Epoch 44/100\n","50000/50000 [==============================] - 5s 108us/step - loss: 1.4098 - acc: 0.4933 - val_loss: 1.4091 - val_acc: 0.5047\n","Epoch 45/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.4107 - acc: 0.4928 - val_loss: 1.4222 - val_acc: 0.4945\n","Epoch 46/100\n","50000/50000 [==============================] - 5s 108us/step - loss: 1.4081 - acc: 0.4943 - val_loss: 1.4226 - val_acc: 0.4954\n","Epoch 47/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.4046 - acc: 0.4933 - val_loss: 1.3945 - val_acc: 0.5014\n","Epoch 48/100\n","50000/50000 [==============================] - 5s 108us/step - loss: 1.3997 - acc: 0.4965 - val_loss: 1.4203 - val_acc: 0.4988\n","Epoch 49/100\n","50000/50000 [==============================] - 5s 108us/step - loss: 1.3903 - acc: 0.5016 - val_loss: 1.4273 - val_acc: 0.4938\n","Epoch 50/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3969 - acc: 0.4981 - val_loss: 1.3920 - val_acc: 0.5015\n","Epoch 51/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3875 - acc: 0.5026 - val_loss: 1.4145 - val_acc: 0.4958\n","Epoch 52/100\n","50000/50000 [==============================] - 5s 108us/step - loss: 1.3867 - acc: 0.5013 - val_loss: 1.3969 - val_acc: 0.5087\n","Epoch 53/100\n","50000/50000 [==============================] - 5s 108us/step - loss: 1.3773 - acc: 0.5043 - val_loss: 1.4096 - val_acc: 0.4958\n","Epoch 54/100\n","50000/50000 [==============================] - 5s 108us/step - loss: 1.3886 - acc: 0.4999 - val_loss: 1.3898 - val_acc: 0.5123\n","Epoch 55/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3818 - acc: 0.5021 - val_loss: 1.4098 - val_acc: 0.4973\n","Epoch 56/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3762 - acc: 0.5043 - val_loss: 1.3928 - val_acc: 0.5003\n","Epoch 57/100\n","50000/50000 [==============================] - 5s 108us/step - loss: 1.3719 - acc: 0.5053 - val_loss: 1.4036 - val_acc: 0.5010\n","Epoch 58/100\n","50000/50000 [==============================] - 5s 108us/step - loss: 1.3768 - acc: 0.5067 - val_loss: 1.4072 - val_acc: 0.4974\n","Epoch 59/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3772 - acc: 0.5072 - val_loss: 1.4022 - val_acc: 0.5009\n","Epoch 60/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3665 - acc: 0.5107 - val_loss: 1.3933 - val_acc: 0.5018\n","Epoch 61/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3620 - acc: 0.5106 - val_loss: 1.3950 - val_acc: 0.5054\n","Epoch 62/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3586 - acc: 0.5130 - val_loss: 1.3858 - val_acc: 0.5118\n","Epoch 63/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3588 - acc: 0.5125 - val_loss: 1.3935 - val_acc: 0.4938\n","Epoch 64/100\n","50000/50000 [==============================] - 5s 108us/step - loss: 1.3510 - acc: 0.5129 - val_loss: 1.3879 - val_acc: 0.5065\n","Epoch 65/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3612 - acc: 0.5119 - val_loss: 1.4020 - val_acc: 0.5004\n","Epoch 66/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3508 - acc: 0.5151 - val_loss: 1.3854 - val_acc: 0.5093\n","Epoch 67/100\n","50000/50000 [==============================] - 5s 110us/step - loss: 1.3512 - acc: 0.5141 - val_loss: 1.4042 - val_acc: 0.5051\n","Epoch 68/100\n","50000/50000 [==============================] - 5s 108us/step - loss: 1.3469 - acc: 0.5138 - val_loss: 1.3863 - val_acc: 0.5067\n","Epoch 69/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3465 - acc: 0.5154 - val_loss: 1.3893 - val_acc: 0.5072\n","Epoch 70/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3477 - acc: 0.5172 - val_loss: 1.3959 - val_acc: 0.5072\n","Epoch 71/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3329 - acc: 0.5203 - val_loss: 1.4046 - val_acc: 0.5010\n","Epoch 72/100\n","50000/50000 [==============================] - 5s 110us/step - loss: 1.3437 - acc: 0.5178 - val_loss: 1.3808 - val_acc: 0.5096\n","Epoch 73/100\n","50000/50000 [==============================] - 5s 110us/step - loss: 1.3297 - acc: 0.5198 - val_loss: 1.3897 - val_acc: 0.5012\n","Epoch 74/100\n","50000/50000 [==============================] - 5s 110us/step - loss: 1.3401 - acc: 0.5204 - val_loss: 1.3913 - val_acc: 0.5022\n","Epoch 75/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3399 - acc: 0.5191 - val_loss: 1.3660 - val_acc: 0.5166\n","Epoch 76/100\n","50000/50000 [==============================] - 5s 108us/step - loss: 1.3409 - acc: 0.5175 - val_loss: 1.3912 - val_acc: 0.5081\n","Epoch 77/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3243 - acc: 0.5245 - val_loss: 1.3815 - val_acc: 0.5164\n","Epoch 78/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3318 - acc: 0.5229 - val_loss: 1.3997 - val_acc: 0.5003\n","Epoch 79/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3269 - acc: 0.5229 - val_loss: 1.3818 - val_acc: 0.5097\n","Epoch 80/100\n","50000/50000 [==============================] - 5s 108us/step - loss: 1.3276 - acc: 0.5220 - val_loss: 1.3822 - val_acc: 0.5135\n","Epoch 81/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3177 - acc: 0.5275 - val_loss: 1.3831 - val_acc: 0.5107\n","Epoch 82/100\n","50000/50000 [==============================] - 5s 108us/step - loss: 1.3252 - acc: 0.5240 - val_loss: 1.4075 - val_acc: 0.5024\n","Epoch 83/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3172 - acc: 0.5242 - val_loss: 1.3787 - val_acc: 0.5104\n","Epoch 84/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3154 - acc: 0.5247 - val_loss: 1.3850 - val_acc: 0.5042\n","Epoch 85/100\n","50000/50000 [==============================] - 5s 110us/step - loss: 1.3156 - acc: 0.5271 - val_loss: 1.3651 - val_acc: 0.5109\n","Epoch 86/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3143 - acc: 0.5295 - val_loss: 1.3962 - val_acc: 0.5079\n","Epoch 87/100\n","50000/50000 [==============================] - 5s 108us/step - loss: 1.3099 - acc: 0.5286 - val_loss: 1.3874 - val_acc: 0.5029\n","Epoch 88/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3101 - acc: 0.5279 - val_loss: 1.3948 - val_acc: 0.5055\n","Epoch 89/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3061 - acc: 0.5327 - val_loss: 1.3950 - val_acc: 0.5045\n","Epoch 90/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3021 - acc: 0.5340 - val_loss: 1.3911 - val_acc: 0.5033\n","Epoch 91/100\n","50000/50000 [==============================] - 5s 108us/step - loss: 1.2986 - acc: 0.5332 - val_loss: 1.3731 - val_acc: 0.5144\n","Epoch 92/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.2967 - acc: 0.5330 - val_loss: 1.3830 - val_acc: 0.5028\n","Epoch 93/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.2991 - acc: 0.5364 - val_loss: 1.3747 - val_acc: 0.5147\n","Epoch 94/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3069 - acc: 0.5321 - val_loss: 1.3888 - val_acc: 0.5130\n","Epoch 95/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.3009 - acc: 0.5347 - val_loss: 1.3988 - val_acc: 0.5002\n","Epoch 96/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.2983 - acc: 0.5321 - val_loss: 1.3825 - val_acc: 0.5083\n","Epoch 97/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.2913 - acc: 0.5375 - val_loss: 1.3868 - val_acc: 0.5017\n","Epoch 98/100\n","50000/50000 [==============================] - 5s 109us/step - loss: 1.2935 - acc: 0.5383 - val_loss: 1.3738 - val_acc: 0.5128\n","Epoch 99/100\n","50000/50000 [==============================] - 5s 108us/step - loss: 1.2929 - acc: 0.5366 - val_loss: 1.3900 - val_acc: 0.5088\n","Epoch 100/100\n","50000/50000 [==============================] - 5s 108us/step - loss: 1.2909 - acc: 0.5363 - val_loss: 1.3912 - val_acc: 0.5038\n","Test loss: 1.3911784294128418\n","Test acc: 0.5038\n"],"name":"stdout"}]},{"metadata":{"id":"nbzKmSVlIrfx","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"-_afOcsIt67y","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"zTrq5-EDEhgq","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"EjAvhjGTEiCm","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"je9nD6Qgt3ZY","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qKaaNstdD-U4","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"p2M_vmR9D0Oi","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"OnprQiO1t1jj","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"s0IlHWyptwK3","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"-jC7Oo7HtsJ7","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"Py0WQKLHrrUp","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"xzkowKeHpEh0","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"gWLOn3ObnzEK","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"yUh5H4qwnopA","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"NjLsTBRw-Fyc","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}